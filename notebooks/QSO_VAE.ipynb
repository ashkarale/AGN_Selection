{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7eb2c4",
   "metadata": {},
   "source": [
    "# Performing QSO Classification using Variational AutoencodersÂ¶\n",
    "\n",
    "This notebook performs Quasar Classification using variational autoencoders.\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* Ash Karale\n",
    "    \n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Importing Modules\n",
    "2. Data Acquisition\n",
    "3. Data Processing\n",
    "4. Model Definition\n",
    "5. Model Training\n",
    "\n",
    "\n",
    "## Versions:\n",
    "\n",
    "Initial Version: November 2022 (Ash Karale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac234f8",
   "metadata": {},
   "source": [
    "### 1. Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648dcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:01:00) \n",
      "[Clang 13.0.1 ]\n"
     ]
    }
   ],
   "source": [
    "#Importing all required modules\n",
    "\n",
    "#system modules \n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pickle\n",
    "import argparse\n",
    "import itertools\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#mathematical operations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "#pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "#sklearn\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "#scipy\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "#astropy\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "#pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d6452",
   "metadata": {},
   "source": [
    "### 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acfd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/ash/Research/Data/DELVE/'\n",
    "\n",
    "# Examine the data directory\n",
    "display(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d274f",
   "metadata": {},
   "source": [
    "Reading in the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "data = Table.read(os.path.join(data_dir, 'fullcat0_15.fits'))\n",
    "fc015_df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be40be1",
   "metadata": {},
   "source": [
    "Inspecting the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc015_df.hist(figsize = [35, 35], bins=1000)\n",
    "plt.show()\n",
    "display(fc015_df['classprob_dsc_combmod_quasar'].isna().value_counts())\n",
    "display(fc015_df['classprob_dsc_combmod_quasar'].value_counts(), fc015_df['classprob_dsc_combmod_quasar'].value_counts().sum())\n",
    "display(fc015_df.describe())\n",
    "display(fc015_df.quantile([0.95]))\n",
    "display(fc015_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcba208",
   "metadata": {},
   "source": [
    "Checking the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94431c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc015_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b8e57",
   "metadata": {},
   "source": [
    "### 3. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data without employing stratification & removing the quasar class from the test sample\n",
    "\n",
    "for row in fc015_df:\n",
    "    data = dict()\n",
    "    data.update(local_timestamp = row[0])\n",
    "    data.update(nse_timestamp = float(row[1].strip('_')))\n",
    "\n",
    "X = fc015_df.drop(['classprob_dsc_combmod_quasar','classprob_dsc_combmod_galaxy','classprob_dsc_combmod_star','ra','dec'], axis=1)\n",
    "y = fc015_df['classprob_dsc_combmod_quasar']\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state = 123)\n",
    "X_test, X_val, y_test, y_val = model_selection.train_test_split(X_test, y_test, test_size = 0.66, \n",
    "                                                                random_state = 123)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "fc015_df(X_test, columns=X.columns).hist(figsize = [35, 35], bins=500)\n",
    "plt.show()\n",
    "display(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a87859",
   "metadata": {},
   "source": [
    "### 4. Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783ae71",
   "metadata": {},
   "source": [
    "Defining functions for the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_f(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "    y: true labels, numpy.array with shape (n_samples,)\n",
    "    y_pred: predicted labels, numpy.array with shape (n_samples,)\n",
    "    # Return\n",
    "    accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "        ind = np.transpose(np.asarray(linear_assignment(w.max() - w)))\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "nmi_f = normalized_mutual_info_score\n",
    "ari_f = adjusted_rand_score\n",
    "\n",
    "def label_assignement(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    original_labels = np.array([0, 1, 2]) # 0 Star, 1 Gal, 2 Qso\n",
    "    unsupervised_labels = np.array([np.argmax(cf[0, :]), np.argmax(cf[1, :]), np.argmax(cf[2, :])])\n",
    "    y_pred_copy = y_pred.copy()\n",
    "    for i, j in zip(original_labels, unsupervised_labels):\n",
    "        y_pred_copy[y_pred == j] = i\n",
    "        #display(i, j)\n",
    "    y_pred_corr = y_pred_copy\n",
    "    return y_pred_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bbd415",
   "metadata": {},
   "source": [
    "Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildNetwork(layers, activation=\"relu\", dropout=0, is_bn=False):\n",
    "    net = []\n",
    "    for i in range(1, len(layers)):\n",
    "        net.append(nn.Linear(layers[i-1], layers[i]))\n",
    "        if is_bn:\n",
    "            net.append(nn.BatchNorm1d(layers[i]))\n",
    "        if activation==\"relu\":\n",
    "            net.append(nn.ReLU())\n",
    "        elif activation==\"sigmoid\":\n",
    "            net.append(nn.Sigmoid())\n",
    "        elif activation == \"prelu\":\n",
    "            net.append(nn.PReLU())\n",
    "        elif activation == \"elu\":\n",
    "            net.append(nn.ELU())\n",
    "        if dropout > 0:\n",
    "            net.append(nn.Dropout(dropout))\n",
    "    return nn.Sequential(*net)   #*net : input is a list\n",
    "\n",
    "def adjust_learning_rate(init_lr, optimizer, epoch):\n",
    "    lr = max(init_lr * (0.9 ** (epoch//10)), 0.0002)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "    return lr\n",
    "\n",
    "def log_likelihood_samples_unit_gaussian(samples):\n",
    "    return -0.5*math.log(2*math.pi)*samples.size()[1] - torch.sum(0.5*(samples)**2, 1)\n",
    "\n",
    "def log_likelihood_samplesImean_sigma(samples, mu, logvar):  #logvar:log(sigma^2)\n",
    "    return -0.5*log2pi*samples.size()[1] - torch.sum(0.5*(samples-mu)**2/torch.exp(logvar) + 0.5*logvar, 1)\n",
    "\n",
    "\n",
    "class VaDE(nn.Module):\n",
    "    def __init__(self, input_dim=64, z_dim=3, n_centroids=3, binary=False,\n",
    "                 encodeLayer=[64, 25], decodeLayer=[25, 64], activation=\"elu\", dropout=0, is_bn=False):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.n_centroids = n_centroids\n",
    "        self.encoder = buildNetwork([input_dim] + encodeLayer, activation=activation, \n",
    "                                    dropout=dropout, is_bn=binary)\n",
    "        self.decoder = buildNetwork([z_dim] + decodeLayer, activation=activation, dropout=dropout, is_bn=is_bn)\n",
    "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim) # why linear no activation?\n",
    "        self._enc_log_sigma = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        self._dec_mu = nn.Linear(decodeLayer[-1], input_dim)\n",
    "        self._dec_log_sigma = nn.Linear(decodeLayer[-1], input_dim)\n",
    "        self._dec_act = None\n",
    "        self.binary = binary\n",
    "        if binary:\n",
    "            self._dec_act = nn.Sigmoid()\n",
    "\n",
    "        self.create_gmmparam(n_centroids, z_dim)\n",
    "\n",
    "    def create_gmmparam(self, n_centroids, z_dim):\n",
    "        self.theta_p = nn.Parameter(torch.ones(n_centroids)/n_centroids)\n",
    "        self.u_p = nn.Parameter(torch.zeros(z_dim, n_centroids))\n",
    "        self.lambda_p = nn.Parameter(torch.ones(z_dim, n_centroids)) #variance\n",
    "\n",
    "    def initialize_gmm(self, dataloader):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        #use_cuda = False\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        self.eval()\n",
    "        \n",
    "        data = []\n",
    "        for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            z, outputs,out_logvar, mu, logvar = self.forward(inputs)\n",
    "            data.append(z.data.cpu().numpy())\n",
    "        data = np.concatenate(data)\n",
    "        gmm = GaussianMixture(n_components=self.n_centroids,covariance_type='diag')\n",
    "        gmm.fit(data)\n",
    "        self.u_p.data.copy_(torch.from_numpy(gmm.means_.T.astype(np.float32)))  # why transpose?\n",
    "        self.lambda_p.data.copy_(torch.from_numpy(gmm.covariances_.T.astype(np.float32)))\n",
    "\n",
    "    def gmm_kmeans_cluster(self, dataloader):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        #use_cuda = False\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "            \n",
    "        self.eval()\n",
    "        data = []\n",
    "        Y = []\n",
    "        for batch_idx, (inputs, y) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            _, _, _, mu, _ = self.forward(inputs)\n",
    "            data.append(mu.data.cpu().numpy())\n",
    "            Y.append(y.numpy())\n",
    "        data = np.concatenate(data)\n",
    "        Y = np.concatenate(Y).flatten()\n",
    "        gmm = GaussianMixture(n_components=self.n_centroids, covariance_type='full', \n",
    "                              init_params='kmeans', n_init=50)\n",
    "        gmm.fit(data)\n",
    "        y_pred_gmm = gmm.predict(data)\n",
    "        #print(Y.shape, y_pred_gmm.shape)\n",
    "        acc = np.round(acc_f(Y, y_pred_gmm), 5)\n",
    "        nmi = np.round(nmi_f(Y, y_pred_gmm), 5)\n",
    "        ari = np.round(ari_f(Y, y_pred_gmm), 5)\n",
    "        print('GMM fit of AutoEncoder embedding: acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari))\n",
    "\n",
    "        km = KMeans(n_clusters=self.n_centroids, n_init=20)\n",
    "        y_pred_kmeans = km.fit_predict(data)\n",
    "        acc = np.round(acc_f(Y, y_pred_kmeans), 5)\n",
    "        nmi = np.round(nmi_f(Y, y_pred_kmeans), 5)\n",
    "        ari = np.round(ari_f(Y, y_pred_kmeans), 5)\n",
    "        print('Kmeans fit of AutoEncoder embedding: acc = %.5f, nmi = %.5f, ari = %.5f' % (acc, nmi, ari))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "          # num = np.array([[ 1.096506  ,  0.3686553 , -0.43172026,  1.27677995,  1.26733758,\n",
    "          #       1.30626082,  0.14179629,  0.58619505, -0.76423112,  2.67965817]], dtype=np.float32)\n",
    "          # num = np.repeat(num, mu.size()[0], axis=0)\n",
    "          # eps = Variable(torch.from_numpy(num))\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self._enc_mu(h)\n",
    "        logvar = self._enc_log_sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_mu,x_logvar = self.decode(z)\n",
    "        return z, x_mu, x_logvar, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder(z)\n",
    "        x_mu = self._dec_mu(h)\n",
    "        x_logvar = self._dec_log_sigma(h)\n",
    "        if self._dec_act is not None:\n",
    "            x_mu = self._dec_act(x_mu)\n",
    "        return x_mu, x_logvar\n",
    "\n",
    "    def get_gamma(self, z, z_mean, z_log_var):\n",
    "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
    "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
    "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
    "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
    "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
    "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
    "\n",
    "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
    "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
    "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True)\n",
    "\n",
    "        return gamma\n",
    "\n",
    "    def loss_function(self, recon_x_mu, recon_x_logvar, x, z, z_mean, z_log_var):\n",
    "        alpha=3e-3\n",
    "        Z = z.unsqueeze(2).expand(z.size()[0], z.size()[1], self.n_centroids) # NxDxK\n",
    "        z_mean_t = z_mean.unsqueeze(2).expand(z_mean.size()[0], z_mean.size()[1], self.n_centroids)\n",
    "        z_log_var_t = z_log_var.unsqueeze(2).expand(z_log_var.size()[0], z_log_var.size()[1], self.n_centroids)\n",
    "        u_tensor3 = self.u_p.unsqueeze(0).expand(z.size()[0], self.u_p.size()[0], self.u_p.size()[1]) # NxDxK\n",
    "        lambda_tensor3 = self.lambda_p.unsqueeze(0).expand(z.size()[0], self.lambda_p.size()[0], self.lambda_p.size()[1])\n",
    "        theta_tensor2 = self.theta_p.unsqueeze(0).expand(z.size()[0], self.n_centroids) # NxK\n",
    "        \n",
    "        p_c_z = torch.exp(torch.log(theta_tensor2) - torch.sum(0.5*torch.log(2*math.pi*lambda_tensor3)+\\\n",
    "            (Z-u_tensor3)**2/(2*lambda_tensor3), dim=1)) + 1e-10 # NxK\n",
    "        gamma = p_c_z / torch.sum(p_c_z, dim=1, keepdim=True) # NxK\n",
    "\n",
    "        #NX1\n",
    "        if self.binary:\n",
    "            BCE = -torch.sum(x*torch.log(torch.clamp(recon_x_mu, min=1e-10))+(1-x)*torch.log(torch.clamp(1-recon_x_mu, min=1e-10)), 1)\n",
    "        else:\n",
    "            BCE = torch.sum(0.5*math.log(2*math.pi)+0.5*recon_x_logvar+0.5*(x-recon_x_mu)**2/torch.exp(recon_x_logvar),1)\n",
    "        logpzc = torch.sum(0.5*gamma*torch.sum(math.log(2*math.pi)+torch.log(lambda_tensor3)+\n",
    "            torch.exp(z_log_var_t)/lambda_tensor3 + (z_mean_t-u_tensor3)**2/lambda_tensor3, dim=1), dim=1)\n",
    "        qentropy = -0.5*torch.sum(1+z_log_var+math.log(2*math.pi), 1)\n",
    "        logpc = -torch.sum(torch.log(theta_tensor2)*gamma, 1)\n",
    "        logqcx = torch.sum(torch.log(gamma)*gamma, 1)\n",
    "\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        loss = torch.mean(alpha*BCE + logpzc + qentropy + logpc + logqcx)\n",
    "\n",
    "        return gamma, loss\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        pretrained_dict = torch.load(path, map_location=lambda storage, loc: storage)\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "        model_dict.update(pretrained_dict) \n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def fit(self, trainloader, validloader, model_name, save_inter=200 ,lr=0.001, batch_size=128, num_epochs=50,\n",
    "        visualize=False, anneal=False):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        #use_cuda = False\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "        optimizer = Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "\n",
    "        logfile = open('./NNcheckpoints' + model_name + 'cluster_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['epoch', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # train 1 epoch\n",
    "            self.train()\n",
    "            if anneal:\n",
    "                epoch_lr = adjust_learning_rate(lr, optimizer, epoch)\n",
    "            train_loss = 0.0\n",
    "            for batch_idx, (inputs, _) in enumerate(trainloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(inputs)\n",
    "                \n",
    "                z, outputs, out_logvar, mu, logvar = self.forward(inputs)\n",
    "                _, loss = self.loss_function(outputs, out_logvar, inputs, z, mu, logvar)\n",
    "                train_loss += loss.data.item()*len(inputs)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            # validate\n",
    "            if epoch % save_inter == 0:\n",
    "                self.eval()\n",
    "                valid_loss = 0.0\n",
    "                total_num = 0\n",
    "                Y = []\n",
    "                Y_pred = []\n",
    "                for batch_idx, (inputs, labels) in enumerate(validloader):\n",
    "                    inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                    if use_cuda:\n",
    "                        inputs = inputs.cuda()\n",
    "                    inputs = Variable(inputs)\n",
    "                    with torch.no_grad():\n",
    "                        z, outputs, out_logvar, mu, logvar = self.forward(inputs)\n",
    "                    gamma, loss = self.loss_function(outputs, out_logvar, inputs, z, mu, logvar)\n",
    "                    valid_loss += loss.data.item() * len(inputs)\n",
    "                    total_num += len(inputs)\n",
    "                    Y.append(labels.numpy())\n",
    "                    Y_pred.append(np.argmax(gamma.data.cpu().numpy(), axis=1))\n",
    "\n",
    "                valid_loss = valid_loss/total_num\n",
    "                Y = np.concatenate(Y).flatten()\n",
    "                Y_pred = np.concatenate(Y_pred).flatten()\n",
    "                # valid_loss = total_loss / total_num\n",
    "\n",
    "                acc = np.round(acc_f(Y, Y_pred), 5)\n",
    "                nmi = np.round(nmi_f(Y, Y_pred), 5)\n",
    "                ari = np.round(ari_f(Y, Y_pred), 5)\n",
    "                loss = np.round(valid_loss, 5)\n",
    "                logdict = dict(epoch=epoch, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                logwriter.writerow(logdict)\n",
    "                print('Epoch %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (epoch, acc, nmi, ari), ' ; loss=', loss)\n",
    "                if acc>0.973:\n",
    "                    #torch.save(self.state_dict(), \"./NNcheckpoints/Vade_vade{:.4f}\".format(acc))\n",
    "                    pass\n",
    "        logfile.close()\n",
    "\n",
    "    def log_marginal_likelihood_estimate(self, x, num_samples):\n",
    "        weight = torch.zeros(x.size(0))\n",
    "        for i in range(num_samples):\n",
    "            z, recon_x, mu, logvar = self.forward(x)\n",
    "            zloglikelihood = log_likelihood_samples_unit_gaussian(z)\n",
    "            dataloglikelihood = torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\\\n",
    "                (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n",
    "            log_qz = log_likelihood_samplesImean_sigma(z, mu, logvar)\n",
    "            weight += torch.exp(dataloglikelihood + zloglikelihood - log_qz).data\n",
    "        # pdb.set_trace()\n",
    "        return torch.log(torch.clamp(weight/num_samples, min=1e-40))\n",
    "    \n",
    "    def predict_cluster(self, dataloader):\n",
    "        self.eval()\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        #use_cuda = False\n",
    "        Y_pred = []\n",
    "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            with torch.no_grad():\n",
    "                z, outputs, out_logvar, mu, logvar = self.forward(inputs)\n",
    "            #gamma, loss = self.loss_function(outputs, out_logvar, inputs, z, mu, logvar)\n",
    "            #                   loss_function(recon_x_mu, recon_x_logvar, x, z, z_mean, z_log_var):\n",
    "            #                       get_gamma(z, z_mean, z_log_var):\n",
    "                gamma = self.get_gamma(z, mu, logvar)\n",
    "            Y_pred.append(np.argmax(gamma.data.cpu().numpy(), axis=1))\n",
    "        Y_pred = np.concatenate(Y_pred).flatten()\n",
    "        return Y_pred.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549b713",
   "metadata": {},
   "source": [
    "Defining the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b71159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=64, z_dim=3, binary=False,\n",
    "        encodeLayer=[64, 25], decodeLayer=[25, 64], activation=\"elu\"):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.encoder = buildNetwork([input_dim] + encodeLayer, activation=activation)\n",
    "        self.decoder = buildNetwork([z_dim] + decodeLayer, activation=activation)\n",
    "        self._enc_mu = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        self._enc_log_sigma = nn.Linear(encodeLayer[-1], z_dim)\n",
    "        self._dec = nn.Linear(decodeLayer[-1], input_dim)\n",
    "        self._dec_act = None\n",
    "        if binary:\n",
    "            self._dec_act = nn.Sigmoid()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder(z)\n",
    "        x = self._dec(h)\n",
    "        if self._dec_act is not None:\n",
    "            x = self._dec_act(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self._enc_mu(h)\n",
    "        logvar = self._enc_log_sigma(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        #MSE = F.binary_cross_entropy(recon_x, x)\n",
    "        MSE = 5e4*F.mse_loss(recon_x, x)\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= x.size()[0] * x.size()[1]\n",
    "        return MSE + KLD\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        print(\"Gpu available \".format(use_cuda))\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        #use_cuda = False\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                inputs = Variable(inputs)\n",
    "                x_mu, mu, logvar = vae.forward(inputs)\n",
    "    \n",
    "    def fit(self, trainloader, validloader, lr=0.001, batch_size=128, num_epochs=10):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        print(\"Gpu available \".format(use_cuda))\n",
    "        #use_cuda = False\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "        optimizer = Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=lr)\n",
    "        \n",
    "        # validation after initialization\n",
    "        self.eval()\n",
    "        valid_loss = 0.0\n",
    "        for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "            inputs = inputs.view(inputs.size(0), -1).float()\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            inputs = Variable(inputs)\n",
    "            outputs, mu, logvar = self.forward(inputs)\n",
    "\n",
    "            loss = self.loss_function(outputs, inputs, mu, logvar)\n",
    "            valid_loss += loss.data.item()\n",
    "            #valid_loss += loss.data[0]\n",
    "\n",
    "        # valid_loss = total_loss / total_num\n",
    "        print(\"#Epoch -1: Valid Loss: %.5f\" % (valid_loss / len(validloader.dataset)))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # train 1 epoch\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "            for batch_idx, (inputs, _) in enumerate(trainloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                inputs = Variable(inputs)\n",
    "                \n",
    "                outputs, mu, logvar = self.forward(inputs)\n",
    "                loss = self.loss_function(outputs, inputs, mu, logvar)\n",
    "                train_loss += loss.data.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # print(\"    #Iter %3d: Reconstruct Loss: %.3f\" % (\n",
    "                #     batch_idx, recon_loss.data[0]))\n",
    "\n",
    "            # validate\n",
    "            self.eval()\n",
    "            valid_loss = 0.0\n",
    "            for batch_idx, (inputs, _) in enumerate(validloader):\n",
    "                inputs = inputs.view(inputs.size(0), -1).float()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                inputs = Variable(inputs)\n",
    "                outputs, mu, logvar = self.forward(inputs)\n",
    "\n",
    "                loss = self.loss_function(outputs, inputs, mu, logvar)\n",
    "                valid_loss += loss.data.item()\n",
    "                # total_loss += valid_recon_loss.data[0] * inputs.size()[0]\n",
    "                # total_num += inputs.size()[0]\n",
    "\n",
    "            # valid_loss = total_loss / total_num\n",
    "            print(\"#Epoch %3d: Train Loss: %.5f, Valid Loss: %.5f\" % (\n",
    "                epoch, train_loss / len(trainloader.dataset), valid_loss / len(validloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01d723",
   "metadata": {},
   "source": [
    "### 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012794f0",
   "metadata": {},
   "source": [
    "Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, y, batch_size=128):\n",
    "    Xcp = X.copy()\n",
    "    ycp = y.to_numpy().copy()\n",
    "    print(X.shape)\n",
    "        \n",
    "    X = torch.tensor(Xcp, dtype=torch.float)\n",
    "    y = torch.tensor(ycp, dtype=torch.int)    \n",
    "    X=torch.cat([X.view(-1, 64)],0)\n",
    "    y=torch.cat([y.view(-1, 1)],0)\n",
    "    \n",
    "    dataset=dict()\n",
    "    dataset['X']=X\n",
    "    dataset['y']=y\n",
    "\n",
    "    dataloader=DataLoader(TensorDataset(X,y),batch_size=batch_size,shuffle=False,num_workers=0)\n",
    "\n",
    "    return dataloader, dataset\n",
    "\n",
    "class Args:\n",
    "    datadir = '/Users/ash/Research/Data/DELVE/'\n",
    "    input_dim = 64\n",
    "    batch_size = 1024\n",
    "    n_centroids = 3\n",
    "    z_dim = 3\n",
    "    def printself(self):\n",
    "        print(\"printing model parameters\")\n",
    "        print(self.datadir, self.batch_size, self.n_centroids, self.z_dim)\n",
    "\n",
    "#display(X.shape, y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
