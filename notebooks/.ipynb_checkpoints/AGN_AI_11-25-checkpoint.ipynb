{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ee1374",
   "metadata": {},
   "source": [
    "# AGN Selection Through Artificial IntelligenceÂ¶\n",
    "\n",
    "This notebook performs AGN Selection via an Autoencoder. The frameworks used for this deep learning model are TensorFlow and Pytorch.\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* Ash Karale\n",
    "    \n",
    "\n",
    "## Contents:\n",
    "\n",
    "* [I. Introduction](#one)\n",
    "* [II. MNIST Experiment](#two)\n",
    "* [IIa. MNIST AGN Experiment](#three)\n",
    "* [III. AGN DC Experiment](#four)\n",
    "* [IIIa. Data Acquisition](#five)\n",
    "* [IIIb. Exploratory Data Analysis](#six)\n",
    "* [IIIc. Data Processing](#seven)\n",
    "* [IV. Autoencoder Experiments](#eight)\n",
    "* [IVa. Model Definition](#nine)\n",
    "* [IVb. Model Visualization](#ten)\n",
    "* [IVc. Model Training](#eleven)\n",
    "* [IVd. Model Output](#twelve)\n",
    "\n",
    "\n",
    "## Versions:\n",
    "\n",
    "Initial Version: November 2022 (Ash Karale)\n",
    "\n",
    "Latest Version: November 2023 (Ash Karale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a97de",
   "metadata": {},
   "source": [
    "\n",
    "## I. Introduction <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd4bbc",
   "metadata": {},
   "source": [
    "With the upcoming Legacy Survey of Space and Time potentially identifying 40 billion celestial objects, our goal is to pinpoint the 100 million that are AGNs - a classic \"needle-in-a- haystack\" problem that requires sophisticated machine learning techniques to handle. We explore the use of autoencoders for AGN selection from data akin to the anticipated LSST survey. \n",
    "\n",
    "The data is gathered from the AGN Data Challenge which comprises of two primary sources known as the Sloan Digital Sky Survey Stripe 82, and the XMM-Large Scale Structure area. Furthermore, we explore the comparative effectiveness of TensorFlow and PyTorch in constructing autoencoders, providing an interface between computer science and astronomy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6467b",
   "metadata": {},
   "source": [
    "### Visual Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a27411",
   "metadata": {},
   "source": [
    "Our primary goal is to gain a solid understanding of neural networks and a specialized type of them- autoencoders, so we can use them to do AGN science.\n",
    "\n",
    "Neural networks are a cornerstone of artificial intelligence and machine learning. They draw inspiration from the human brain, using interconnected nodes or \"neurons\" to process information and make decisions. A neural network takes in inputs, which are processed in hidden layers using weights that are adjusted during training. The model then outputs a prediction. The weights are adjusted to find patterns in order to map inputs to outputs.\n",
    "\\\n",
    "![Neural Networks](neural_networks.png)\n",
    "\\\n",
    "One particular type of neural network that we will delve deeper into is an autoencoder. Autoencoders are unsupervised learning models that are used for data compression, noise reduction, and feature extraction. They have an interesting architecture, consisting of an encoder that compresses the input data and a decoder that attempts to reconstruct the original input</font> from the compressed data.\n",
    "\\\n",
    "![Autoencoder](autoencoder.png)\n",
    "\\\n",
    "We will be implementing a simple autoencoder using the popular deep learning library, Keras, and the well-known MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b30306",
   "metadata": {},
   "source": [
    "## II. MNIST Experiment <a class=\"anchor\" id=\"two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef602c75",
   "metadata": {},
   "source": [
    "We will use the famous MNIST dataset for the purpose of demonstration. Note that this dataset is used because of its simplicity and ready availability, though the concepts can be applied to more complex datasets, including those related to active galaxies in astronomy.\n",
    "\n",
    "In Python, we use the Keras API which works on top of TensorFlow to implement our autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d332eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# Normalize all values between 0 and 1 for easier processing by the neural network.\n",
    "# Neural networks often perform better on normalized data.\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Flatten the 28x28 images into vectors of size 784\n",
    "# GTR: Why don't you just use the \"flatten function\"??\n",
    "# Ash: The reshape function is indeed similar to flatten. Using flatten() is more\n",
    "# explicit and could be clearer to new students. The following line is equivalent:\n",
    "# x_train = x_train.reshape((len(x_train), -1))\n",
    "# The -1 in reshape automatically calculates the size of the flattened dimension.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed86767",
   "metadata": {},
   "source": [
    "The MNIST dataset contains 60,000 training images and 10,000 test images, each 28x28 pixels in size. By reshaping the data and normalizing pixel values to a [0,1] range, we prepare our input for the autoencoder. Although TensorFlow provides a flatten method, using reshape is a straightforward approach to convert the 2D image matrices into 1D vectors required for our dense neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30d3b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of our encoded representations\n",
    "# GTR: Should note that this is/was an arbitrary choice\n",
    "# Ash: The encoding dimension is a hyperparameter that defines the size of the compressed\n",
    "# representation. We chose 32 arbitrarily for this example, but in practice, we would\n",
    "# experiment with different sizes to find the most effective encoding.\n",
    "encoding_dim = 32  \n",
    "\n",
    "# input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# Adding a hidden layer before the bottleneck\n",
    "hidden_layer = Dense(128, activation='relu')(input_img)\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# Adding a hidden layer after the bottleneck\n",
    "hidden_layer_decoded = Dense(128, activation='relu')(encoded)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ac45d1",
   "metadata": {},
   "source": [
    "We choose an encoding dimension of 32 to compress the input; this is a balance between retaining enough information and achieving meaningful compression. ReLU is selected for the encoder to introduce non-linearity, helping the model learn complex patterns without the vanishing gradient problem. For the decoder, we use a sigmoid activation because we want the output values to be in the range [0,1], the same as our input data.\n",
    "The addition of a 128-node hidden layer before and after the encoding layer introduces additional levels of abstraction, allowing the network to learn more complex representations. This also enhances the \"funneling\" effect, where data is compressed down to a lower-dimensional latent space and then expanded back to its original dimension. This structure encourages the autoencoder to learn a more robust representation of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc4db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# GTR: briefly explain the choice of parameters here (and in the 5 lines of code above)\n",
    "# Ash: Here we're setting up our model with the 'adam' optimizer which is a popular choice\n",
    "# due to its efficiency. We're using 'binary_crossentropy' as our loss function since\n",
    "# we're treating the reconstruction of each pixel as a binary classification problem\n",
    "# (the output is between 0 and 1 due to the sigmoid activation).\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc7900",
   "metadata": {},
   "source": [
    "The adam optimizer is used for its adaptive learning rate capabilities, which help converge to the optimal set of weights more efficiently. We employ binary cross-entropy as the loss function, which is suited for a reconstruction task where the output is a probability in the range [0,1], representing the likelihood of each pixel being active (or white in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538a2dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/235 [..............................] - ETA: 1:08 - loss: 0.6943"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:03:12.104544: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - ETA: 0s - loss: 0.2775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x96x1x784xi1>'\n",
      "2023-11-27 13:03:13.826325: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x256x1x784xi1>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 2s 8ms/step - loss: 0.2775 - val_loss: 0.1903\n",
      "Epoch 2/50\n",
      " 28/235 [==>...........................] - ETA: 1s - loss: 0.1900"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x16x1x784xi1>'\n",
      "loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/495c257e-668e-11ee-93ce-926038f30c31/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.gain_offset_control' op result #0 must be 4D/5D memref of 16-bit float or 8-bit signed integer or 8-bit unsigned integer values, but got 'memref<1x16x1x784xi1>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 2s 6ms/step - loss: 0.1711 - val_loss: 0.1524\n",
      "Epoch 3/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1429 - val_loss: 0.1326\n",
      "Epoch 4/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1276 - val_loss: 0.1208\n",
      "Epoch 5/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1180 - val_loss: 0.1128\n",
      "Epoch 6/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1113 - val_loss: 0.1071\n",
      "Epoch 7/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.1062 - val_loss: 0.1028\n",
      "Epoch 8/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.1026 - val_loss: 0.0999\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0998 - val_loss: 0.0976\n",
      "Epoch 10/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0979 - val_loss: 0.0959\n",
      "Epoch 11/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0965 - val_loss: 0.0947\n",
      "Epoch 12/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0956 - val_loss: 0.0940\n",
      "Epoch 13/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0950 - val_loss: 0.0936\n",
      "Epoch 14/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0946 - val_loss: 0.0931\n",
      "Epoch 15/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0943 - val_loss: 0.0929\n",
      "Epoch 16/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0940 - val_loss: 0.0926\n",
      "Epoch 17/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0939 - val_loss: 0.0925\n",
      "Epoch 18/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0937 - val_loss: 0.0924\n",
      "Epoch 19/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0936 - val_loss: 0.0923\n",
      "Epoch 20/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0935 - val_loss: 0.0922\n",
      "Epoch 21/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 22/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0934 - val_loss: 0.0920\n",
      "Epoch 23/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0933 - val_loss: 0.0920\n",
      "Epoch 24/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 25/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0932 - val_loss: 0.0919\n",
      "Epoch 26/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 27/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0931 - val_loss: 0.0918\n",
      "Epoch 28/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0931 - val_loss: 0.0918\n",
      "Epoch 29/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 30/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 31/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 32/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 33/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 34/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 35/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 36/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 37/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 38/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 39/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 40/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0928 - val_loss: 0.0915\n",
      "Epoch 41/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 42/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 43/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 44/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 45/50\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 46/50\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.0926 - val_loss: 0.0915\n",
      "Epoch 47/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0926 - val_loss: 0.0915\n",
      "Epoch 48/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0926 - val_loss: 0.0914\n",
      "Epoch 49/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0926 - val_loss: 0.0915\n",
      "Epoch 50/50\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.0926 - val_loss: 0.0914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x175bac460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train autoencoder for 50 epochs\n",
    "# GTR: briefly explain the choice of parameters here\n",
    "# Ash: We are training our model for 50 iterations over the entire dataset (epochs),\n",
    "# using a batch size of 256. 'Shuffle=True' shuffles the training data every epoch to prevent\n",
    "# cyclic patterns that could affect learning. We're using the test data as validation data\n",
    "# to monitor overfitting during training.\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52480125",
   "metadata": {},
   "source": [
    "The choice of 50 epochs is to give the model sufficient time to learn, as autoencoders typically require more epochs for training due to their complexity. A batch size of 256 is large enough for efficient processing and small enough to maintain a noisy gradient descent, which often helps in finding global minima. Shuffling the data ensures that each batch is different, which helps in preventing overfitting and makes the model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935c36e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88/313 [=======>......................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:04:28.379377: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use the trained autoencoder to make predictions on the test data\n",
    "# GTR: Be more specific.\n",
    "# Ash: Here, we use the trained autoencoder to predict the test images. We input the flattened image arrays\n",
    "# and the model outputs the reconstructed images. This is a process of encoding the original\n",
    "# high-dimensional data into a lower-dimensional form and then decoding it back into the original space,\n",
    "# inevitably losing some information but trying to minimize this loss.\n",
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7896a",
   "metadata": {},
   "source": [
    "The predict function applies the trained autoencoder to the test data, encoding and then decoding it to produce a reconstruction. This serves as a test to see how well the autoencoder can generalize to new, unseen data, given that it has learned a compact representation of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0b68103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib to visualize the reconstructed inputs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GTR: Remind user how many there are in the dataset and that this is just a sample for visual checking that\n",
    "# GTR: things are indeed working\n",
    "# Ash: The test dataset contains 10,000 images, but we'll only visualize a subset of 10 to see\n",
    "# the reconstruction quality. This provides a quick check to ensure that our autoencoder is\n",
    "# learning to compress and reconstruct the images effectively.\n",
    "n = 10  # Number of digits to display as a sample from the test set\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstructed images\n",
    "    # The subplot below will display the reconstructed image that corresponds to the original image above.\n",
    "    # It is reshaped back to 28x28 pixels from the flattened output of the autoencoder.\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfd0ad",
   "metadata": {},
   "source": [
    "Finally, we visualize 10 examples to qualitatively evaluate the autoencoder's performance. It's important to remember that the MNIST test dataset contains 10,000 images. Here, we're only sampling a small subset for visualization to check if the autoencoder is reconstructing digits that visually resemble the originals, despite the dimensionality reduction and subsequent information loss.\n",
    "\n",
    "This gives us a solid foundation to start understanding how neural networks and autoencoders function. We can apply this same basic concept to selecting AGNs from the AGN DC Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99125b",
   "metadata": {},
   "source": [
    "### Application to AGN DC Dataset: Understanding and Implementing Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1167be",
   "metadata": {},
   "source": [
    "To understand how the process of using an autoencoder relates to our datasetâthe AGN DC, which is the Sloan Digital Sky Survey (SDSS) Stripe 82, and the XMM-Large Scale Structure (XMM-LSS)âwe need to consider the nature of the data and the benefits of the autoencoder model.\n",
    "\n",
    "Our astronomical data sources provide us with a high-dimensional dataset. Each data point (e.g., an observed galaxy, star, or AGN) might come with several associated attributes such as its light curve features, astrometry, photometry, color, morphology, etc.\n",
    "\n",
    "In this high-dimensional space, meaningful patterns or structures may not be immediately evident. Furthermore, as the data from these sources might not fully represent the diversity of astronomical objects that LSST will observe, direct application of supervised learning might lead to biased or incomplete models.\n",
    "\n",
    "This is where autoencoders come into play. Autoencoders are a type of neural network that can learn to compress data into a lower-dimensional representation. In the encoding stage, the autoencoder learns to capture the most salient features of the data, effectively reducing its dimensionality. This process can help reveal underlying structure in the data that might be difficult to discern in the original high-dimensional space.\n",
    "\n",
    "In the decoding stage, the autoencoder learns to reconstruct the original data from the compressed representation. By comparing the original data with the reconstructed data (as we do visually in the code block), we can get a sense of how well the autoencoder has learned to capture the essential features of the data.\n",
    "\n",
    "The encoded representation, or the \"latent space\", holds a compressed form of the data that ideally captures its most important features. This latent space can then be used as input to a supervised classification algorithm, which can be more manageable and more effective than working with the original high-dimensional data.\n",
    "\n",
    "So, in our case, the autoencoder serves two purposes:\n",
    "\n",
    "i. It helps us explore and understand the structure of our high-dimensional astronomical data.\n",
    "\n",
    "ii. It provides a form of unsupervised pre-training that can compensate for the underrepresentation in our training data. The autoencoder learns general features from the entire dataset, and this learning can then be fine-tuned with our specific but limited labeled data.\n",
    "\n",
    "Thus, our visual comparison of original and autoencoder-reconstructed images (or data points) in the code block serves as an intuitive performance check for our model, indicating how well the autoencoder can capture the essential features of our complex astronomical dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2e8f3",
   "metadata": {},
   "source": [
    "## IIa. MNIST AGN Experiment <a class=\"anchor\" id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da723f",
   "metadata": {},
   "source": [
    "In the context of astronomy, each observation (or astronomical object) can be likened to an individual digit in the MNIST dataset, and each feature of this observation can be thought of as a pixel. However, unlike pixels in an image that combine to form a visual pattern, features in astronomical data do not inherently create a visual image but rather comprise a multi-dimensional feature space. Most astronomical data is non-image data, where each feature represents a different characteristic of the observation, such as luminosity, color index, or spectral line properties.\n",
    "\n",
    "Given this, we will use the MNIST dataset as a stand-in for our astronomical data to demonstrate the process before applying it to actual astronomical datasets. In this educational example, we categorize '3s', '5s', and '7s' from MNIST as our three classes analogous to stars, quasars, and galaxies, respectively.\n",
    "\n",
    "Our objective with the autoencoder isn't to reconstruct images but to learn a reduced representation of the data that can help in classifying these astronomical objects into one of the three categories. It's important to note that while an autoencoder is typically used for dimensionality reduction or feature learning, in a real-world scenario, we would likely use a classification algorithm directly, possibly leveraging the features learned by an autoencoder.\n",
    "\n",
    "Here is how we might structure our experimental approach:\n",
    "\n",
    "Generate Synthetic Astronomical Data:\n",
    "Instead of image data, create a dataset where each entry represents an astronomical object with features corresponding to different observations.\n",
    "Normalize this data so that each feature has a similar range, important for training neural networks effectively.\n",
    "\n",
    "1. Define the Autoencoder:\n",
    "Use the same architecture as before but ensure it is suitable for the dimensionality of our synthetic data.\n",
    "Keep in mind that we may need to adjust the size and number of layers depending on the complexity and size of the actual data.\n",
    "\n",
    "2. Training:\n",
    "Train the autoencoder on this synthetic dataset to learn the essential features.\n",
    "While training on MNIST, we are implicitly assuming that each class ('3', '5', '7') is sufficiently distinct in its feature space, analogous to stars, quasars, and galaxies.\n",
    "\n",
    "3. Reconstruction and Visualization:\n",
    "After training, we use the autoencoder to reconstruct the data points for each class.\n",
    "We then visualize the original and reconstructed data using histograms or similar statistical tools to assess the performance. The x-axis of these histograms would represent the feature values, and the y-axis would represent their frequency or distribution.\n",
    "\n",
    "4. Multiclass Classification:\n",
    "Ultimately, the goal is to label each astronomical object with its respective class and provide probabilities for each class.\n",
    "This step might require additional classification layers or algorithms beyond the autoencoder.\n",
    "\n",
    "By first working with the MNIST dataset, we can understand the autoencoder's mechanics in a controlled environment before moving to real astronomical data, which may be more complex and less structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660435c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 1/70 [..............................] - ETA: 25s - loss: 3.3803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:04:33.749536: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 1s 8ms/step - loss: 0.2793\n",
      "Epoch 2/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0819e-04\n",
      "Epoch 3/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 9.5586e-08\n",
      "Epoch 4/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.7526e-11\n",
      "Epoch 5/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.6935e-13\n",
      "Epoch 6/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1424e-13\n",
      "Epoch 7/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1449e-13\n",
      "Epoch 8/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1470e-13\n",
      "Epoch 9/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1512e-13\n",
      "Epoch 10/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1507e-13\n",
      "Epoch 11/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1550e-13\n",
      "Epoch 12/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1537e-13\n",
      "Epoch 13/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1529e-13\n",
      "Epoch 14/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1614e-13\n",
      "Epoch 15/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1639e-13\n",
      "Epoch 16/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1622e-13\n",
      "Epoch 17/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1686e-13\n",
      "Epoch 18/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1656e-13\n",
      "Epoch 19/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1692e-13\n",
      "Epoch 20/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1693e-13\n",
      "Epoch 21/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1711e-13\n",
      "Epoch 22/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1723e-13\n",
      "Epoch 23/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1732e-13\n",
      "Epoch 24/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1741e-13\n",
      "Epoch 25/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1794e-13\n",
      "Epoch 26/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1898e-13\n",
      "Epoch 27/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1917e-13\n",
      "Epoch 28/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2116e-13\n",
      "Epoch 29/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2030e-13\n",
      "Epoch 30/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2011e-13\n",
      "Epoch 31/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1938e-13\n",
      "Epoch 32/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2202e-13\n",
      "Epoch 33/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2532e-13\n",
      "Epoch 34/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2541e-13\n",
      "Epoch 35/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2443e-13\n",
      "Epoch 36/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2615e-13\n",
      "Epoch 37/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2841e-13\n",
      "Epoch 38/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2943e-13\n",
      "Epoch 39/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.3472e-13\n",
      "Epoch 40/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3752e-13\n",
      "Epoch 41/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0019\n",
      "Epoch 42/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 6.9147e-05\n",
      "Epoch 43/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 4.9936e-07\n",
      "Epoch 44/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.9068e-09\n",
      "Epoch 45/200\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.2227e-10\n",
      "Epoch 46/200\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 2.0992e-12\n",
      "Epoch 47/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.4651e-13\n",
      "Epoch 48/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1421e-13\n",
      "Epoch 49/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1432e-13\n",
      "Epoch 50/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1654e-13\n",
      "Epoch 51/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1667e-13\n",
      "Epoch 52/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1867e-13\n",
      "Epoch 53/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2429e-13\n",
      "Epoch 54/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2695e-13\n",
      "Epoch 55/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2360e-13\n",
      "Epoch 56/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3977e-13\n",
      "Epoch 57/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.6843e-13\n",
      "Epoch 58/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.1207e-13\n",
      "Epoch 59/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 0.0012\n",
      "Epoch 60/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.2575e-04\n",
      "Epoch 61/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.7247e-05\n",
      "Epoch 62/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.7342e-06\n",
      "Epoch 63/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.2227e-07\n",
      "Epoch 64/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1711e-07\n",
      "Epoch 65/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.1922e-07\n",
      "Epoch 66/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.3966e-06\n",
      "Epoch 67/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.6527e-06\n",
      "Epoch 68/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.1409e-05\n",
      "Epoch 69/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.1814e-04\n",
      "Epoch 70/200\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 7.9454e-05\n",
      "Epoch 71/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.5048e-05\n",
      "Epoch 72/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.5586e-05\n",
      "Epoch 73/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.6006e-05\n",
      "Epoch 74/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.5066e-05\n",
      "Epoch 75/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.3662e-05\n",
      "Epoch 76/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.8985e-05\n",
      "Epoch 77/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.8517e-05\n",
      "Epoch 78/200\n",
      "70/70 [==============================] - 1s 8ms/step - loss: 1.0865e-04\n",
      "Epoch 79/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.3931e-04\n",
      "Epoch 80/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.1889e-05\n",
      "Epoch 81/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0115e-04\n",
      "Epoch 82/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2240e-04\n",
      "Epoch 83/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 5.1404e-05\n",
      "Epoch 84/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.5287e-04\n",
      "Epoch 85/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1421e-04\n",
      "Epoch 86/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1782e-04\n",
      "Epoch 87/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 9.3206e-05\n",
      "Epoch 88/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.3958e-05\n",
      "Epoch 89/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.7098e-05\n",
      "Epoch 90/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.7061e-05\n",
      "Epoch 91/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.4089e-04\n",
      "Epoch 92/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.5124e-04\n",
      "Epoch 93/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.6639e-05\n",
      "Epoch 94/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.8095e-05\n",
      "Epoch 95/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 9.9377e-06\n",
      "Epoch 96/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 9.7220e-06\n",
      "Epoch 97/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0935e-05\n",
      "Epoch 98/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5609e-04\n",
      "Epoch 99/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.4551e-04\n",
      "Epoch 100/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.7115e-04\n",
      "Epoch 101/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.9632e-05\n",
      "Epoch 102/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.6465e-05\n",
      "Epoch 103/200\n",
      "70/70 [==============================] - 1s 7ms/step - loss: 1.8720e-05\n",
      "Epoch 104/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 4.9027e-05\n",
      "Epoch 105/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.6276e-04\n",
      "Epoch 106/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 8.6144e-05\n",
      "Epoch 107/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.2320e-04\n",
      "Epoch 108/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 5.1590e-04\n",
      "Epoch 109/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1264e-04\n",
      "Epoch 110/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 4.3296e-05\n",
      "Epoch 111/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.3763e-05\n",
      "Epoch 112/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.9841e-05\n",
      "Epoch 113/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 2.5958e-05\n",
      "Epoch 114/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.8408e-05\n",
      "Epoch 115/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.5365e-05\n",
      "Epoch 116/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.4574e-05\n",
      "Epoch 117/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.7204e-05\n",
      "Epoch 118/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.7422e-05\n",
      "Epoch 119/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.9200e-05\n",
      "Epoch 120/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 7.6489e-05\n",
      "Epoch 121/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.0561e-05\n",
      "Epoch 122/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5216e-04\n",
      "Epoch 123/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.4035e-04\n",
      "Epoch 124/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.8882e-04\n",
      "Epoch 125/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5153e-04\n",
      "Epoch 126/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.1362e-05\n",
      "Epoch 127/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2995e-05\n",
      "Epoch 128/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.0691e-05\n",
      "Epoch 129/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3524e-04\n",
      "Epoch 130/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.3484e-04\n",
      "Epoch 131/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3588e-04\n",
      "Epoch 132/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0994e-04\n",
      "Epoch 133/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3083e-04\n",
      "Epoch 134/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.6126e-05\n",
      "Epoch 135/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.4149e-04\n",
      "Epoch 136/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.3386e-05\n",
      "Epoch 137/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.7731e-05\n",
      "Epoch 138/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.6356e-05\n",
      "Epoch 139/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.2811e-05\n",
      "Epoch 140/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.3455e-05\n",
      "Epoch 141/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.5081e-05\n",
      "Epoch 142/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.7206e-04\n",
      "Epoch 143/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1251e-04\n",
      "Epoch 144/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.3681e-05\n",
      "Epoch 145/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.1509e-05\n",
      "Epoch 146/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.3205e-04\n",
      "Epoch 147/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2501e-04\n",
      "Epoch 148/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.8924e-05\n",
      "Epoch 149/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.9378e-05\n",
      "Epoch 150/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.1151e-05\n",
      "Epoch 151/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.6426e-05\n",
      "Epoch 152/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.7084e-05\n",
      "Epoch 153/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.1142e-04\n",
      "Epoch 154/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 1.1267e-04\n",
      "Epoch 155/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.9345e-05\n",
      "Epoch 156/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.7160e-05\n",
      "Epoch 157/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 3.2052e-05\n",
      "Epoch 158/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.9875e-05\n",
      "Epoch 159/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.8179e-04\n",
      "Epoch 160/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.8405e-04\n",
      "Epoch 161/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1128e-04\n",
      "Epoch 162/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.0228e-05\n",
      "Epoch 163/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3147e-05\n",
      "Epoch 164/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.7526e-06\n",
      "Epoch 165/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.1487e-05\n",
      "Epoch 166/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.7818e-05\n",
      "Epoch 167/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1831e-04\n",
      "Epoch 168/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.7723e-04\n",
      "Epoch 169/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.1614e-05\n",
      "Epoch 170/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.6558e-05\n",
      "Epoch 171/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 3.9325e-05\n",
      "Epoch 172/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.2937e-05\n",
      "Epoch 173/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3463e-04\n",
      "Epoch 174/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5780e-04\n",
      "Epoch 175/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.2866e-04\n",
      "Epoch 176/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3030e-04\n",
      "Epoch 177/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0222e-04\n",
      "Epoch 178/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1350e-04\n",
      "Epoch 179/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.3938e-05\n",
      "Epoch 180/200\n",
      "70/70 [==============================] - 0s 7ms/step - loss: 9.4935e-05\n",
      "Epoch 181/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 4.9356e-05\n",
      "Epoch 182/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 6.5052e-05\n",
      "Epoch 183/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.8766e-05\n",
      "Epoch 184/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.0064e-05\n",
      "Epoch 185/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.7901e-04\n",
      "Epoch 186/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.2491e-05\n",
      "Epoch 187/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 8.3374e-05\n",
      "Epoch 188/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0148e-04\n",
      "Epoch 189/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 5.0992e-05\n",
      "Epoch 190/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.4195e-04\n",
      "Epoch 191/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.0627e-04\n",
      "Epoch 192/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 0s 6ms/step - loss: 7.4073e-05\n",
      "Epoch 193/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.1360e-05\n",
      "Epoch 194/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.6516e-06\n",
      "Epoch 195/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.1060e-05\n",
      "Epoch 196/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 9.2500e-05\n",
      "Epoch 197/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5159e-04\n",
      "Epoch 198/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 2.3784e-04\n",
      "Epoch 199/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.5659e-04\n",
      "Epoch 200/200\n",
      "70/70 [==============================] - 0s 6ms/step - loss: 1.3277e-04\n",
      " 79/557 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 13:06:05.840401: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557/557 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "# Assume we have three classes and create a list of class names\n",
    "class_names = ['star', 'quasar', 'galaxy']\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Filter data for only \"3\", \"5\", \"7\" digits and normalize, flatten\n",
    "# GTR: Need more comments about this (either here or in markdown cells)\n",
    "# Ash: Filtering MNIST data to only use 3s, 5s, and 7s to represent stars, quasars, and galaxies respectively. \n",
    "# Each image is flattened from a 28x28 matrix to a 784-element vector and then normalized so each pixel value is between 0 and 1.\n",
    "star_data = x_train[y_train == 3].reshape(-1, 784).astype('float32') / 255\n",
    "quasar_data = x_train[y_train == 5].reshape(-1, 784).astype('float32') / 255\n",
    "galaxy_data = x_train[y_train == 7].reshape(-1, 784).astype('float32') / 255\n",
    "\n",
    "# Concatenate all data into a single array\n",
    "# GTR: Say something about this repackaging all three data types into one data set \n",
    "# (where we wouldn't necessarily know the class).  Might be good to force some class imbalance here.\n",
    "# GTR: Or do it balanced first, then unbalanced to see what difference that makes and talk about solutions.\n",
    "# Ash: Combining star, quasar, and galaxy data into one dataset to simulate a mixed, unlabeled astronomical dataset.\n",
    "all_data = np.concatenate([star_data, quasar_data, galaxy_data])\n",
    "\n",
    "# Normalizing the data by a MinMaxScaler and fit the scaler to the data and transform it\n",
    "# GTR: Worth noting here that applying such a scalar might not make sense for real astro data.\n",
    "# GTR: Or at least not for all the features. E.g., some features are log quantities, some are negative, etc.\n",
    "# Ash: Normalizing the concatenated data, though real astronomical data may require different preprocessing.\n",
    "scaler = MinMaxScaler()\n",
    "all_data = scaler.fit_transform(all_data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "# GTR: Note that we are doing this mostly for visualization purposes. So that we can plot the input and output data.\n",
    "# GTR: Otherwise, we have to check 784 features. (Although, I suppose that we could just spot check 10 of them.)\n",
    "# Ash: Reducing dimensionality to 10 principal components for simplifying visualization and interpretation.\n",
    "pca = PCA(n_components=10)\n",
    "all_data_reduced = pca.fit_transform(all_data)\n",
    "\n",
    "# Define autoencoder architecture\n",
    "# Define input layer, adjust the input dimension to match the PCA-reduced data\n",
    "input_layer = Input(shape=(all_data_reduced.shape[1],))\n",
    "# Define the encoding layers with linear activation\n",
    "encoded = Dense(128, activation='linear')(input_layer)\n",
    "encoded = Dense(64, activation='linear')(encoded)\n",
    "encoded = Dense(32, activation='linear')(encoded)\n",
    "\n",
    "# Define the decoding layers with linear activation\n",
    "decoded = Dense(64, activation='linear')(encoded)\n",
    "decoded = Dense(128, activation='linear')(decoded)\n",
    "decoded = Dense(all_data_reduced.shape[1], activation='linear')(decoded)  # Match output dimension to input\n",
    "\n",
    "# Instantiate and compile the autoencoder model\n",
    "# GTR: As above. Why linear? Why mse? Etc.\n",
    "# Ash: Using linear activation functions and MSE loss as this is a simple example focusing on reconstructing input values.\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the model to the PCA-reduced data\n",
    "autoencoder.fit(all_data_reduced, all_data_reduced,\n",
    "                epochs=200,\n",
    "                batch_size=256,\n",
    "                shuffle=True)\n",
    "\n",
    "# Generate reconstructions of the input data\n",
    "decoded_data = autoencoder.predict(all_data_reduced)\n",
    "\n",
    "# Following the reconstruction, we would compare the original and decoded datasets to evaluate the autoencoder's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c3533",
   "metadata": {},
   "source": [
    "In the following cells, we will visualize the reconstruction quality of the autoencoder. We start by examining the first principal component for its distinctiveness across our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3753d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_data_lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Loop over each class for eigenvector 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, class_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(class_names):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Extract original and decoded data for the first eigenvector for current class\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     original_data_j \u001b[38;5;241m=\u001b[39m all_data_reduced[:, eigenvector_index][\u001b[43moriginal_data_lengths\u001b[49m[j]\u001b[38;5;241m*\u001b[39mj:original_data_lengths[j]\u001b[38;5;241m*\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m     decoded_data_j \u001b[38;5;241m=\u001b[39m decoded_data_pca[:, eigenvector_index][original_data_lengths[j]\u001b[38;5;241m*\u001b[39mj:original_data_lengths[j]\u001b[38;5;241m*\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Plot original data for eigenvector 1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_data_lengths' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, plot for Eigenvector 1 to assess its distinctiveness across classes\n",
    "fig, ax = plt.subplots(figsize=(10, 5))  # Single plot for eigenvector 1\n",
    "eigenvector_index = 0  # Index for the first eigenvector\n",
    "\n",
    "# Loop over each class for eigenvector 1\n",
    "for j, class_name in enumerate(class_names):\n",
    "    # Extract original and decoded data for the first eigenvector for current class\n",
    "    original_data_j = all_data_reduced[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "    decoded_data_j = decoded_data_pca[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "    \n",
    "    # Plot original data for eigenvector 1\n",
    "    ax.hist(original_data_j, bins=50, color=colors_original[j], edgecolor='black', alpha=0.7, label=f'{class_name} Original')\n",
    "    \n",
    "    # Plot decoded data for eigenvector 1\n",
    "    ax.hist(decoded_data_j, bins=50, color=colors_decoded[j], edgecolor='black', alpha=0.5, linestyle='dashed', label=f'{class_name} Decoded')\n",
    "\n",
    "# Set title and labels, and add a legend for eigenvector 1\n",
    "ax.set_title(f'Eigenvector {eigenvector_index+1}')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot for eigenvector 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b918d50",
   "metadata": {},
   "source": [
    "Next, we will examine selected eigenvectors (2, 5, 8, and 9) where one class is expected to separate well from the others. Following this, we will look at the remaining eigenvectors to analyze class overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588402b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for selected eigenvectors with potentially good class separation\n",
    "well_separated_eigenvectors = [1, 4, 7, 8]  # considering 0-indexing, these correspond to eigenvectors 2, 5, 8, and 9\n",
    "fig, axes = plt.subplots(len(well_separated_eigenvectors), 1, figsize=(10, 5 * len(well_separated_eigenvectors)))\n",
    "\n",
    "# Loop over each selected eigenvector\n",
    "for idx, eigenvector_index in enumerate(well_separated_eigenvectors):\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        # Extract original and decoded data for current eigenvector and class\n",
    "        original_data_j = all_data_reduced[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "        decoded_data_j = decoded_data_pca[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "\n",
    "        # Plot original data\n",
    "        axes[idx].hist(original_data_j, bins=50, color=colors_original[j], edgecolor='black', alpha=0.7, label=f'{class_name} Original')\n",
    "        \n",
    "        # Plot decoded data\n",
    "        axes[idx].hist(decoded_data_j, bins=50, color=colors_decoded[j], edgecolor='black', alpha=0.5, linestyle='dashed', label=f'{class_name} Decoded')\n",
    "\n",
    "    # Set title and labels, and add a legend for each eigenvector\n",
    "    axes[idx].set_title(f'Eigenvector {eigenvector_index+1}')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "\n",
    "# Show the plot for selected eigenvectors\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027a25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for the remaining eigenvectors with less class separation\n",
    "other_eigenvectors = [i for i in range(10) if i not in well_separated_eigenvectors]\n",
    "fig, axes = plt.subplots(len(other_eigenvectors), 1, figsize=(10, 5 * len(other_eigenvectors)))\n",
    "\n",
    "for idx, eigenvector_index in enumerate(other_eigenvectors):\n",
    "    for j, class_name in enumerate(class_names):\n",
    "        original_data_j = all_data_reduced[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "        decoded_data_j = decoded_data_pca[:, eigenvector_index][original_data_lengths[j]*j:original_data_lengths[j]*(j+1)]\n",
    "\n",
    "        axes[idx].hist(original_data_j, bins=50, color=colors_original[j], edgecolor='black', alpha=0.7, label=f'{class_name} Original')\n",
    "        axes[idx].hist(decoded_data_j, bins=50, color=colors_decoded[j], edgecolor='black', alpha=0.5, linestyle='dashed', label=f'{class_name} Decoded')\n",
    "\n",
    "    axes[idx].set_title(f'Eigenvector {eigenvector_index+1}')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f3262",
   "metadata": {},
   "source": [
    "We have explored the reconstruction quality of the autoencoder using PCA-reduced data across different principal components. This approach provided insight into the distinctiveness and overlap of classes within a reduced feature space, which is valuable for understanding the potential performance of classification models trained on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf97c4",
   "metadata": {},
   "source": [
    "## III. AGN DC Experiment <a class=\"anchor\" id=\"four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f68eb64",
   "metadata": {},
   "source": [
    "### We Now Begin Exploring the AGN DC:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49ce124",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ab2cb",
   "metadata": {},
   "source": [
    "It is widely recommended to include the import statements for all the necessary modules at the beginning of a Jupyter Notebook or any Python program. \n",
    "This practice ensures that the required dependencies are properly imported and accessible at the required points in the code, thus avoiding any potential issues or errors related to missing modules or dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17329286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required modules\n",
    "\n",
    "# System modules allow Python programs to interact with the operating system and perform tasks \n",
    "# such as reading and writing files, managing processes, and accessing environment variables \n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pickle\n",
    "import argparse\n",
    "import itertools\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Data manipulation modules allow users to perform various operations on data,\n",
    "# such as cleaning, transforming, aggregating, filtering, and visualizing data\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization modules allow users to create visual representations of data\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import palettable\n",
    "import seaborn as sns\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "# pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "# Scikit-learn provides a range of supervised and unsupervised learning algorithms,\n",
    "# as well as tools for model selection and data preprocessing\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "# Scipy is a Python library for scientific computing and technical computing\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "# Astropy is a Python library for astronomy and astrophysics\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "# TensorFlow is an open-source machine learning library that provides an extensive set of tools and libraries\n",
    "# for building,training, and deploying neural networks, as well as other machine learning algorithms\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "# PyTorch is an open-source machine learning library for Python that provides a range of tools\n",
    "# and functions for building and training neural networks and other machine learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360110b",
   "metadata": {},
   "source": [
    "## IIIa. Data Acquisition <a class=\"anchor\" id=\"five\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc48707",
   "metadata": {},
   "source": [
    "Data acquisition involves the collection and aggregation of data from diverse sources. This crucial initial stage in the data analysis pipeline entails recognizing data sources and acquiring the data in a format suitable for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d937af",
   "metadata": {},
   "source": [
    "The provided statement establishes the data pathway. If an alternative data source is required, the line in the subsequent cell should be substituted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88377a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable named 'data_dir' and assigning it the string value /Users/ash/Research/Data/AGN_DataChallenge/ \n",
    "# This is the path to the directory where the dataset is stored on the local machine\n",
    "data_dir = '/Users/ash/Research/Data/AGN_DataChallenge/ObjectTable.parquet'\n",
    "\n",
    "# Using the display() function to display the value of the 'data_dir' variable in the output of the Jupyter Notebook\n",
    "display(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35415f4f",
   "metadata": {},
   "source": [
    "#### Data types\n",
    "\n",
    "The measurements can be classified into several key categories:\n",
    "- __Astrometry__ includes measurements of celestial coordinates such as right ascension (RA), declination (Dec), proper motion, and parallax.\n",
    "- __Photometry__ encompasses both point and extended source photometry, providing measurements in terms of AB magnitudes and fluxes (expressed in nJy).\n",
    "- __Color__ is determined by computing the ratios of fluxes in different wavelength bands.\n",
    "- __Morphology__ is indicated by a binary value, with 1 representing extended sources and 0 representing point-like sources.\n",
    "- __Light Curve Features__ are extracted from the SDSS light curves when a match is found.\n",
    "- __Redshift__ is provided whenever available, including both spectroscopic and photometric measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609d390",
   "metadata": {},
   "source": [
    "Inspecting the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_df = pd.read_parquet(data_dir)\n",
    "display(object_df.describe())\n",
    "display(object_df.shape)\n",
    "\n",
    "# Number of objects in each class + unlabeled\n",
    "display(object_df['class'].value_counts())\n",
    "display(\"Number of unlabeled objects: {}\".format(object_df['class'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be10a88d",
   "metadata": {},
   "source": [
    "Stripe 82 is a region of the sky that has been observed multiple times by the Sloan Digital Sky Survey (SDSS). It is located along the celestial equator and covers about 300 square degrees. Because Stripe 82 has been observed so many times, it has very deep imaging, which means it can detect fainter objects than a typical SDSS image.\n",
    "\n",
    "XMM-LSS stands for XMM-Large Scale Structure. It's an X-ray survey of the sky conducted by the XMM-Newton space telescope. It covers a region of about 11.1 square degrees and is designed to study large-scale cosmic structures like galaxy clusters and cosmic filaments.\n",
    "\n",
    "The merging of these datasets likely aims to provide a more comprehensive view of the studied celestial objects, combining data from both optical (Stripe 82) and X-ray (XMM-LSS) observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d8d61",
   "metadata": {},
   "source": [
    "#### Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e540c",
   "metadata": {},
   "source": [
    "#### Transforming Class Labels to Numerical Values\n",
    "\n",
    "For computational efficiency and to facilitate the use of algorithms that require numerical input, we convert the original class labels into numerical values. We assign:\n",
    "\n",
    "- `0` to `Star`\n",
    "- `1` to `Gal`\n",
    "- `2` to `Qso`, `Agn`, and `highZQso`\n",
    "\n",
    "This transformation also merges `Agn` and `highZQso` into the `Qso` class. This is done to simplify our classification problem into three distinct categories, which are now represented by the numbers 0, 1, and 2, respectively. The following code cell executes this transformation and provides a summary of the distribution of the transformed class labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace original class labels with numerical values for easier processing\n",
    "# 'Star' is assigned 0, 'Gal' is assigned 1, and 'Qso', 'Agn', 'highZQso' are assigned 2\n",
    "# Effectively, 'Agn' and 'highZQso' are merged into 'Qso' class\n",
    "## GTR: Better to explain in an markdown cell?\n",
    "# Ash: Explained the label encoding process and the merging of classes in the markdown cell above.\n",
    "\n",
    "object_df_new = object_df.replace({'class': {'Star': 0, 'Gal': 1, 'Qso': 2, 'Agn': 2, 'highZQso': 2}})\n",
    "\n",
    "# Display the shape of 'class' column in the new dataframe to see total instances\n",
    "# The shape will give us the total number of instances including labeled and unlabeled ones.\n",
    "display(object_df_new['class'].shape)\n",
    "\n",
    "# Count and display number of instances for each class (0, 1, 2)\n",
    "# This will help us understand the distribution of our labeled instances.\n",
    "display(object_df_new['class'].value_counts())\n",
    "\n",
    "# Calculate and display the number of instances in 'class' column with no label (NaN values)\n",
    "# This confirms the total number of unlabeled objects which may need further handling.\n",
    "display(\"Number of unlabeled objects: {}\".format(object_df_new['class'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793c95b",
   "metadata": {},
   "source": [
    "## IIIb. Exploratory Data Analysis <a class=\"anchor\" id=\"six\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430c616",
   "metadata": {},
   "source": [
    "Our goal is to be able to distinguish one type of astronomical source from another. Specifically, AGNs and quasars from stars (of various temperatures), normal (inactive) galaxies, exploding stars, etc. These different objects emit different amounts of light at the different wavelengths.\n",
    "\n",
    "We split the wavelength region into multiple bands, specifically for our dataset, the main source being SDSS- it is 5 bands: ugriz\n",
    "\n",
    "The relevant features in our dataset fluxes in each of the ugriz bandpasses: psFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_list = [\n",
    "    # Point source parallax and proper motion\n",
    "    'psParallax', 'psPm_ra', 'psPm_dec',\n",
    "\n",
    "    # Point source flux in different bands\n",
    "    'psFlux_u', 'psFlux_g', 'psFlux_r', 'psFlux_i', 'psFlux_z', 'psFlux_y',\n",
    "\n",
    "    # Point source flux errors in different bands\n",
    "    'psFluxErr_u', 'psFluxErr_g', 'psFluxErr_r', 'psFluxErr_i', 'psFluxErr_z', 'psFluxErr_y',\n",
    "\n",
    "    # Bright disk flux in different bands\n",
    "    'bdFlux_u', 'bdFlux_g', 'bdFlux_r', 'bdFlux_i', 'bdFlux_z', 'bdFlux_y',\n",
    "\n",
    "    # Bright disk flux errors in different bands\n",
    "    'bdFluxErr_u', 'bdFluxErr_g', 'bdFluxErr_r', 'bdFluxErr_i', 'bdFluxErr_z', 'bdFluxErr_y',\n",
    "\n",
    "    # Point source magnitude in different bands\n",
    "    'psMag_u', 'psMag_g', 'psMag_r', 'psMag_i', 'psMag_z', 'psMag_y',\n",
    "\n",
    "    # Point source magnitude errors in different bands\n",
    "    'psMagErr_u', 'psMagErr_g', 'psMagErr_r', 'psMagErr_i', 'psMagErr_z', 'psMagErr_y',\n",
    "\n",
    "    # Bright disk magnitude in different bands\n",
    "    'bdMag_u', 'bdMag_g', 'bdMag_r', 'bdMag_i', 'bdMag_z', 'bdMag_y',\n",
    "\n",
    "    # Bright disk magnitude errors in different bands\n",
    "    'bdMagErr_u', 'bdMagErr_g', 'bdMagErr_r', 'bdMagErr_i', 'bdMagErr_z', 'bdMagErr_y',\n",
    "\n",
    "    # Extendedness in different bands\n",
    "    'extendedness_u', 'extendedness_g', 'extendedness_r', 'extendedness_i', 'extendedness_z', 'extendedness_y',\n",
    "\n",
    "    # Standard colors and their errors\n",
    "    'stdColor_0', 'stdColor_1', 'stdColor_2', 'stdColor_3', 'stdColor_4', \n",
    "    'stdColorErr_0', 'stdColorErr_1', 'stdColorErr_2', 'stdColorErr_3', 'stdColorErr_4',\n",
    "\n",
    "    # Class, estimated photometric redshift, true redshift, and flags in different bands\n",
    "    'class', 'photoZ_pest', 'z','ebv', 'flags_u', 'flags_g', 'flags_r', 'flags_i', 'flags_z', 'flags_y',\n",
    "\n",
    "    # Spectroscopic identifiers\n",
    "    'spec_fiberid', 'spec_plate', 'spec_mjd',\n",
    "\n",
    "    # Light curve periodic features in different bands for different periodic indices\n",
    "    'lcPeriodic[0]_g',\n",
    "    'lcPeriodic[0]_r', 'lcPeriodic[0]_i', 'lcPeriodic[1]_g', 'lcPeriodic[1]_r', 'lcPeriodic[1]_i', \n",
    "    'lcPeriodic[2]_g','lcPeriodic[2]_r', 'lcPeriodic[2]_i', 'lcPeriodic[3]_g', 'lcPeriodic[3]_r', \n",
    "    'lcPeriodic[3]_i', 'lcPeriodic[4]_u','lcPeriodic[4]_g', 'lcPeriodic[4]_r', 'lcPeriodic[4]_i', \n",
    "    'lcPeriodic[4]_z', 'lcPeriodic[5]_u', 'lcPeriodic[5]_g','lcPeriodic[5]_r', 'lcPeriodic[5]_i', \n",
    "    'lcPeriodic[5]_z', 'lcPeriodic[6]_u', 'lcPeriodic[6]_g', 'lcPeriodic[6]_r','lcPeriodic[6]_i', \n",
    "    'lcPeriodic[6]_z', 'lcPeriodic[7]_u', 'lcPeriodic[7]_g', 'lcPeriodic[7]_r', 'lcPeriodic[7]_i',\n",
    "    'lcPeriodic[7]_z', 'lcPeriodic[8]_u', 'lcPeriodic[8]_g', 'lcPeriodic[8]_r', 'lcPeriodic[8]_i',\n",
    "    'lcPeriodic[8]_z','lcPeriodic[9]_u', 'lcPeriodic[9]_g', 'lcPeriodic[9]_r', 'lcPeriodic[9]_i', \n",
    "    'lcPeriodic[9]_z', 'lcPeriodic[10]_u','lcPeriodic[10]_g', 'lcPeriodic[10]_r', 'lcPeriodic[10]_i', \n",
    "    'lcPeriodic[10]_z', 'lcPeriodic[11]_u',  'lcPeriodic[11]_g', 'lcPeriodic[11]_r', 'lcPeriodic[11]_i', \n",
    "    'lcPeriodic[11]_z', 'lcPeriodic[12]_u', 'lcPeriodic[12]_g', 'lcPeriodic[12]_r', 'lcPeriodic[12]_i', \n",
    "    'lcPeriodic[12]_z', 'lcPeriodic[13]_u',  'lcPeriodic[13]_g', 'lcPeriodic[13]_r', 'lcPeriodic[13]_i', \n",
    "    'lcPeriodic[13]_z', 'lcPeriodic[14]_u', 'lcPeriodic[14]_g', 'lcPeriodic[14]_r', 'lcPeriodic[14]_i', \n",
    "    'lcPeriodic[14]_z', 'lcPeriodic[15]_u', 'lcPeriodic[15]_g', 'lcPeriodic[15]_r', 'lcPeriodic[15]_i', \n",
    "    'lcPeriodic[15]_z', 'lcPeriodic[16]_u',  'lcPeriodic[16]_g', 'lcPeriodic[16]_r', 'lcPeriodic[16]_i', \n",
    "    'lcPeriodic[16]_z', 'lcPeriodic[17]_u','lcPeriodic[17]_g', 'lcPeriodic[17]_r', 'lcPeriodic[17]_i', \n",
    "    'lcPeriodic[17]_z', 'lcPeriodic[18]_u','lcPeriodic[18]_g', 'lcPeriodic[18]_r', 'lcPeriodic[18]_i', \n",
    "    'lcPeriodic[18]_z', 'lcPeriodic[19]_u','lcPeriodic[19]_g', 'lcPeriodic[19]_r', 'lcPeriodic[19]_i', \n",
    "    'lcPeriodic[19]_z', 'lcPeriodic[20]_u','lcPeriodic[20]_g', 'lcPeriodic[20]_r', 'lcPeriodic[20]_i', \n",
    "    'lcPeriodic[20]_z', 'lcPeriodic[21]_u', 'lcPeriodic[21]_g', 'lcPeriodic[21]_r', 'lcPeriodic[21]_i', \n",
    "    'lcPeriodic[21]_z', 'lcPeriodic[22]_u', 'lcPeriodic[22]_g', 'lcPeriodic[22]_r', 'lcPeriodic[22]_i', \n",
    "    'lcPeriodic[22]_z', 'lcPeriodic[23]_u', 'lcPeriodic[23]_g', 'lcPeriodic[23]_r', 'lcPeriodic[23]_i', \n",
    "    'lcPeriodic[23]_z', 'lcPeriodic[24]_u', 'lcPeriodic[24]_g', 'lcPeriodic[24]_r', 'lcPeriodic[24]_i', \n",
    "    'lcPeriodic[24]_z', 'lcPeriodic[25]_u',  'lcPeriodic[25]_g', 'lcPeriodic[25]_r', 'lcPeriodic[25]_i',\n",
    "    'lcPeriodic[25]_z', 'lcPeriodic[26]_u',  'lcPeriodic[26]_g', 'lcPeriodic[26]_r', 'lcPeriodic[26]_i', \n",
    "    'lcPeriodic[26]_z', 'lcPeriodic[27]_u',  'lcPeriodic[27]_g', 'lcPeriodic[27]_r', 'lcPeriodic[27]_i',\n",
    "    'lcPeriodic[27]_z', 'lcPeriodic[28]_u', 'lcPeriodic[28]_g', 'lcPeriodic[28]_r', 'lcPeriodic[28]_i', \n",
    "    'lcPeriodic[28]_z', 'lcPeriodic[29]_u', 'lcPeriodic[29]_g', 'lcPeriodic[29]_r', 'lcPeriodic[29]_i', \n",
    "    'lcPeriodic[29]_z', 'lcPeriodic[30]_u', 'lcPeriodic[30]_g', 'lcPeriodic[30]_r', 'lcPeriodic[30]_i', \n",
    "    'lcPeriodic[30]_z', 'lcPeriodic[31]_u',  'lcPeriodic[31]_g', 'lcPeriodic[31]_r', 'lcPeriodic[31]_i',\n",
    "    'lcPeriodic[31]_z', 'lcPeriodic[32]_u', 'lcPeriodic[32]_g', 'lcPeriodic[32]_r', 'lcPeriodic[32]_i',\n",
    "    'lcPeriodic[32]_z',\n",
    "\n",
    "    # Light curve non-periodic features in different bands for different non-periodic indices\n",
    "    'lcNonPeriodic[0]_u',\n",
    "    'lcNonPeriodic[0]_g', 'lcNonPeriodic[0]_r', 'lcNonPeriodic[0]_i', 'lcNonPeriodic[0]_z', 'lcNonPeriodic[1]_u',\n",
    "    'lcNonPeriodic[1]_g', 'lcNonPeriodic[1]_r', 'lcNonPeriodic[1]_i', 'lcNonPeriodic[1]_z', 'lcNonPeriodic[2]_u',\n",
    "    'lcNonPeriodic[2]_g', 'lcNonPeriodic[2]_r', 'lcNonPeriodic[2]_i', 'lcNonPeriodic[2]_z', 'lcNonPeriodic[3]_u',\n",
    "    'lcNonPeriodic[3]_g', 'lcNonPeriodic[3]_r', 'lcNonPeriodic[3]_i', 'lcNonPeriodic[3]_z', 'lcNonPeriodic[4]_u',\n",
    "    'lcNonPeriodic[4]_g', 'lcNonPeriodic[4]_r', 'lcNonPeriodic[4]_i', 'lcNonPeriodic[4]_z', 'lcNonPeriodic[5]_u',\n",
    "    'lcNonPeriodic[5]_g', 'lcNonPeriodic[5]_r', 'lcNonPeriodic[5]_i', 'lcNonPeriodic[5]_z', 'lcNonPeriodic[6]_u',\n",
    "    'lcNonPeriodic[6]_g', 'lcNonPeriodic[6]_r', 'lcNonPeriodic[6]_i', 'lcNonPeriodic[6]_z', 'lcNonPeriodic[7]_u', \n",
    "    'lcNonPeriodic[7]_g', 'lcNonPeriodic[7]_r', 'lcNonPeriodic[7]_i', 'lcNonPeriodic[7]_z', 'lcNonPeriodic[8]_u',\n",
    "    'lcNonPeriodic[8]_g', 'lcNonPeriodic[8]_r', 'lcNonPeriodic[8]_i', 'lcNonPeriodic[8]_z', 'lcNonPeriodic[9]_u',\n",
    "    'lcNonPeriodic[9]_g', 'lcNonPeriodic[9]_r', 'lcNonPeriodic[9]_i', 'lcNonPeriodic[9]_z', 'lcNonPeriodic[10]_u',\n",
    "    'lcNonPeriodic[10]_g', 'lcNonPeriodic[10]_r', 'lcNonPeriodic[10]_i', 'lcNonPeriodic[10]_z',\n",
    "    'lcNonPeriodic[11]_u', 'lcNonPeriodic[11]_g', 'lcNonPeriodic[11]_r', 'lcNonPeriodic[11]_i',\n",
    "    'lcNonPeriodic[11]_z', 'lcNonPeriodic[12]_u', 'lcNonPeriodic[12]_g', 'lcNonPeriodic[12]_r', \n",
    "    'lcNonPeriodic[12]_i', 'lcNonPeriodic[12]_z', 'lcNonPeriodic[13]_u', 'lcNonPeriodic[13]_g',\n",
    "    'lcNonPeriodic[13]_r', 'lcNonPeriodic[13]_i', 'lcNonPeriodic[13]_z', 'lcNonPeriodic[14]_u', \n",
    "    'lcNonPeriodic[14]_g', 'lcNonPeriodic[14]_r', 'lcNonPeriodic[14]_i', 'lcNonPeriodic[14]_z', \n",
    "    'lcNonPeriodic[15]_u', 'lcNonPeriodic[15]_g', 'lcNonPeriodic[15]_r', 'lcNonPeriodic[15]_i', \n",
    "    'lcNonPeriodic[15]_z', 'lcNonPeriodic[16]_u', 'lcNonPeriodic[16]_g', 'lcNonPeriodic[16]_r', \n",
    "    'lcNonPeriodic[16]_i', 'lcNonPeriodic[16]_z', 'lcNonPeriodic[17]_u', 'lcNonPeriodic[17]_g', \n",
    "    'lcNonPeriodic[17]_r', 'lcNonPeriodic[17]_i', 'lcNonPeriodic[17]_z', 'lcNonPeriodic[18]_u',\n",
    "    'lcNonPeriodic[18]_g', 'lcNonPeriodic[18]_r', 'lcNonPeriodic[18]_i', 'lcNonPeriodic[18]_z',\n",
    "    'lcNonPeriodic[19]_u', 'lcNonPeriodic[19]_g', 'lcNonPeriodic[19]_r', 'lcNonPeriodic[19]_i', \n",
    "    'lcNonPeriodic[19]_z', 'lcNonPeriodic[20]_u', 'lcNonPeriodic[20]_g', 'lcNonPeriodic[20]_r', \n",
    "    'lcNonPeriodic[20]_i', 'lcNonPeriodic[20]_z', 'lcNonPeriodic[21]_u', 'lcNonPeriodic[21]_g',\n",
    "    'lcNonPeriodic[21]_r', 'lcNonPeriodic[21]_i', 'lcNonPeriodic[21]_z', 'lcNonPeriodic[22]_u', \n",
    "    'lcNonPeriodic[22]_g', 'lcNonPeriodic[22]_r', 'lcNonPeriodic[22]_i', 'lcNonPeriodic[22]_z', \n",
    "    'lcNonPeriodic[23]_u', 'lcNonPeriodic[23]_g', 'lcNonPeriodic[23]_r', 'lcNonPeriodic[23]_i', \n",
    "    'lcNonPeriodic[23]_z', 'lcNonPeriodic[24]_u', 'lcNonPeriodic[24]_g', 'lcNonPeriodic[24]_r',\n",
    "    'lcNonPeriodic[24]_i', 'lcNonPeriodic[24]_z', 'lcNonPeriodic[25]_u', 'lcNonPeriodic[25]_g', \n",
    "    'lcNonPeriodic[25]_r', 'lcNonPeriodic[25]_i', 'lcNonPeriodic[25]_z', 'lcNonPeriodic[26]_u',\n",
    "    'lcNonPeriodic[26]_g', 'lcNonPeriodic[26]_r', 'lcNonPeriodic[26]_i', 'lcNonPeriodic[26]_z', \n",
    "    'lcNonPeriodic[27]_u', 'lcNonPeriodic[27]_g', 'lcNonPeriodic[27]_r', 'lcNonPeriodic[27]_i', \n",
    "    'lcNonPeriodic[27]_z', 'lcNonPeriodic[28]_u', 'lcNonPeriodic[28]_g', 'lcNonPeriodic[28]_r',\n",
    "    'lcNonPeriodic[28]_i','lcNonPeriodic[28]_z'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_noerror = [\n",
    "    # Point source parallax and proper motion\n",
    "    'psParallax', 'psPm_ra', 'psPm_dec',\n",
    "\n",
    "    # Point source flux in different bands\n",
    "    'psFlux_u', 'psFlux_g', 'psFlux_r', 'psFlux_i', 'psFlux_z', 'psFlux_y',\n",
    "\n",
    "    # Bright disk flux in different bands\n",
    "    'bdFlux_u', 'bdFlux_g', 'bdFlux_r', 'bdFlux_i', 'bdFlux_z', 'bdFlux_y',\n",
    "\n",
    "    # Point source magnitude in different bands\n",
    "    'psMag_u', 'psMag_g', 'psMag_r', 'psMag_i', 'psMag_z', 'psMag_y',\n",
    "\n",
    "    # Bright disk magnitude in different bands\n",
    "    'bdMag_u', 'bdMag_g', 'bdMag_r', 'bdMag_i', 'bdMag_z', 'bdMag_y',\n",
    "\n",
    "    # Extendedness in different bands\n",
    "    'extendedness_u', 'extendedness_g', 'extendedness_r', 'extendedness_i', 'extendedness_z', 'extendedness_y',\n",
    "\n",
    "    # Standard colors\n",
    "    'stdColor_0', 'stdColor_1', 'stdColor_2', 'stdColor_3', 'stdColor_4',\n",
    "\n",
    "    # Class, estimated photometric redshift, true redshift, and flags in different bands\n",
    "    'class', 'photoZ_pest', 'z','ebv', 'flags_u', 'flags_g', 'flags_r', 'flags_i', 'flags_z', 'flags_y',\n",
    "\n",
    "    # Spectroscopic identifiers\n",
    "    'spec_fiberid', 'spec_plate', 'spec_mjd',\n",
    "\n",
    "    # Light curve periodic features in different bands for different periodic indices\n",
    "    'lcPeriodic[0]_g',\n",
    "    'lcPeriodic[0]_r', 'lcPeriodic[0]_i', 'lcPeriodic[1]_g', 'lcPeriodic[1]_r', 'lcPeriodic[1]_i', \n",
    "    'lcPeriodic[2]_g','lcPeriodic[2]_r', 'lcPeriodic[2]_i', 'lcPeriodic[3]_g', 'lcPeriodic[3]_r', \n",
    "    'lcPeriodic[3]_i', 'lcPeriodic[4]_u','lcPeriodic[4]_g', 'lcPeriodic[4]_r', 'lcPeriodic[4]_i', \n",
    "    'lcPeriodic[4]_z', 'lcPeriodic[5]_u', 'lcPeriodic[5]_g','lcPeriodic[5]_r', 'lcPeriodic[5]_i', \n",
    "    'lcPeriodic[5]_z', 'lcPeriodic[6]_u', 'lcPeriodic[6]_g', 'lcPeriodic[6]_r','lcPeriodic[6]_i', \n",
    "    'lcPeriodic[6]_z', 'lcPeriodic[7]_u', 'lcPeriodic[7]_g', 'lcPeriodic[7]_r', 'lcPeriodic[7]_i',\n",
    "    'lcPeriodic[7]_z', 'lcPeriodic[8]_u', 'lcPeriodic[8]_g', 'lcPeriodic[8]_r', 'lcPeriodic[8]_i',\n",
    "    'lcPeriodic[8]_z','lcPeriodic[9]_u', 'lcPeriodic[9]_g', 'lcPeriodic[9]_r', 'lcPeriodic[9]_i', \n",
    "    'lcPeriodic[9]_z', 'lcPeriodic[10]_u','lcPeriodic[10]_g', 'lcPeriodic[10]_r', 'lcPeriodic[10]_i', \n",
    "    'lcPeriodic[10]_z', 'lcPeriodic[11]_u',  'lcPeriodic[11]_g', 'lcPeriodic[11]_r', 'lcPeriodic[11]_i', \n",
    "    'lcPeriodic[11]_z', 'lcPeriodic[12]_u', 'lcPeriodic[12]_g', 'lcPeriodic[12]_r', 'lcPeriodic[12]_i', \n",
    "    'lcPeriodic[12]_z', 'lcPeriodic[13]_u',  'lcPeriodic[13]_g', 'lcPeriodic[13]_r', 'lcPeriodic[13]_i', \n",
    "    'lcPeriodic[13]_z', 'lcPeriodic[14]_u', 'lcPeriodic[14]_g', 'lcPeriodic[14]_r', 'lcPeriodic[14]_i', \n",
    "    'lcPeriodic[14]_z', 'lcPeriodic[15]_u', 'lcPeriodic[15]_g', 'lcPeriodic[15]_r', 'lcPeriodic[15]_i', \n",
    "    'lcPeriodic[15]_z', 'lcPeriodic[16]_u',  'lcPeriodic[16]_g', 'lcPeriodic[16]_r', 'lcPeriodic[16]_i', \n",
    "    'lcPeriodic[16]_z', 'lcPeriodic[17]_u','lcPeriodic[17]_g', 'lcPeriodic[17]_r', 'lcPeriodic[17]_i', \n",
    "    'lcPeriodic[17]_z', 'lcPeriodic[18]_u','lcPeriodic[18]_g', 'lcPeriodic[18]_r', 'lcPeriodic[18]_i', \n",
    "    'lcPeriodic[18]_z', 'lcPeriodic[19]_u','lcPeriodic[19]_g', 'lcPeriodic[19]_r', 'lcPeriodic[19]_i', \n",
    "    'lcPeriodic[19]_z', 'lcPeriodic[20]_u','lcPeriodic[20]_g', 'lcPeriodic[20]_r', 'lcPeriodic[20]_i', \n",
    "    'lcPeriodic[20]_z', 'lcPeriodic[21]_u', 'lcPeriodic[21]_g', 'lcPeriodic[21]_r', 'lcPeriodic[21]_i', \n",
    "    'lcPeriodic[21]_z', 'lcPeriodic[22]_u', 'lcPeriodic[22]_g', 'lcPeriodic[22]_r', 'lcPeriodic[22]_i', \n",
    "    'lcPeriodic[22]_z', 'lcPeriodic[23]_u', 'lcPeriodic[23]_g', 'lcPeriodic[23]_r', 'lcPeriodic[23]_i', \n",
    "    'lcPeriodic[23]_z', 'lcPeriodic[24]_u', 'lcPeriodic[24]_g', 'lcPeriodic[24]_r', 'lcPeriodic[24]_i', \n",
    "    'lcPeriodic[24]_z', 'lcPeriodic[25]_u',  'lcPeriodic[25]_g', 'lcPeriodic[25]_r', 'lcPeriodic[25]_i',\n",
    "    'lcPeriodic[25]_z', 'lcPeriodic[26]_u',  'lcPeriodic[26]_g', 'lcPeriodic[26]_r', 'lcPeriodic[26]_i', \n",
    "    'lcPeriodic[26]_z', 'lcPeriodic[27]_u',  'lcPeriodic[27]_g', 'lcPeriodic[27]_r', 'lcPeriodic[27]_i',\n",
    "    'lcPeriodic[27]_z', 'lcPeriodic[28]_u', 'lcPeriodic[28]_g', 'lcPeriodic[28]_r', 'lcPeriodic[28]_i', \n",
    "    'lcPeriodic[28]_z', 'lcPeriodic[29]_u', 'lcPeriodic[29]_g', 'lcPeriodic[29]_r', 'lcPeriodic[29]_i', \n",
    "    'lcPeriodic[29]_z', 'lcPeriodic[30]_u', 'lcPeriodic[30]_g', 'lcPeriodic[30]_r', 'lcPeriodic[30]_i', \n",
    "    'lcPeriodic[30]_z', 'lcPeriodic[31]_u',  'lcPeriodic[31]_g', 'lcPeriodic[31]_r', 'lcPeriodic[31]_i',\n",
    "    'lcPeriodic[31]_z', 'lcPeriodic[32]_u', 'lcPeriodic[32]_g', 'lcPeriodic[32]_r', 'lcPeriodic[32]_i',\n",
    "    'lcPeriodic[32]_z',\n",
    "\n",
    "    # Light curve non-periodic features in different bands for different non-periodic indices\n",
    "    'lcNonPeriodic[0]_u',\n",
    "    'lcNonPeriodic[0]_g', 'lcNonPeriodic[0]_r', 'lcNonPeriodic[0]_i', 'lcNonPeriodic[0]_z', 'lcNonPeriodic[1]_u',\n",
    "    'lcNonPeriodic[1]_g', 'lcNonPeriodic[1]_r', 'lcNonPeriodic[1]_i', 'lcNonPeriodic[1]_z', 'lcNonPeriodic[2]_u',\n",
    "    'lcNonPeriodic[2]_g', 'lcNonPeriodic[2]_r', 'lcNonPeriodic[2]_i', 'lcNonPeriodic[2]_z', 'lcNonPeriodic[3]_u',\n",
    "    'lcNonPeriodic[3]_g', 'lcNonPeriodic[3]_r', 'lcNonPeriodic[3]_i', 'lcNonPeriodic[3]_z', 'lcNonPeriodic[4]_u',\n",
    "    'lcNonPeriodic[4]_g', 'lcNonPeriodic[4]_r', 'lcNonPeriodic[4]_i', 'lcNonPeriodic[4]_z', 'lcNonPeriodic[5]_u',\n",
    "    'lcNonPeriodic[5]_g', 'lcNonPeriodic[5]_r', 'lcNonPeriodic[5]_i', 'lcNonPeriodic[5]_z', 'lcNonPeriodic[6]_u',\n",
    "    'lcNonPeriodic[6]_g', 'lcNonPeriodic[6]_r', 'lcNonPeriodic[6]_i', 'lcNonPeriodic[6]_z', 'lcNonPeriodic[7]_u', \n",
    "    'lcNonPeriodic[7]_g', 'lcNonPeriodic[7]_r', 'lcNonPeriodic[7]_i', 'lcNonPeriodic[7]_z', 'lcNonPeriodic[8]_u',\n",
    "    'lcNonPeriodic[8]_g', 'lcNonPeriodic[8]_r', 'lcNonPeriodic[8]_i', 'lcNonPeriodic[8]_z', 'lcNonPeriodic[9]_u',\n",
    "    'lcNonPeriodic[9]_g', 'lcNonPeriodic[9]_r', 'lcNonPeriodic[9]_i', 'lcNonPeriodic[9]_z', 'lcNonPeriodic[10]_u',\n",
    "    'lcNonPeriodic[10]_g', 'lcNonPeriodic[10]_r', 'lcNonPeriodic[10]_i', 'lcNonPeriodic[10]_z',\n",
    "    'lcNonPeriodic[11]_u', 'lcNonPeriodic[11]_g', 'lcNonPeriodic[11]_r', 'lcNonPeriodic[11]_i',\n",
    "    'lcNonPeriodic[11]_z', 'lcNonPeriodic[12]_u', 'lcNonPeriodic[12]_g', 'lcNonPeriodic[12]_r', \n",
    "    'lcNonPeriodic[12]_i', 'lcNonPeriodic[12]_z', 'lcNonPeriodic[13]_u', 'lcNonPeriodic[13]_g',\n",
    "    'lcNonPeriodic[13]_r', 'lcNonPeriodic[13]_i', 'lcNonPeriodic[13]_z', 'lcNonPeriodic[14]_u', \n",
    "    'lcNonPeriodic[14]_g', 'lcNonPeriodic[14]_r', 'lcNonPeriodic[14]_i', 'lcNonPeriodic[14]_z', \n",
    "    'lcNonPeriodic[15]_u', 'lcNonPeriodic[15]_g', 'lcNonPeriodic[15]_r', 'lcNonPeriodic[15]_i', \n",
    "    'lcNonPeriodic[15]_z', 'lcNonPeriodic[16]_u', 'lcNonPeriodic[16]_g', 'lcNonPeriodic[16]_r', \n",
    "    'lcNonPeriodic[16]_i', 'lcNonPeriodic[16]_z', 'lcNonPeriodic[17]_u', 'lcNonPeriodic[17]_g', \n",
    "    'lcNonPeriodic[17]_r', 'lcNonPeriodic[17]_i', 'lcNonPeriodic[17]_z', 'lcNonPeriodic[18]_u',\n",
    "    'lcNonPeriodic[18]_g', 'lcNonPeriodic[18]_r', 'lcNonPeriodic[18]_i', 'lcNonPeriodic[18]_z',\n",
    "    'lcNonPeriodic[19]_u', 'lcNonPeriodic[19]_g', 'lcNonPeriodic[19]_r', 'lcNonPeriodic[19]_i', \n",
    "    'lcNonPeriodic[19]_z', 'lcNonPeriodic[20]_u', 'lcNonPeriodic[20]_g', 'lcNonPeriodic[20]_r', \n",
    "    'lcNonPeriodic[20]_i', 'lcNonPeriodic[20]_z', 'lcNonPeriodic[21]_u', 'lcNonPeriodic[21]_g',\n",
    "    'lcNonPeriodic[21]_r', 'lcNonPeriodic[21]_i', 'lcNonPeriodic[21]_z', 'lcNonPeriodic[22]_u', \n",
    "    'lcNonPeriodic[22]_g', 'lcNonPeriodic[22]_r', 'lcNonPeriodic[22]_i', 'lcNonPeriodic[22]_z', \n",
    "    'lcNonPeriodic[23]_u', 'lcNonPeriodic[23]_g', 'lcNonPeriodic[23]_r', 'lcNonPeriodic[23]_i', \n",
    "    'lcNonPeriodic[23]_z', 'lcNonPeriodic[24]_u', 'lcNonPeriodic[24]_g', 'lcNonPeriodic[24]_r',\n",
    "    'lcNonPeriodic[24]_i', 'lcNonPeriodic[24]_z', 'lcNonPeriodic[25]_u', 'lcNonPeriodic[25]_g', \n",
    "    'lcNonPeriodic[25]_r', 'lcNonPeriodic[25]_i', 'lcNonPeriodic[25]_z', 'lcNonPeriodic[26]_u',\n",
    "    'lcNonPeriodic[26]_g', 'lcNonPeriodic[26]_r', 'lcNonPeriodic[26]_i', 'lcNonPeriodic[26]_z', \n",
    "    'lcNonPeriodic[27]_u', 'lcNonPeriodic[27]_g', 'lcNonPeriodic[27]_r', 'lcNonPeriodic[27]_i', \n",
    "    'lcNonPeriodic[27]_z', 'lcNonPeriodic[28]_u', 'lcNonPeriodic[28]_g', 'lcNonPeriodic[28]_r',\n",
    "    'lcNonPeriodic[28]_i','lcNonPeriodic[28]_z'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88379579",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_nolc = [\n",
    "    # Point source parallax and proper motion\n",
    "    'psParallax', 'psPm_ra', 'psPm_dec',\n",
    "\n",
    "    # Point source flux in different bands\n",
    "    'psFlux_u', 'psFlux_g', 'psFlux_r', 'psFlux_i', 'psFlux_z', 'psFlux_y',\n",
    "\n",
    "    # Point source flux errors in different bands\n",
    "    'psFluxErr_u', 'psFluxErr_g', 'psFluxErr_r', 'psFluxErr_i', 'psFluxErr_z', 'psFluxErr_y',\n",
    "\n",
    "    # Bright disk flux in different bands\n",
    "    'bdFlux_u', 'bdFlux_g', 'bdFlux_r', 'bdFlux_i', 'bdFlux_z', 'bdFlux_y',\n",
    "\n",
    "    # Bright disk flux errors in different bands\n",
    "    'bdFluxErr_u', 'bdFluxErr_g', 'bdFluxErr_r', 'bdFluxErr_i', 'bdFluxErr_z', 'bdFluxErr_y',\n",
    "\n",
    "    # Point source magnitude in different bands\n",
    "    'psMag_u', 'psMag_g', 'psMag_r', 'psMag_i', 'psMag_z', 'psMag_y',\n",
    "\n",
    "    # Point source magnitude errors in different bands\n",
    "    'psMagErr_u', 'psMagErr_g', 'psMagErr_r', 'psMagErr_i', 'psMagErr_z', 'psMagErr_y',\n",
    "\n",
    "    # Bright disk magnitude in different bands\n",
    "    'bdMag_u', 'bdMag_g', 'bdMag_r', 'bdMag_i', 'bdMag_z', 'bdMag_y',\n",
    "\n",
    "    # Bright disk magnitude errors in different bands\n",
    "    'bdMagErr_u', 'bdMagErr_g', 'bdMagErr_r', 'bdMagErr_i', 'bdMagErr_z', 'bdMagErr_y',\n",
    "\n",
    "    # Extendedness in different bands\n",
    "    'extendedness_u', 'extendedness_g', 'extendedness_r', 'extendedness_i', 'extendedness_z', 'extendedness_y',\n",
    "\n",
    "    # Standard colors and their errors\n",
    "    'stdColor_0', 'stdColor_1', 'stdColor_2', 'stdColor_3', 'stdColor_4', \n",
    "    'stdColorErr_0', 'stdColorErr_1', 'stdColorErr_2', 'stdColorErr_3', 'stdColorErr_4',\n",
    "\n",
    "    # Class, estimated photometric redshift, true redshift, and flags in different bands\n",
    "    'class', 'photoZ_pest', 'z','ebv', 'flags_u', 'flags_g', 'flags_r', 'flags_i', 'flags_z', 'flags_y',\n",
    "\n",
    "    # Spectroscopic identifiers\n",
    "    'spec_fiberid', 'spec_plate', 'spec_mjd'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a164a",
   "metadata": {},
   "source": [
    "#### Color-Color Plots\n",
    "\n",
    "The color-color plots are created before passing data into the autoencoder to provide an exploratory visualization of the data distributions and potential correlations or clusters. These plots can also help identify any noticeable patterns, anomalies, or trends in the data across different categories (Star, Galaxy, Quasar), which could influence how the autoencoder learns to represent the data.\n",
    "\n",
    "Define separate data frames for the stars, galaxies, and quasars (and unlabeled objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_list = ['psMag_i', 'psPm_ra', 'psPm_dec',\\\n",
    "                'stdColor_0', 'stdColor_1', 'stdColor_2', 'stdColor_3',\\\n",
    "                'lcNonPeriodic[15]_i', 'lcNonPeriodic[26]_i',\\\n",
    "                'class','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7490698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for 'Star', 'Gal', and 'Quasar' objects and remove rows with any missing values\n",
    "object_df_new_Star = object_df_new[short_list].loc[(object_df_new['class'] == 0)].dropna()\n",
    "object_df_new_Gal = object_df_new[short_list].loc[(object_df_new['class'] == 1)].dropna()\n",
    "object_df_new_Quasar = object_df_new[short_list].loc[(object_df_new['class'] == 2)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where the 'class' is unlabeled (NaN) and drop the 'class' and 'z' columns\n",
    "object_df_new_Unlab = object_df_new[short_list][object_df_new['class'].isna()]\n",
    "object_df_new_Unlab.drop(['class','z'],axis=1,inplace=True)\n",
    "object_df_new_Unlab.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bbd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the number of instances in each class-specific dataset and in the unlabeled dataset\n",
    "print(object_df_new_Star.shape[0])\n",
    "print(object_df_new_Gal.shape[0])\n",
    "print(object_df_new_Quasar.shape[0])\n",
    "print(object_df_new_Unlab.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695791b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that will be used to make plot similar to scatter_dot in SM\n",
    "# Thus contour+dot plot style is necessary given the data volume and also the importance of outliers\n",
    "\n",
    "def contour_scatter(x, y, exkernel=None, ax=None, color='C0', cmap='Blues_r',\n",
    "                    lims=None, levels=None, scatter=True, nlevel=1,\n",
    "                    kwargs_contour={},\n",
    "                    kwargs_plot={}):\n",
    "    \"\"\" Contour and scatter plot with no points inside contours \"\"\"\n",
    "\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # fig, ax = plt.subplots(1)\n",
    "    if lims is None:\n",
    "        xmin = x.min()\n",
    "        xmax = x.max()\n",
    "        ymin = y.min()\n",
    "        ymax = y.max()\n",
    "    else:\n",
    "        xmin, xmax, ymin, ymax = lims\n",
    "\n",
    "    X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    positions = np.vstack([X.ravel(), Y.ravel()])\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    Z = np.reshape(kernel(positions).T, X.shape)\n",
    "\n",
    "    CS = ax.contour(X,\n",
    "                    Y,\n",
    "                    Z,\n",
    "                    cmap=cmap,\n",
    "                    levels=levels,\n",
    "                    **kwargs_contour)\n",
    "\n",
    "    if scatter is True:\n",
    "        #print(CS.levels)\n",
    "        threshold = CS.levels[nlevel] # nlevel=1 or 0 depending on Python version?\n",
    "        z = kernel(values)\n",
    "\n",
    "        # mask points above density threshold\n",
    "        x = np.ma.array(x)\n",
    "        y = np.ma.array(y)\n",
    "        if exkernel:\n",
    "            exz = exkernel(values)\n",
    "            x[exz > threshold] = np.ma.masked\n",
    "            y[exz > threshold] = np.ma.masked\n",
    "\n",
    "        x[z > threshold] = np.ma.masked\n",
    "        y[z > threshold] = np.ma.masked\n",
    "\n",
    "        ax.scatter(x, y, color=color, **kwargs_plot)\n",
    "\n",
    "    if exkernel is False:\n",
    "        return kernel\n",
    "    \n",
    "    \n",
    "#4-class Dark\n",
    "csdark = palettable.colorbrewer.qualitative.Dark2_4.mpl_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "limsugr = np.array([[-2,8], [-2,4]])\n",
    "limsugr = limsugr.flatten()\n",
    "limsgri = np.array([[-2,4], [-2,3.5]])\n",
    "limsgri = limsgri.flatten()\n",
    "limsriz = np.array([[-1,2.5], [-1,1.5]])\n",
    "limsriz = limsriz.flatten()\n",
    "limsriimag = np.array([[-1,2.5], [14,22]])\n",
    "limsriimag = limsriimag.flatten()\n",
    "levels = None\n",
    "nlevel=1\n",
    "\n",
    "# Make the figure:\n",
    "#fig,ax = plt.figure(figsize=(8,8))\n",
    "fig, ax = plt.subplots(2,2,figsize=(12,12))\n",
    "\n",
    "\n",
    "handles, labels = (0, 0)\n",
    "\n",
    "    \n",
    "for i, axis in enumerate(ax.ravel()):\n",
    "    print(i)\n",
    "\n",
    "    if i==0: \n",
    "        contour_scatter(object_df_new_Star['stdColor_0'], object_df_new_Star['stdColor_1'], lims=limsugr, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3, 'label': 'Star'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Gal['stdColor_0'], object_df_new_Gal['stdColor_1'], lims=limsugr, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3, 'label': 'Galaxy'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Quasar['stdColor_0'], object_df_new_Quasar['stdColor_1'], lims=limsugr, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3, 'label': 'Quasar'}, ax=axis)\n",
    "        #Do the unlabeled in another set of plots to avoid confusion\n",
    "        #contour_scatter(object_df_new_Unlab['stdColor_0'], object_df_new_Unlab['stdColor_1'], lims=limsugr, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(-0.8,5)\n",
    "        axis.set_ylim(-0.7,2.5)\n",
    "        axis.set_xlabel('u-g')\n",
    "        axis.set_ylabel('g-r')\n",
    "        axis.legend(loc='upper right')\n",
    "        axis.set_box_aspect(1)\n",
    "\n",
    "    if i==1:\n",
    "        contour_scatter(object_df_new_Star['stdColor_1'], object_df_new_Star['stdColor_2'], lims=limsgri, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3, 'label': 'Star'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Gal['stdColor_1'], object_df_new_Gal['stdColor_2'], lims=limsgri, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3,'label': 'Galaxy'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Quasar['stdColor_1'], object_df_new_Quasar['stdColor_2'], lims=limsgri, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3, 'label': 'Quasar'}, ax=axis)\n",
    "        #contour_scatter(object_df_new_Unlab['stdColor_1'], object_df_new_Unlab['stdColor_2'], lims=limsgri, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(-0.7,2.5)\n",
    "        axis.set_ylim(-0.8,2.2)\n",
    "        axis.set_xlabel('g-r')\n",
    "        axis.set_ylabel('r-i')\n",
    "        axis.legend(loc='upper right')\n",
    "        axis.set_box_aspect(1)\n",
    "        \n",
    "    if i==2:\n",
    "        contour_scatter(object_df_new_Star['stdColor_2'], object_df_new_Star['stdColor_3'], lims=limsriz, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3, 'label': 'Star'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Gal['stdColor_2'], object_df_new_Gal['stdColor_3'], lims=limsriz, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3,'label': 'Galaxy'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Quasar['stdColor_2'], object_df_new_Quasar['stdColor_3'], lims=limsriz, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3, 'label': 'Quasar'}, ax=axis)\n",
    "        #contour_scatter(object_df_new_Unlab['stdColor_2'], object_df_new_Unlab['stdColor_3'], lims=limsriz, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(-0.8,2.3)\n",
    "        axis.set_ylim(-0.8,1.3)\n",
    "        axis.set_xlabel('r-i')\n",
    "        axis.set_ylabel('i-z')\n",
    "        axis.legend(loc='upper right')\n",
    "        axis.set_box_aspect(1)\n",
    "        \n",
    "    if i==3:\n",
    "        contour_scatter(object_df_new_Star['stdColor_2'], object_df_new_Star['psMag_i'], lims=limsriimag, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3, 'label': 'Star'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Gal['stdColor_2'], object_df_new_Gal['psMag_i'], lims=limsriimag, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3,'label': 'Galaxy'}, ax=axis)\n",
    "        contour_scatter(object_df_new_Quasar['stdColor_2'], object_df_new_Quasar['psMag_i'], lims=limsriimag, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3,'label': 'Quasar'}, ax=axis)\n",
    "        #contour_scatter(object_df_new_Unlab['stdColor_2'], object_df_new_Unlab['psMag_i'], lims=limsriimag, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(-0.8,2.3)\n",
    "        axis.set_ylim(14,22)\n",
    "        axis.set_xlabel('r-i')\n",
    "        axis.set_ylabel('psfMag i')\n",
    "        axis.legend(loc='upper right')\n",
    "        axis.set_box_aspect(1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f0dbab5",
   "metadata": {},
   "source": [
    "# Assuming 'csdark' is a list of color codes for the plots\n",
    "csdark = ['#FFA07A', '#20B2AA', '#778899']  # Replace with actual colors\n",
    "\n",
    "# Dummy data - replace with actual data frames and feature names\n",
    "object_df_new_Star = {'feature_good': np.random.normal(0, 1, 1000)}\n",
    "object_df_new_Gal = {'feature_good': np.random.normal(2, 1, 1000)}\n",
    "object_df_new_Quasar = {'feature_good': np.random.normal(4, 1, 1000)}\n",
    "\n",
    "object_df_new_Star.update({'feature_mixed1': np.random.normal(2, 2, 1000)})\n",
    "object_df_new_Gal.update({'feature_mixed1': np.random.normal(2, 2, 1000)})\n",
    "object_df_new_Quasar.update({'feature_mixed1': np.random.normal(2, 2, 1000)})\n",
    "\n",
    "object_df_new_Star.update({'feature_mixed2': np.random.normal(5, 3, 1000)})\n",
    "object_df_new_Gal.update({'feature_mixed2': np.random.normal(5, 3, 1000)})\n",
    "object_df_new_Quasar.update({'feature_mixed2': np.random.normal(5, 3, 1000)})\n",
    "\n",
    "# Plotting the 1-D histograms for each feature and class\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5), tight_layout=True)\n",
    "\n",
    "def plot_1d_histogram(data, ax, colors, labels):\n",
    "    for i, (label, d) in enumerate(data.items()):\n",
    "        ax.hist(d, bins=30, color=colors[i], alpha=0.6, label=label)\n",
    "    ax.set_yticks([])\n",
    "    ax.legend()\n",
    "\n",
    "# A feature where all three classes are well-separated.\n",
    "plot_1d_histogram({\n",
    "    'Star': object_df_new_Star['feature_good'],\n",
    "    'Galaxy': object_df_new_Gal['feature_good'],\n",
    "    'Quasar': object_df_new_Quasar['feature_good']\n",
    "}, axs[0], csdark, ['Star', 'Galaxy', 'Quasar'])\n",
    "\n",
    "# A feature where two classes are separable, and one is mixed.\n",
    "plot_1d_histogram({\n",
    "    'Star': object_df_new_Star['feature_mixed1'],\n",
    "    'Galaxy': object_df_new_Gal['feature_mixed1'],\n",
    "    'Quasar': object_df_new_Quasar['feature_mixed1']\n",
    "}, axs[1], csdark, ['Star', 'Galaxy', 'Quasar'])\n",
    "\n",
    "# A feature where all three classes are mixed.\n",
    "plot_1d_histogram({\n",
    "    'Star': object_df_new_Star['feature_mixed2'],\n",
    "    'Galaxy': object_df_new_Gal['feature_mixed2'],\n",
    "    'Quasar': object_df_new_Quasar['feature_mixed2']\n",
    "}, axs[2], csdark, ['Star', 'Galaxy', 'Quasar'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "049a593d",
   "metadata": {},
   "source": [
    "# Dummy data for 2-D plots - replace with actual data frames and feature names\n",
    "object_df_new_Star.update({'feature_x': np.random.normal(0, 1, 1000), 'feature_y': np.random.normal(0, 1, 1000)})\n",
    "object_df_new_Gal.update({'feature_x': np.random.normal(2, 1, 1000), 'feature_y': np.random.normal(2, 1, 1000)})\n",
    "object_df_new_Quasar.update({'feature_x': np.random.normal(4, 1, 1000), 'feature_y': np.random.normal(5, 1, 1000)})\n",
    "\n",
    "# 2-D scatter plot function\n",
    "def plot_2d_scatter(data, ax, colors, labels):\n",
    "    for i, (label, (x, y)) in enumerate(data.items()):\n",
    "        ax.scatter(x, y, color=colors[i], alpha=0.6, label=label, edgecolors='w')\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Using multiple features to distinguish the classes better\n",
    "plot_2d_scatter({\n",
    "    'Star': (object_df_new_Star['feature_x'], object_df_new_Star['feature_y']),\n",
    "    'Galaxy': (object_df_new_Gal['feature_x'], object_df_new_Gal['feature_y']),\n",
    "    'Quasar': (object_df_new_Quasar['feature_x'], object_df_new_Quasar['feature_y'])\n",
    "}, ax, csdark, ['Star', 'Galaxy', 'Quasar'])\n",
    "\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Feature Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9094cd",
   "metadata": {},
   "source": [
    "## IIIc. Data Processing <a class=\"anchor\" id=\"seven\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d6c727",
   "metadata": {},
   "source": [
    "Data processing involves converting unprocessed data into a format that is appropriate for analysis. This encompasses several stages, such as data cleansing, data integration, data transformation, data reduction, and data visualization, with the objective of making the data more usable and insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the dimensions, column names, and some statistical properties\n",
    "# of our dataset to understand what features we have at our disposal\n",
    "display(object_df_new.shape, object_df_new.columns, object_df_new.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de238b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the list of column names we are interested in\n",
    "photometry_columns = ['psParallax', 'psMag_i', 'psPm_ra', 'psPm_dec',\\\n",
    "                'psFlux_u', 'psFlux_g', 'psFlux_r', 'psFlux_i', 'psFlux_z', 'psFlux_y',\\\n",
    "                'psMag_u', 'psMag_g', 'psMag_r', 'psMag_i', 'psMag_z', 'psMag_y',\\\n",
    "                'bdFlux_u', 'bdFlux_g', 'bdFlux_r', 'bdFlux_i', 'bdFlux_z', 'bdFlux_y',\\\n",
    "                'bdMag_u', 'bdMag_g', 'bdMag_r', 'bdMag_i', 'bdMag_z', 'bdMag_y',\\\n",
    "                'stdColor_0', 'stdColor_1', 'stdColor_2', 'stdColor_3', 'stdColor_4',\\\n",
    "                'lcNonPeriodic[15]_i', 'lcNonPeriodic[26]_i',\\\n",
    "                'class','z']\n",
    "\n",
    "# Subset the dataframe to include only these columns\n",
    "photometry_df = object_df_new[photometry_columns]\n",
    "\n",
    "# Now display the information for this subsetted dataframe\n",
    "display(photometry_df.shape, photometry_df.columns, photometry_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a valid subset of the data that has labels\n",
    "photometry_df_label = photometry_df[photometry_df['class'].notna()]\n",
    "\n",
    "display(photometry_df_label['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X to the labeled DataFrame\n",
    "X = photometry_df_label\n",
    "# Remove the 'class' and redshift column from X as it is the target variable\n",
    "X = X.drop(['class','z'], axis=1)\n",
    "\n",
    "# Set y to the 'class' column of the DataFrame\n",
    "y = photometry_df_label['class']\n",
    "\n",
    "# Display the shapes and summary statistics \n",
    "display(X.shape, y.shape)\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69663e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "## GTR: Say why that choice for splitting\n",
    "# Ash: We chose an 80-20 train-test split as a common starting point that offers a good balance between having \n",
    "# enough data for training and enough to validate our model. This is a conventional choice but can be adjusted\n",
    "# based on model performance and dataset size.\n",
    "\n",
    "# Applying the split with the standard 80-20 ratio\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.20, random_state = 1, shuffle=True)\n",
    "\n",
    "## GTR: Can you show some results for different splits?  Say 40-60 and 60-40.  Different random states and shuffle on/off?\n",
    "## GTR: That will help us understand the cause of the test loss being smaller than training.\n",
    "# Ash: Conducting these additional splits can provide insight into the robustness of our model. \n",
    "# Below, I will add code to perform splits with different proportions and random states.\n",
    "# Experimenting with different splits and random states to investigate test loss\n",
    "splits = [(0.40, 0), (0.40, 42), (0.60, 0), (0.60, 42)]  # Different test sizes and random states\n",
    "split_results = {}\n",
    "\n",
    "for test_size, random_state in splits:\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = model_selection.train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    split_results[(test_size, random_state)] = (X_train_split.shape, X_test_split.shape)\n",
    "\n",
    "# Display the shapes of all the split datasets\n",
    "for params, shapes in split_results.items():\n",
    "    print(f\"Test size: {params[0]}, Random state: {params[1]} => Train shape: {shapes[0]}, Test shape: {shapes[1]}\")\n",
    "\n",
    "\n",
    "# Check for infinite values in the training set and replace with NaN\n",
    "infinite_mask_train = np.isinf(X_train)\n",
    "if np.any(infinite_mask_train):\n",
    "    X_train[infinite_mask_train] = np.nan\n",
    "\n",
    "# Check for infinite values in the test set and replace with NaN\n",
    "infinite_mask_test = np.isinf(X_test)\n",
    "if np.any(infinite_mask_test):\n",
    "    X_test[infinite_mask_test] = np.nan\n",
    "\n",
    "# Remove rows with NaN values in the training set\n",
    "nan_mask_train = np.isnan(X_train).any(axis=1)\n",
    "X_train = X_train[~nan_mask_train]\n",
    "y_train = y_train[~nan_mask_train]\n",
    "\n",
    "# Remove rows with NaN values in the test set\n",
    "nan_mask_test = np.isnan(X_test).any(axis=1)\n",
    "X_test = X_test[~nan_mask_test]\n",
    "y_test = y_test[~nan_mask_test]\n",
    "\n",
    "\n",
    "## GTR: Good to be careful here.\n",
    "# Ash: We're being diligent in ensuring that our data is clean before we proceed to scale it, as outliers or incorrect values can distort the scaling process.\n",
    "\n",
    "# Create a StandardScaler object to standardize the features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the scaler using the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the training data using the fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shapes of the scaled training and testing data\n",
    "display(X_train_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bb5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(np.any(np.isnan(X_train_scaled)))\n",
    "print(np.any(np.isnan(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff11761",
   "metadata": {},
   "source": [
    "## IV. Autoencoder Experiments <a class=\"anchor\" id=\"eight\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf2c25",
   "metadata": {},
   "source": [
    "<font color=\"red\">Do this with the MNIST examples instead??</font>\n",
    "\n",
    "I have kept the DC dataset for the autoencoder experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67894ca1",
   "metadata": {},
   "source": [
    "## IVa. Model Definition <a class=\"anchor\" id=\"nine\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32066fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "# TensorFlow, paired with Keras, is like a recipe that you plan out in advance. \n",
    "# You list all the ingredients (layers of the network) and the steps (operations like activations)\n",
    "# before you start cooking (running the network). You don't deviate from the recipe once you start. \n",
    "# In our case, the recipe consists of an encoder (which reduces the data down to a smaller, dense representation) \n",
    "# and a decoder (which tries to reconstruct the original data from this dense representation).\n",
    "\n",
    "# Here we are setting up the architecture of the simple autoencoder. This setup is static and is defined\n",
    "# before the training starts. We are using the functional API which allows for direct manipulation of the data flow.\n",
    "\n",
    "# Define the simple autoencoder function\n",
    "def Autoencoder_Simple(input_size):\n",
    "    # Calculate the hidden layer size (half of the input size) \n",
    "    #Ash: Defining network dimensions based on input.\n",
    "    hidden_size = int(input_size / 2.0)\n",
    "    # Calculate the bottleneck layer size (half of the hidden layer size) \n",
    "    # Ash: Further reducing dimension for the bottleneck.\n",
    "    bottleneck_size = int(hidden_size / 2.0)\n",
    "    \n",
    "    # Ash: Same as PyTorch, setting the hidden layer size to half of the input and the bottleneck size to half of the hidden layer.\n",
    "    \n",
    "    # Define the input layer with the specified input size\n",
    "    input_tab = Input(shape=(input_size,))\n",
    "    # Define the encoder layers with 'relu' activation function \n",
    "    # Ash: Structuring the encoder part.\n",
    "    hidden_1 = layers.Dense(hidden_size, activation='relu')(input_tab)\n",
    "    bottleneck = layers.Dense(bottleneck_size, activation='relu')(hidden_1)\n",
    "    # Define the decoder layers \n",
    "    # Ash: Structuring the decoder part to reconstruct the input.\n",
    "    hidden_2 = layers.Dense(hidden_size, activation='relu')(bottleneck)\n",
    "    output_tab = layers.Dense(input_size, activation='linear')(hidden_2)\n",
    "    \n",
    "    # Create the encoder model, which includes the input layer and bottleneck layer\n",
    "    # Ash: Defining the encoder model.\n",
    "    encoder = Model(input_tab, bottleneck) \n",
    "    # Create the full autoencoder model, which includes the input and output layers\n",
    "    # Ash: Defining the full autoencoder model.\n",
    "    autoencoder = Model(input_tab, output_tab) \n",
    "    \n",
    "    # Return both the full autoencoder model and the encoder model\n",
    "    # Ash: Returning both models for potential separate use.\n",
    "    return autoencoder, encoder \n",
    "\n",
    "    # There is no need for a 'forward' function in TensorFlow as the model's flow is predefined.\n",
    "    # the model's flow of data is static and predefined, which means a forward function is not necessary. \n",
    "    # The model already knows the path the data will take. In PyTorch, the model's flow of data is dynamically defined\n",
    "    # during runtime, hence a forward function is required to describe this flow each time data is passed through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "# Define number of splits\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare to collect the scores\n",
    "validation_scores = []\n",
    "\n",
    "input_size = X_train_scaled.shape[1]\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    # Split the data\n",
    "    X_train_kf, X_val_kf = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "\n",
    "    # Create a new model (resetting weights)\n",
    "    model, _ = Autoencoder_Simple(input_size)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_kf, X_train_kf, epochs=n_epochs, batch_size=16, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_score = model.evaluate(X_val_kf, X_val_kf, verbose=0)\n",
    "    validation_scores.append(val_score)\n",
    "\n",
    "# Calculate the average validation score\n",
    "average_val_score = sum(validation_scores) / n_splits\n",
    "print(\"Average validation MSE:\", average_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "# PyTorch is more like improvisational cooking. \n",
    "# You decide what to cook next based on how things are looking at each step. \n",
    "# This allows you to be more creative and adjust your recipe as you go. For our autoencoder, \n",
    "# this means we can define its structure by creating a class with an init method (listing the ingredients)\n",
    "# and a forward method (the cooking steps), where we can potentially make decisions on-the-fly.\n",
    "\n",
    "# PyTorch models are defined in a class where layers are set up in the __init__ method,\n",
    "# and the data flow is defined in the 'forward' method which is called during training.\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Defining the size of layers \n",
    "        # Ash: Similar to TensorFlow, we define the sizes of the hidden and bottleneck layers here.\n",
    "        hidden_size = int(input_size / 2)\n",
    "        bottleneck_size = int(hidden_size / 2)\n",
    "        \n",
    "        # Building the encoder\n",
    "        # Ash: Using Sequential for clarity and similarity with TensorFlow's structure.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, bottleneck_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Building the decoder \n",
    "        # Ash: Decoder mirrors the encoder, forming a symmetric structure.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            # The lack of an activation function on the output layer is a design choice consistent with the TensorFlow model.\n",
    "        )\n",
    "    \n",
    "    # The forward method is specific to PyTorch and defines the computation graph dynamically.\n",
    "    # TensorFlow uses static computation graphs which are defined beforehand.\n",
    "    def forward(self, x):\n",
    "        # Encoding \n",
    "        #Ash: Applying the encoder and decoder in sequence.\n",
    "        x = self.encoder(x)\n",
    "        # Decoding\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f83205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(split_size):\n",
    "    # Split the data with the given split size\n",
    "    X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "        X, y, test_size=split_size, random_state=42\n",
    "    )\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_split)\n",
    "    X_train_scaled_split = scaler.transform(X_train_split)\n",
    "    X_test_scaled_split = scaler.transform(X_test_split)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    train_tensor_split = torch.FloatTensor(X_train_scaled_split)\n",
    "    test_tensor_split = torch.FloatTensor(X_test_scaled_split)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader_split = DataLoader(TensorDataset(train_tensor_split, train_tensor_split), batch_size=32, shuffle=True)\n",
    "    test_loader_split = DataLoader(TensorDataset(test_tensor_split, test_tensor_split), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    input_size = X_train_scaled_split.shape[1]\n",
    "    model = Autoencoder(input_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader_split:\n",
    "            inputs = data[0]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader_split))\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader_split:\n",
    "                inputs = data[0]\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_loader_split))\n",
    "        \n",
    "        #print(f\"Train losses for split {split_size}: {train_losses}\")\n",
    "        #print(f\"Test losses for split {split_size}: {test_losses}\")\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f36889",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50  # Define the number of epochs\n",
    "\n",
    "# Define different validation sizes, including a very small one\n",
    "validation_sizes = [0.2, 0.4, 0.01]  # 0.01 is very small\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for size in validation_sizes:\n",
    "    train_losses, test_losses = train_test_model(size)\n",
    "    epochs = range(1, n_epochs + 1)\n",
    "    plt.plot(epochs, train_losses, label=f'Train Loss (Val size: {size})')\n",
    "    plt.plot(epochs, test_losses, label=f'Test Loss (Val size: {size})')\n",
    "\n",
    "plt.title('Training and Testing Loss for Different Validation Sizes')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ash: Data preparation and loaders are similar in functionality to TensorFlow's,\n",
    "# but the implementation details differ because of library-specific requirements.\n",
    "\n",
    "# Convert the scaled training and testing sets to PyTorch tensors \n",
    "# Ash: Preparing data for PyTorch's expected format.\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Ash: The TensorDataset and DataLoader are PyTorch-specific utilities that help with batching and shuffling.\n",
    "# Create PyTorch TensorDataset objects \n",
    "# Ash: Wrapping tensors into a dataset object for the DataLoader.\n",
    "train_dataset = TensorDataset(X_train_torch)  \n",
    "test_dataset = TensorDataset(X_test_torch)\n",
    "\n",
    "# Create PyTorch DataLoaders \n",
    "# Ash: DataLoader objects for batch processing and shuffling of the dataset.\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Data validation checks are good practice to ensure data quality before model training.\n",
    "# These checks are typically similar across frameworks.\n",
    "# Validation check for nan or inf values \n",
    "# Ash: Ensuring data integrity before model training.\n",
    "print(\"Training data contains nan:\", torch.isnan(X_train_torch).any())\n",
    "print(\"Training data contains inf:\", torch.isinf(X_train_torch).any())\n",
    "print(\"Testing data contains nan:\", torch.isnan(X_test_torch).any())\n",
    "print(\"Testing data contains inf:\", torch.isinf(X_test_torch).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63970e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross-validation\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "validation_scores = []\n",
    "input_size = X_train_scaled.shape[1]\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train_kf = torch.tensor(X_train_scaled[train_index], dtype=torch.float32)\n",
    "    X_val_kf = torch.tensor(X_train_scaled[val_index], dtype=torch.float32)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Autoencoder(input_size)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_kf)\n",
    "        loss = criterion(outputs, X_train_kf)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_kf)\n",
    "        val_loss = criterion(outputs, X_val_kf).item()\n",
    "    validation_scores.append(val_loss)\n",
    "\n",
    "average_val_score = sum(validation_scores) / n_splits\n",
    "print(\"Average validation MSE:\", average_val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c693a4",
   "metadata": {},
   "source": [
    "I have ensured that the architecture of both TensorFlow and PyTorch implementations\n",
    "follows a similar structure with explicit encoder and decoder segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cf429",
   "metadata": {},
   "source": [
    "## IVb. Model Visualization <a class=\"anchor\" id=\"ten\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c1dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "# TensorFlow operates with a static computational graph, which means that the structure of the neural network\n",
    "# model is defined and fixed before any data is processed. It's akin to setting up a blueprint from which \n",
    "# TensorFlow will work every time the model runs. The plot_model utility in TensorFlow provides a visual \n",
    "# representation of this predefined model. It illustrates the architecture of the network, including the layers\n",
    "# and how they are connected. Because the graph is static, the visualization remains the same unless you redefine\n",
    "# and rebuild the model structure.\n",
    "\n",
    "# In TensorFlow, the Autoencoder_Simple function is called with the input_size argument, \n",
    "# which returns two Keras Model objects: the full autoencoder (decoder) and the encoder. \n",
    "# The structure of the model is then visualized using the plot_model function from tensorflow.keras.utils, \n",
    "# which generates a diagram of the model architecture and saves it as an image file.\n",
    "\n",
    "input_size = X.shape[1]\n",
    "decoder, encoder = Autoencoder_Simple(input_size)\n",
    "plot_model(decoder, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "# PyTorch, conversely, uses a dynamic computational graph known as the define-by-run approach. \n",
    "# In this paradigm, the graph is built on-the-fly as the operations occur during the forward pass of data\n",
    "# through the network. This means the structure of the neural network can change with each run, as it's\n",
    "# constructed dynamically based on how the data moves through the model. The torchviz tool generates a graph\n",
    "# that shows the operations that have occurred and the way tensors have changed as the model processed\n",
    "# the input data. This visualization is  specific to the particular forward pass and the specific data\n",
    "# that was processed, so it may change from one batch of datato another.\n",
    "\n",
    "# In PyTorch, an instance of the Autoencoder_Simple class is created with the input_size argument. \n",
    "# This returns a single nn.Module object that contains both the encoder and decoder as submodules. \n",
    "# The structure of the model is visualized using the make_dot function from the torchviz library, \n",
    "# which generates a graph of the computational operations (i.e., the forward pass) and saves it as an image file.\n",
    "\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Set the input size based on the number of features in the dataset\n",
    "input_size = X.shape[1]\n",
    "# Call the Autoencoder, passing the input_size as an argument\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Visualize the model architecture\n",
    "x = torch.randn(1, input_size)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"autoencoder_model\", format=\"png\")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('autoencoder_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3ed3e",
   "metadata": {},
   "source": [
    "## IVc. Model Training <a class=\"anchor\" id=\"eleven\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd619d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "# In TensorFlow, he compile method is used to set the optimizer and the loss function for the model. \n",
    "# Then the fit method is called to train the model, specifying the number of epochs, batch size, and validation data.\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set the number of epochs\n",
    "n_epochs = 200\n",
    "\n",
    "# Compile the model with Adam optimizer and mean squared error loss function\n",
    "decoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Ash: Aligning the batch sizes for both implementations for consistency\n",
    "batch_size_tf = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Training Loop\n",
    "\n",
    "# Ash: Following the structure of the PyTorch implementation, \n",
    "# the training loop is abstracted by the `fit` method in TensorFlow\n",
    "\n",
    "history = decoder.fit(\n",
    "    X_train_scaled, X_train_scaled,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size_tf,  # Using the aligned batch size\n",
    "    verbose=2,\n",
    "    validation_data=(X_test_scaled, X_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48448f8a",
   "metadata": {},
   "source": [
    "# Example split sizes\n",
    "split_sizes = [0.05, 0.2, 0.4]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for split_size in split_sizes:\n",
    "    train_losses, test_losses = train_test_model(split_size)\n",
    "    \n",
    "    epochs = range(1, n_epochs + 1)\n",
    "    plt.plot(epochs, train_losses, label=f'Training loss (split {split_size})')\n",
    "    plt.plot(epochs, test_losses, label=f'Testing loss (split {split_size})')\n",
    "\n",
    "plt.title('Training and Testing Loss for Different Validation Sizes')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8388e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "# In PyTorch, we define the loss function (criterion) and the optimizer separately. \n",
    "# The optimizer is initialized with the model parameters and the learning rate. \n",
    "# We then write a training loop in which we manually perform the forward pass, compute the loss, \n",
    "# perform the backward pass (i.e., backpropagation), and update the model parameters with the optimizer. \n",
    "# PyTorch provides more control over the training process, but it also requires more manual coding.\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize lists to store training and testing losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = X_train_scaled.shape[1]\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size_pytorch = 32  # Aligned with TensorFlow batch size\n",
    "train_loader = DataLoader(train_tensor, batch_size=batch_size_pytorch, shuffle=True)\n",
    "test_loader = DataLoader(test_tensor, batch_size=batch_size_pytorch, shuffle=False)\n",
    "\n",
    "# Ash: The above code block is separated from the training loop for clarity\n",
    "# and to make both implementations look similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e27b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Training Loop\n",
    "\n",
    "# Ash: The training and evaluation phases are clearly separated, mirroring the TensorFlow structure.\n",
    "# Additionally, tracking of losses is done in a similar fashion to TensorFlow's history object.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize the losses for the current epoch\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for inputs in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    \n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            test_loss += loss.item()\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    \n",
    "    # Output the training and testing loss\n",
    "    print(f\"Epoch: {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Test Loss: {test_loss/len(test_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb31dd",
   "metadata": {},
   "source": [
    "## IVd. Model Output <a class=\"anchor\" id=\"twelve\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5bd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "#Learning Curve\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb035c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Implementation\n",
    "\n",
    "# In TensorFlow, the fit method automatically computes the loss on the validation data at the end of\n",
    "# each epoch, and it returns a History object that contains the loss values for both training and testing data. \n",
    "# These values can then be accessed via history.history['loss'] and history.history['val_loss'].\n",
    "\n",
    "# Plot the training and testing loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "# Learning Curve\n",
    "\n",
    "epochs = range(1, n_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_losses, 'bo', label='Training loss')\n",
    "plt.plot(epochs, test_losses, 'b', label='Testing loss')\n",
    "plt.title('Training and Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c777f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "\n",
    "# In PyTorch, we manually compute the loss on the testing data within the training loop\n",
    "# and store the loss values in a list. PyTorch does not automatically track the loss history \n",
    "# like TensorFlow does, so we need to write additional code to do this. \n",
    "# This gives us more control over the training process, but it also requires more manual coding.\n",
    "\n",
    "# Plot the training and testing loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17928335",
   "metadata": {},
   "source": [
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be834d4",
   "metadata": {},
   "source": [
    "# Enocde the test and training data to see what those outputs look like as a function of class\n",
    "X_train_scaled_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_scaled_encoded = encoder.predict(X_test_scaled)\n",
    "X_scaled_encoded = encoder.predict(X_scaled)\n",
    "\n",
    "# As above, turn these back into DataFrames for the sake of plotting\n",
    "X_train_scaled_encoded = pd.DataFrame(X_train_scaled_encoded, index=X_train.index)\n",
    "X_test_scaled_encoded = pd.DataFrame(X_test_scaled_encoded, index=X_test.index)\n",
    "X_scaled_encoded = pd.DataFrame(X_scaled_encoded, index=X.index)\n",
    "\n",
    "# Also turn \"y\" into a DataFrame, then merge with \"X\"\n",
    "y_df = pd.DataFrame(y_train).sort_index(ascending=True)\n",
    "\n",
    "encodedXy_df = pd.concat([X_train_scaled_encoded, y_df], axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "787e3610",
   "metadata": {},
   "source": [
    "# Visualizing the distribution of classes in the bottleneck parameters\n",
    "encodedXy_df_Star = encodedXy_df.loc[(encodedXy_df['class'] == 0)]\n",
    "encodedXy_df_Gal = encodedXy_df.loc[(encodedXy_df['class'] == 1)]\n",
    "encodedXy_df_Quasar = encodedXy_df.loc[(encodedXy_df['class'] == 2)]\n",
    "\n",
    "print(encodedXy_df_Star.shape)\n",
    "print(encodedXy_df_Gal.shape)\n",
    "print(encodedXy_df_Quasar.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0f0582e",
   "metadata": {},
   "source": [
    "# Color-color plots for the autoencoder features\n",
    "lims01 = np.array([[0,100], [0,100]])\n",
    "lims01 = limsugr.flatten()\n",
    "lims12 = np.array([[0,100], [0,100]])\n",
    "lims12 = limsgri.flatten()\n",
    "lims23 = np.array([[0,100], [0,100]])\n",
    "lims23 = limsriz.flatten()\n",
    "lims34 = np.array([[0,100], [0,100]])\n",
    "lims34 = limsriimag.flatten()\n",
    "lims45 = np.array([[0,100], [0,100]])\n",
    "lims45 = limsugr.flatten()\n",
    "lims56 = np.array([[0,100], [0,100]])\n",
    "lims56 = limsgri.flatten()\n",
    "\n",
    "levels = None\n",
    "nlevel=1\n",
    "\n",
    "# Make the figure:\n",
    "#fig,ax = plt.figure(figsize=(8,8))\n",
    "fig, ax = plt.subplots(3,2,figsize=(18,12))\n",
    "\n",
    "\n",
    "handles, labels = (0, 0)\n",
    "\n",
    "    \n",
    "for i, axis in enumerate(ax.ravel()):\n",
    "\n",
    "    if i==0: \n",
    "        contour_scatter(encodedXy_df_Star[0], encodedXy_df_Star[1], lims=lims01, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[0], encodedXy_df_Gal[1], lims=lims01, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[0], encodedXy_df_Quasar[1], lims=lims01, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[0], final_df_unlab[1], lims=limsugr, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,25)\n",
    "        axis.set_ylim(0,25)\n",
    "        axis.set_xlabel('0')\n",
    "        axis.set_ylabel('1')\n",
    "        axis.set_box_aspect(1)\n",
    "\n",
    "    if i==1:\n",
    "        contour_scatter(encodedXy_df_Star[1], encodedXy_df_Star[2], lims=lims12, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[1], encodedXy_df_Gal[2], lims=lims12, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[1], encodedXy_df_Quasar[2], lims=lims12, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[1], final_df_unlab[2], lims=limsgri, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,25)\n",
    "        axis.set_ylim(0,30)\n",
    "        axis.set_xlabel('1')\n",
    "        axis.set_ylabel('2')\n",
    "        axis.set_box_aspect(1)\n",
    "        \n",
    "    if i==2:\n",
    "        contour_scatter(encodedXy_df_Star[2], encodedXy_df_Star[3], lims=lims23, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[2], encodedXy_df_Gal[3], lims=lims23, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[2], encodedXy_df_Quasar[3], lims=lims23, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[2], final_df_unlab[3], lims=limsriz, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,30)\n",
    "        axis.set_ylim(0,20)\n",
    "        axis.set_xlabel('2')\n",
    "        axis.set_ylabel('3')\n",
    "        axis.set_box_aspect(1)\n",
    "        \n",
    "    if i==3:\n",
    "        contour_scatter(encodedXy_df_Star[3], encodedXy_df_Star[4], lims=lims34, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[3], encodedXy_df_Gal[4], lims=lims34, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[3], encodedXy_df_Quasar[4], lims=lims34, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[2], final_df_unlab[3], lims=limsriz, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,20)\n",
    "        axis.set_ylim(0,30)\n",
    "        axis.set_xlabel('3')\n",
    "        axis.set_ylabel('4')\n",
    "        axis.set_box_aspect(1)\n",
    "                        \n",
    "    if i==4: \n",
    "        contour_scatter(encodedXy_df_Star[4], encodedXy_df_Star[5], lims=lims45, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[4], encodedXy_df_Gal[5], lims=lims45, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[4], encodedXy_df_Quasar[5], lims=lims45, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[0], final_df_unlab[1], lims=limsugr, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,30)\n",
    "        axis.set_ylim(0,25)\n",
    "        axis.set_xlabel('4')\n",
    "        axis.set_ylabel('5')\n",
    "        axis.set_box_aspect(1)\n",
    "\n",
    "    if i==5:\n",
    "        contour_scatter(encodedXy_df_Star[5], encodedXy_df_Star[6], lims=lims56, levels=levels, nlevel=nlevel, cmap='Oranges_r', color=csdark[1], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Gal[5], encodedXy_df_Gal[6], lims=lims56, levels=levels, nlevel=nlevel, cmap='Greens_r', color=csdark[0], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        contour_scatter(encodedXy_df_Quasar[5], encodedXy_df_Quasar[6], lims=lims56, levels=levels, nlevel=nlevel, cmap='Purples_r', color=csdark[2], kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        #contour_scatter(final_df_unlab[1], final_df_unlab[2], lims=limsgri, levels=levels, nlevel=nlevel, cmap='gray', color='k', kwargs_plot={'rasterized':True, 's':3}, ax=axis)\n",
    "        axis.set_xlim(0,25)\n",
    "        axis.set_ylim(0,30)\n",
    "        axis.set_xlabel('5')\n",
    "        axis.set_ylabel('6')\n",
    "        axis.set_box_aspect(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
