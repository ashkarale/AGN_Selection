{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7eb2c4",
   "metadata": {},
   "source": [
    "# Performing QSO Classification using Variational Autoencoders¶\n",
    "\n",
    "This notebook performs Quasar Classification via a simple Autoencoder. The frameworks used for this deep learning model are TensorFlow and Pytorch.\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "* Ash Karale\n",
    "    \n",
    "\n",
    "## Contents:\n",
    "\n",
    "* [Introduction](#one)\n",
    "* [Importing Modules](#two)\n",
    "* [Data Acquisition](#three)\n",
    "* [Data Processing](#four)\n",
    "* [TensorFlow](#five)\n",
    "* [PyTorch](#six)\n",
    "* [TensorFlow vs PyTorch](#seven)\n",
    "\n",
    "\n",
    "## Versions:\n",
    "\n",
    "Initial Version: November 2022 (Ash Karale)\n",
    "\n",
    "Updated Version: April 2023 (Ash Karale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6223a6a",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206739fd",
   "metadata": {},
   "source": [
    "### Plan is to explain the dataset first, and what we aim to do with it. Next, an introduction to autoencoders and why we chose it. Lastly, an introduction to TensorFlow and PyTorch- I am thinking just a brief paragraph or two as I will write more about them in their cell blocks below\n",
    "\n",
    "TensorFlow and PyTorch are two of the most popular deep learning frameworks. They allow developers to build and train machine learning models using a variety of techniques, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial networks (GANs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac234f8",
   "metadata": {},
   "source": [
    "## Importing Modules <a class=\"anchor\" id=\"two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468c32b",
   "metadata": {},
   "source": [
    "It is considered good practice to import all the modules at the beginning of a Jupyter Notebook or any Python program.\n",
    "By importing all the modules at the start, we ensure that the required dependencies are present and available when we need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "648dcac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:48:25) \n",
      "[Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "# Importing all required modules\n",
    "\n",
    "# System modules allow Python programs to interact with the operating system and perform tasks \n",
    "# such as reading and writing files, managing processes, and accessing environment variables \n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import pickle\n",
    "import argparse\n",
    "import itertools\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Data manipulation modules allow users to perform various operations on data,\n",
    "# such as cleaning, transforming, aggregating, filtering, and visualizing data\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization modules allow users to create visual representations of data\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import palettable\n",
    "import seaborn as sns\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "# pd.set_option('display.max_columns', 1000)\n",
    "\n",
    "# Scikit-learn provides a range of supervised and unsupervised learning algorithms,\n",
    "# as well as tools for model selection and data preprocessing\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, normalized_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "# Scipy is a Python library for scientific computing and technical computing\n",
    "from scipy import stats\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "# Astropy is a Python library for astronomy and astrophysics\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "# TensorFlow is an open-source machine learning library that provides an extensive set of tools and libraries\n",
    "# for building,training, and deploying neural networks, as well as other machine learning algorithms\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# PyTorch is an open-source machine learning library for Python that provides a range of tools\n",
    "# and functions for building and training neural networks and other machine learning models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d6452",
   "metadata": {},
   "source": [
    "## Data Acquisition <a class=\"anchor\" id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfde60f",
   "metadata": {},
   "source": [
    "Data Acquisition refers to the process of collecting and gathering data from various sources. It is the first step in the data analysis pipeline and involves identifying the sources of data and obtaining the data in a usable format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192039dd",
   "metadata": {},
   "source": [
    "This line sets the path to the data. Should another data source be used, replace the line in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97acfd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ash/Research/Data/DELVE/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining a variable named 'data_dir' and assigning it the string value /Users/ash/Research/Data/DELVE/ \n",
    "# This is the path to the directory where the dataset is stored on the local machine\n",
    "data_dir = '/Users/ash/Research/Data/DELVE/'\n",
    "\n",
    "# Using the display() function to display the value of the 'data_dir' variable in the output of the Jupyter Notebook\n",
    "display(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d274f",
   "metadata": {},
   "source": [
    "Reading in the data file.\n",
    "We use Astropy's Table to read in data files because it provides a powerful and flexible way to manipulate and work with tabular data, such as data stored in CSV, FITS, or other formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92bfb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "\n",
    "# Reading a data file stored in the FITS format using the Table.read() method \n",
    "# The path to the data file is constructed using the os.path.join() method to join the data_dir variable, \n",
    "# which specifies the directory containing the data file, and the filename 'fullcat15_30.fits'\n",
    "data = Table.read(os.path.join(data_dir, 'fullcat15_30.fits'))\n",
    "\n",
    "# Converting the FITS formatted data to a Pandas DataFrame using the to_pandas() method\n",
    "# of the Table object for easier pandas manipulation\n",
    "fcDF_15_30 = data.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aeef54",
   "metadata": {},
   "source": [
    "#### Data types\n",
    "\n",
    "Measurements fall into the following main catalogries:\n",
    "- __Astromety__ -> ra, dec, proper motion and parallax\n",
    "- __Photometry__ -> point and extended source photometry, in both AB magnitdues and fluxes (nJy)\n",
    "- __Color__ -> Computed using the fluxes\n",
    "- __Morphology__ -> 1 for extended and 0 for point-like\n",
    "- __Light Curve Features__ -> Extrated on the SDSS light curves if matched\n",
    "- __Redshift__ -> Both spectroscopic and photometric, wherever available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be40be1",
   "metadata": {},
   "source": [
    "Inspecting the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e8b051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a  list of feature column names for the dataset\n",
    "# These features include photometric magnitudes, extended class, proper motion, and radial velocity\n",
    "fc_list = [\n",
    "    'mag_auto_g', 'mag_auto_i', 'mag_auto_r', 'mag_auto_z', \n",
    "    # Magnitudes in g, i, r, and z bandsfrom AUTO photometry\n",
    "    'ypetromag', 'jpetromag', 'hpetromag', 'kspetromag',\n",
    "    # Magnitudes in Y, J, H, and Ks bands from Petrosian photometry\n",
    "    'w1mpro', 'w2mpro',\n",
    "    # Magnitudes in WISE 1 and WISE 2 bands\n",
    "    'extended_class_g', 'extended_class_r', 'extended_class_i', 'extended_class_z', \n",
    "    # Extended class in g, r, i, and z bands\n",
    "    'pm', 'pmdec', 'pmra', \n",
    "    # Total proper motion, proper motion in declination, and proper motion in right ascension\n",
    "    'radial_velocity',  \n",
    "    # Radial velocity of the objects\n",
    "    'classprob_dsc_combmod_star','classprob_dsc_combmod_galaxy','classprob_dsc_combmod_quasar', \n",
    "    # Classification of the objects (e.g., star, galaxy, QSO)\n",
    "]\n",
    "\n",
    "# Selecting a subset of columns from the DataFrame 'fcDF_15_30' based on the list 'fc_list'\n",
    "fcDF_15_30 = fcDF_15_30[fc_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ba7c4",
   "metadata": {},
   "source": [
    "Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cafe091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag_auto_g</th>\n",
       "      <th>mag_auto_i</th>\n",
       "      <th>mag_auto_r</th>\n",
       "      <th>mag_auto_z</th>\n",
       "      <th>ypetromag</th>\n",
       "      <th>jpetromag</th>\n",
       "      <th>hpetromag</th>\n",
       "      <th>kspetromag</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>extended_class_r</th>\n",
       "      <th>extended_class_i</th>\n",
       "      <th>extended_class_z</th>\n",
       "      <th>pm</th>\n",
       "      <th>pmdec</th>\n",
       "      <th>pmra</th>\n",
       "      <th>radial_velocity</th>\n",
       "      <th>classprob_dsc_combmod_star</th>\n",
       "      <th>classprob_dsc_combmod_galaxy</th>\n",
       "      <th>classprob_dsc_combmod_quasar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>1.312523e+06</td>\n",
       "      <td>7.681616e+06</td>\n",
       "      <td>2.162878e+06</td>\n",
       "      <td>6.286130e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>48494.000000</td>\n",
       "      <td>1.854957e+06</td>\n",
       "      <td>1.854957e+06</td>\n",
       "      <td>1.854957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.621252e+01</td>\n",
       "      <td>2.265878e+01</td>\n",
       "      <td>2.480181e+01</td>\n",
       "      <td>2.248277e+01</td>\n",
       "      <td>1.857671e+01</td>\n",
       "      <td>1.867103e+01</td>\n",
       "      <td>1.783285e+01</td>\n",
       "      <td>1.714291e+01</td>\n",
       "      <td>1.680659e+01</td>\n",
       "      <td>1.663629e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607298e+00</td>\n",
       "      <td>1.857492e+00</td>\n",
       "      <td>1.831789e+00</td>\n",
       "      <td>1.386721e+01</td>\n",
       "      <td>-3.923559e+00</td>\n",
       "      <td>8.006602e+00</td>\n",
       "      <td>15.065438</td>\n",
       "      <td>9.447181e-01</td>\n",
       "      <td>3.015133e-02</td>\n",
       "      <td>2.166022e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.001855e+01</td>\n",
       "      <td>1.282219e+01</td>\n",
       "      <td>1.652845e+01</td>\n",
       "      <td>1.344820e+01</td>\n",
       "      <td>1.565382e+00</td>\n",
       "      <td>1.523811e+00</td>\n",
       "      <td>1.433349e+00</td>\n",
       "      <td>1.274097e+00</td>\n",
       "      <td>1.079368e+00</td>\n",
       "      <td>1.070547e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.654142e+00</td>\n",
       "      <td>2.206325e+00</td>\n",
       "      <td>2.266141e+00</td>\n",
       "      <td>1.607001e+01</td>\n",
       "      <td>1.240325e+01</td>\n",
       "      <td>1.473784e+01</td>\n",
       "      <td>42.134888</td>\n",
       "      <td>2.224325e-01</td>\n",
       "      <td>1.683584e-01</td>\n",
       "      <td>1.381415e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.278190e+01</td>\n",
       "      <td>1.167814e+01</td>\n",
       "      <td>1.210906e+01</td>\n",
       "      <td>1.139630e+01</td>\n",
       "      <td>1.037403e+01</td>\n",
       "      <td>9.689001e+00</td>\n",
       "      <td>8.260656e+00</td>\n",
       "      <td>8.256360e+00</td>\n",
       "      <td>7.068000e+00</td>\n",
       "      <td>6.085000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>2.600000e-03</td>\n",
       "      <td>-8.026215e+02</td>\n",
       "      <td>-3.656444e+02</td>\n",
       "      <td>-389.880100</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.137134e+01</td>\n",
       "      <td>1.974994e+01</td>\n",
       "      <td>2.029234e+01</td>\n",
       "      <td>1.943825e+01</td>\n",
       "      <td>1.778231e+01</td>\n",
       "      <td>1.793413e+01</td>\n",
       "      <td>1.716681e+01</td>\n",
       "      <td>1.654000e+01</td>\n",
       "      <td>1.628600e+01</td>\n",
       "      <td>1.608700e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.070828e+00</td>\n",
       "      <td>-7.266419e+00</td>\n",
       "      <td>8.808007e-01</td>\n",
       "      <td>-5.499497</td>\n",
       "      <td>9.995270e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.271964e+01</td>\n",
       "      <td>2.084213e+01</td>\n",
       "      <td>2.150048e+01</td>\n",
       "      <td>2.049579e+01</td>\n",
       "      <td>1.887614e+01</td>\n",
       "      <td>1.897750e+01</td>\n",
       "      <td>1.812667e+01</td>\n",
       "      <td>1.739955e+01</td>\n",
       "      <td>1.693600e+01</td>\n",
       "      <td>1.674200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>9.908950e+00</td>\n",
       "      <td>-1.996961e+00</td>\n",
       "      <td>5.454478e+00</td>\n",
       "      <td>12.312459</td>\n",
       "      <td>9.999640e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.406641e+01</td>\n",
       "      <td>2.170479e+01</td>\n",
       "      <td>2.257661e+01</td>\n",
       "      <td>2.128124e+01</td>\n",
       "      <td>1.971436e+01</td>\n",
       "      <td>1.974666e+01</td>\n",
       "      <td>1.882400e+01</td>\n",
       "      <td>1.802353e+01</td>\n",
       "      <td>1.753000e+01</td>\n",
       "      <td>1.738200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.731142e+01</td>\n",
       "      <td>9.636427e-01</td>\n",
       "      <td>1.218151e+01</td>\n",
       "      <td>31.317938</td>\n",
       "      <td>9.999900e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.868277e+01</td>\n",
       "      <td>3.056738e+01</td>\n",
       "      <td>2.972356e+01</td>\n",
       "      <td>3.215351e+01</td>\n",
       "      <td>2.004100e+01</td>\n",
       "      <td>1.889200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.026368e+02</td>\n",
       "      <td>5.512816e+02</td>\n",
       "      <td>6.766674e+02</td>\n",
       "      <td>761.085100</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mag_auto_g    mag_auto_i    mag_auto_r    mag_auto_z     ypetromag  \\\n",
       "count  8.226904e+06  8.226904e+06  8.226904e+06  8.226904e+06  1.312523e+06   \n",
       "mean   3.621252e+01  2.265878e+01  2.480181e+01  2.248277e+01  1.857671e+01   \n",
       "std    3.001855e+01  1.282219e+01  1.652845e+01  1.344820e+01  1.565382e+00   \n",
       "min    1.278190e+01  1.167814e+01  1.210906e+01  1.139630e+01  1.037403e+01   \n",
       "25%    2.137134e+01  1.974994e+01  2.029234e+01  1.943825e+01  1.778231e+01   \n",
       "50%    2.271964e+01  2.084213e+01  2.150048e+01  2.049579e+01  1.887614e+01   \n",
       "75%    2.406641e+01  2.170479e+01  2.257661e+01  2.128124e+01  1.971436e+01   \n",
       "max    9.900000e+01  9.900000e+01  9.900000e+01  9.900000e+01  2.868277e+01   \n",
       "\n",
       "          jpetromag     hpetromag    kspetromag        w1mpro        w2mpro  \\\n",
       "count  7.681616e+06  2.162878e+06  6.286130e+06  8.226904e+06  8.226904e+06   \n",
       "mean   1.867103e+01  1.783285e+01  1.714291e+01  1.680659e+01  1.663629e+01   \n",
       "std    1.523811e+00  1.433349e+00  1.274097e+00  1.079368e+00  1.070547e+00   \n",
       "min    9.689001e+00  8.260656e+00  8.256360e+00  7.068000e+00  6.085000e+00   \n",
       "25%    1.793413e+01  1.716681e+01  1.654000e+01  1.628600e+01  1.608700e+01   \n",
       "50%    1.897750e+01  1.812667e+01  1.739955e+01  1.693600e+01  1.674200e+01   \n",
       "75%    1.974666e+01  1.882400e+01  1.802353e+01  1.753000e+01  1.738200e+01   \n",
       "max    3.056738e+01  2.972356e+01  3.215351e+01  2.004100e+01  1.889200e+01   \n",
       "\n",
       "       ...  extended_class_r  extended_class_i  extended_class_z  \\\n",
       "count  ...      8.226904e+06      8.226904e+06      8.226904e+06   \n",
       "mean   ...      1.607298e+00      1.857492e+00      1.831789e+00   \n",
       "std    ...      2.654142e+00      2.206325e+00      2.266141e+00   \n",
       "min    ...     -9.000000e+00     -9.000000e+00     -9.000000e+00   \n",
       "25%    ...      0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "50%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "75%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "max    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "\n",
       "                 pm         pmdec          pmra  radial_velocity  \\\n",
       "count  1.557373e+06  1.557373e+06  1.557373e+06     48494.000000   \n",
       "mean   1.386721e+01 -3.923559e+00  8.006602e+00        15.065438   \n",
       "std    1.607001e+01  1.240325e+01  1.473784e+01        42.134888   \n",
       "min    2.600000e-03 -8.026215e+02 -3.656444e+02      -389.880100   \n",
       "25%    5.070828e+00 -7.266419e+00  8.808007e-01        -5.499497   \n",
       "50%    9.908950e+00 -1.996961e+00  5.454478e+00        12.312459   \n",
       "75%    1.731142e+01  9.636427e-01  1.218151e+01        31.317938   \n",
       "max    8.026368e+02  5.512816e+02  6.766674e+02       761.085100   \n",
       "\n",
       "       classprob_dsc_combmod_star  classprob_dsc_combmod_galaxy  \\\n",
       "count                1.854957e+06                  1.854957e+06   \n",
       "mean                 9.447181e-01                  3.015133e-02   \n",
       "std                  2.224325e-01                  1.683584e-01   \n",
       "min                  0.000000e+00                  0.000000e+00   \n",
       "25%                  9.995270e-01                  0.000000e+00   \n",
       "50%                  9.999640e-01                  0.000000e+00   \n",
       "75%                  9.999900e-01                  0.000000e+00   \n",
       "max                  1.000000e+00                  1.000000e+00   \n",
       "\n",
       "       classprob_dsc_combmod_quasar  \n",
       "count                  1.854957e+06  \n",
       "mean                   2.166022e-02  \n",
       "std                    1.381415e-01  \n",
       "min                    0.000000e+00  \n",
       "25%                    0.000000e+00  \n",
       "50%                    0.000000e+00  \n",
       "75%                    0.000000e+00  \n",
       "max                    1.000000e+00  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display descriptive statistics to describe and explore data\n",
    "\n",
    "# The describe() method provides summary statistics for each column of the DataFrame, \n",
    "# giving insight into the distribution and spread of the data \n",
    "fcDF_15_30.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16618961",
   "metadata": {},
   "source": [
    "## Data Processing <a class=\"anchor\" id=\"four\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24666461",
   "metadata": {},
   "source": [
    "Data Processing refers to the process of transforming raw data into a form that is suitable for analysis. It involves a series of steps that may include data cleaning, data integration, data transformation, data reduction, and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df42e7",
   "metadata": {},
   "source": [
    "Create a subset with the maximal number of objects where the data values are meaningful.\n",
    "Specifically-\n",
    "* Merge the Star, Galaxy, and QSO attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f77ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list 'fc15_30' to a Pandas DataFrame using the pd.DataFrame() method\n",
    "fcDF_15_30 = pd.DataFrame(fcDF_15_30)\n",
    "\n",
    "# Replacing the string values of the columns named 'classprob_dsc_combmod_star', 'classprob_dsc_combmod_galaxy',\n",
    "# and 'classprob_dsc_combmod_quasar' with numerical values 0, 1, and 2, respectively.\n",
    "fcDF_15_30 = fcDF_15_30.replace({'classprob_dsc_combmod_star': 0,\n",
    "                           'classprob_dsc_combmod_galaxy': 1,\n",
    "                           'classprob_dsc_combmod_quasar': 2})\n",
    "\n",
    "# Define a function to determine the class based on the highest class probability\n",
    "# The function called 'assign_class' that takes a row as input, and based on the \n",
    "# class probabilities for each object, assigns the object to one of the classes\n",
    "def assign_class(row):\n",
    "    star_prob = row['classprob_dsc_combmod_star']\n",
    "    galaxy_prob = row['classprob_dsc_combmod_galaxy']\n",
    "    quasar_prob = row['classprob_dsc_combmod_quasar']\n",
    "    \n",
    "    if star_prob > galaxy_prob and star_prob > quasar_prob:\n",
    "        return 0\n",
    "    elif galaxy_prob > quasar_prob:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Merging the class probability attributes of galaxies, quasars, and stars into a single 'class' attribute \n",
    "# based on the highest probability value\n",
    "# The apply() method applies the function 'assign_class' to each row of the DataFrame\n",
    "fcDF_15_30['class'] = fcDF_15_30[['classprob_dsc_combmod_galaxy', 'classprob_dsc_combmod_quasar',\n",
    "                            'classprob_dsc_combmod_star']].apply(assign_class, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38b2d56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8226904, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8226904,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag_auto_g</th>\n",
       "      <th>mag_auto_i</th>\n",
       "      <th>mag_auto_r</th>\n",
       "      <th>mag_auto_z</th>\n",
       "      <th>ypetromag</th>\n",
       "      <th>jpetromag</th>\n",
       "      <th>hpetromag</th>\n",
       "      <th>kspetromag</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>extended_class_r</th>\n",
       "      <th>extended_class_i</th>\n",
       "      <th>extended_class_z</th>\n",
       "      <th>pm</th>\n",
       "      <th>pmdec</th>\n",
       "      <th>pmra</th>\n",
       "      <th>radial_velocity</th>\n",
       "      <th>classprob_dsc_combmod_star</th>\n",
       "      <th>classprob_dsc_combmod_galaxy</th>\n",
       "      <th>classprob_dsc_combmod_quasar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>1.312523e+06</td>\n",
       "      <td>7.681616e+06</td>\n",
       "      <td>2.162878e+06</td>\n",
       "      <td>6.286130e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>8.226904e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>1.557373e+06</td>\n",
       "      <td>48494.000000</td>\n",
       "      <td>1.854957e+06</td>\n",
       "      <td>1.854957e+06</td>\n",
       "      <td>1.854957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.621252e+01</td>\n",
       "      <td>2.265878e+01</td>\n",
       "      <td>2.480181e+01</td>\n",
       "      <td>2.248277e+01</td>\n",
       "      <td>1.857671e+01</td>\n",
       "      <td>1.867103e+01</td>\n",
       "      <td>1.783285e+01</td>\n",
       "      <td>1.714291e+01</td>\n",
       "      <td>1.680659e+01</td>\n",
       "      <td>1.663629e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607298e+00</td>\n",
       "      <td>1.857492e+00</td>\n",
       "      <td>1.831789e+00</td>\n",
       "      <td>1.386721e+01</td>\n",
       "      <td>-3.923559e+00</td>\n",
       "      <td>8.006602e+00</td>\n",
       "      <td>15.065438</td>\n",
       "      <td>9.447181e-01</td>\n",
       "      <td>3.015133e-02</td>\n",
       "      <td>2.166022e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.001855e+01</td>\n",
       "      <td>1.282219e+01</td>\n",
       "      <td>1.652845e+01</td>\n",
       "      <td>1.344820e+01</td>\n",
       "      <td>1.565382e+00</td>\n",
       "      <td>1.523811e+00</td>\n",
       "      <td>1.433349e+00</td>\n",
       "      <td>1.274097e+00</td>\n",
       "      <td>1.079368e+00</td>\n",
       "      <td>1.070547e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.654142e+00</td>\n",
       "      <td>2.206325e+00</td>\n",
       "      <td>2.266141e+00</td>\n",
       "      <td>1.607001e+01</td>\n",
       "      <td>1.240325e+01</td>\n",
       "      <td>1.473784e+01</td>\n",
       "      <td>42.134888</td>\n",
       "      <td>2.224325e-01</td>\n",
       "      <td>1.683584e-01</td>\n",
       "      <td>1.381415e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.278190e+01</td>\n",
       "      <td>1.167814e+01</td>\n",
       "      <td>1.210906e+01</td>\n",
       "      <td>1.139630e+01</td>\n",
       "      <td>1.037403e+01</td>\n",
       "      <td>9.689001e+00</td>\n",
       "      <td>8.260656e+00</td>\n",
       "      <td>8.256360e+00</td>\n",
       "      <td>7.068000e+00</td>\n",
       "      <td>6.085000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>2.600000e-03</td>\n",
       "      <td>-8.026215e+02</td>\n",
       "      <td>-3.656444e+02</td>\n",
       "      <td>-389.880100</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.137134e+01</td>\n",
       "      <td>1.974994e+01</td>\n",
       "      <td>2.029234e+01</td>\n",
       "      <td>1.943825e+01</td>\n",
       "      <td>1.778231e+01</td>\n",
       "      <td>1.793413e+01</td>\n",
       "      <td>1.716681e+01</td>\n",
       "      <td>1.654000e+01</td>\n",
       "      <td>1.628600e+01</td>\n",
       "      <td>1.608700e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.070828e+00</td>\n",
       "      <td>-7.266419e+00</td>\n",
       "      <td>8.808007e-01</td>\n",
       "      <td>-5.499497</td>\n",
       "      <td>9.995270e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.271964e+01</td>\n",
       "      <td>2.084213e+01</td>\n",
       "      <td>2.150048e+01</td>\n",
       "      <td>2.049579e+01</td>\n",
       "      <td>1.887614e+01</td>\n",
       "      <td>1.897750e+01</td>\n",
       "      <td>1.812667e+01</td>\n",
       "      <td>1.739955e+01</td>\n",
       "      <td>1.693600e+01</td>\n",
       "      <td>1.674200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>9.908950e+00</td>\n",
       "      <td>-1.996961e+00</td>\n",
       "      <td>5.454478e+00</td>\n",
       "      <td>12.312459</td>\n",
       "      <td>9.999640e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.406641e+01</td>\n",
       "      <td>2.170479e+01</td>\n",
       "      <td>2.257661e+01</td>\n",
       "      <td>2.128124e+01</td>\n",
       "      <td>1.971436e+01</td>\n",
       "      <td>1.974666e+01</td>\n",
       "      <td>1.882400e+01</td>\n",
       "      <td>1.802353e+01</td>\n",
       "      <td>1.753000e+01</td>\n",
       "      <td>1.738200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.731142e+01</td>\n",
       "      <td>9.636427e-01</td>\n",
       "      <td>1.218151e+01</td>\n",
       "      <td>31.317938</td>\n",
       "      <td>9.999900e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.868277e+01</td>\n",
       "      <td>3.056738e+01</td>\n",
       "      <td>2.972356e+01</td>\n",
       "      <td>3.215351e+01</td>\n",
       "      <td>2.004100e+01</td>\n",
       "      <td>1.889200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.026368e+02</td>\n",
       "      <td>5.512816e+02</td>\n",
       "      <td>6.766674e+02</td>\n",
       "      <td>761.085100</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mag_auto_g    mag_auto_i    mag_auto_r    mag_auto_z     ypetromag  \\\n",
       "count  8.226904e+06  8.226904e+06  8.226904e+06  8.226904e+06  1.312523e+06   \n",
       "mean   3.621252e+01  2.265878e+01  2.480181e+01  2.248277e+01  1.857671e+01   \n",
       "std    3.001855e+01  1.282219e+01  1.652845e+01  1.344820e+01  1.565382e+00   \n",
       "min    1.278190e+01  1.167814e+01  1.210906e+01  1.139630e+01  1.037403e+01   \n",
       "25%    2.137134e+01  1.974994e+01  2.029234e+01  1.943825e+01  1.778231e+01   \n",
       "50%    2.271964e+01  2.084213e+01  2.150048e+01  2.049579e+01  1.887614e+01   \n",
       "75%    2.406641e+01  2.170479e+01  2.257661e+01  2.128124e+01  1.971436e+01   \n",
       "max    9.900000e+01  9.900000e+01  9.900000e+01  9.900000e+01  2.868277e+01   \n",
       "\n",
       "          jpetromag     hpetromag    kspetromag        w1mpro        w2mpro  \\\n",
       "count  7.681616e+06  2.162878e+06  6.286130e+06  8.226904e+06  8.226904e+06   \n",
       "mean   1.867103e+01  1.783285e+01  1.714291e+01  1.680659e+01  1.663629e+01   \n",
       "std    1.523811e+00  1.433349e+00  1.274097e+00  1.079368e+00  1.070547e+00   \n",
       "min    9.689001e+00  8.260656e+00  8.256360e+00  7.068000e+00  6.085000e+00   \n",
       "25%    1.793413e+01  1.716681e+01  1.654000e+01  1.628600e+01  1.608700e+01   \n",
       "50%    1.897750e+01  1.812667e+01  1.739955e+01  1.693600e+01  1.674200e+01   \n",
       "75%    1.974666e+01  1.882400e+01  1.802353e+01  1.753000e+01  1.738200e+01   \n",
       "max    3.056738e+01  2.972356e+01  3.215351e+01  2.004100e+01  1.889200e+01   \n",
       "\n",
       "       ...  extended_class_r  extended_class_i  extended_class_z  \\\n",
       "count  ...      8.226904e+06      8.226904e+06      8.226904e+06   \n",
       "mean   ...      1.607298e+00      1.857492e+00      1.831789e+00   \n",
       "std    ...      2.654142e+00      2.206325e+00      2.266141e+00   \n",
       "min    ...     -9.000000e+00     -9.000000e+00     -9.000000e+00   \n",
       "25%    ...      0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "50%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "75%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "max    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "\n",
       "                 pm         pmdec          pmra  radial_velocity  \\\n",
       "count  1.557373e+06  1.557373e+06  1.557373e+06     48494.000000   \n",
       "mean   1.386721e+01 -3.923559e+00  8.006602e+00        15.065438   \n",
       "std    1.607001e+01  1.240325e+01  1.473784e+01        42.134888   \n",
       "min    2.600000e-03 -8.026215e+02 -3.656444e+02      -389.880100   \n",
       "25%    5.070828e+00 -7.266419e+00  8.808007e-01        -5.499497   \n",
       "50%    9.908950e+00 -1.996961e+00  5.454478e+00        12.312459   \n",
       "75%    1.731142e+01  9.636427e-01  1.218151e+01        31.317938   \n",
       "max    8.026368e+02  5.512816e+02  6.766674e+02       761.085100   \n",
       "\n",
       "       classprob_dsc_combmod_star  classprob_dsc_combmod_galaxy  \\\n",
       "count                1.854957e+06                  1.854957e+06   \n",
       "mean                 9.447181e-01                  3.015133e-02   \n",
       "std                  2.224325e-01                  1.683584e-01   \n",
       "min                  0.000000e+00                  0.000000e+00   \n",
       "25%                  9.995270e-01                  0.000000e+00   \n",
       "50%                  9.999640e-01                  0.000000e+00   \n",
       "75%                  9.999900e-01                  0.000000e+00   \n",
       "max                  1.000000e+00                  1.000000e+00   \n",
       "\n",
       "       classprob_dsc_combmod_quasar  \n",
       "count                  1.854957e+06  \n",
       "mean                   2.166022e-02  \n",
       "std                    1.381415e-01  \n",
       "min                    0.000000e+00  \n",
       "25%                    0.000000e+00  \n",
       "50%                    0.000000e+00  \n",
       "75%                    0.000000e+00  \n",
       "max                    1.000000e+00  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standardizing and Scaling\n",
    "\n",
    "# Set X to the entire DataFrame\n",
    "X = fcDF_15_30\n",
    "# Remove the 'class' column from X as it is the target variable\n",
    "X = X.drop(['class'], axis=1)\n",
    "\n",
    "# Set y to the 'class' column of the DataFrame\n",
    "y = fcDF_15_30['class']\n",
    "\n",
    "# Display the shapes and summary statistics \n",
    "display(X.shape, y.shape)\n",
    "display(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c73b4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4113452, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4113452, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets (50% training, 50% testing)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.5, random_state = 1, shuffle=True)\n",
    "\n",
    "# Impute missing values by using the mean or median value\n",
    "X_train = X_train.fillna(X_train.mean()) # or X_train.median()\n",
    "X_test = X_test.fillna(X_test.mean()) # or X_test.median()\n",
    "\n",
    "# Create a StandardScaler object to standardize the features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit the scaler using the training data\n",
    "scaler.fit(X_train)\n",
    "# Transform the training data using the fitted scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "# Transform the testing data using the fitted scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shapes of the scaled training and testing data\n",
    "display(X_train_scaled.shape, X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0259179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag_auto_g</th>\n",
       "      <th>mag_auto_i</th>\n",
       "      <th>mag_auto_r</th>\n",
       "      <th>mag_auto_z</th>\n",
       "      <th>ypetromag</th>\n",
       "      <th>jpetromag</th>\n",
       "      <th>hpetromag</th>\n",
       "      <th>kspetromag</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>extended_class_r</th>\n",
       "      <th>extended_class_i</th>\n",
       "      <th>extended_class_z</th>\n",
       "      <th>pm</th>\n",
       "      <th>pmdec</th>\n",
       "      <th>pmra</th>\n",
       "      <th>radial_velocity</th>\n",
       "      <th>classprob_dsc_combmod_star</th>\n",
       "      <th>classprob_dsc_combmod_galaxy</th>\n",
       "      <th>classprob_dsc_combmod_quasar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.622024e+01</td>\n",
       "      <td>2.265907e+01</td>\n",
       "      <td>2.480370e+01</td>\n",
       "      <td>2.248994e+01</td>\n",
       "      <td>1.857653e+01</td>\n",
       "      <td>1.867119e+01</td>\n",
       "      <td>1.783308e+01</td>\n",
       "      <td>1.714314e+01</td>\n",
       "      <td>1.680671e+01</td>\n",
       "      <td>1.663623e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606736e+00</td>\n",
       "      <td>1.857049e+00</td>\n",
       "      <td>1.830402e+00</td>\n",
       "      <td>1.386853e+01</td>\n",
       "      <td>-3.923129e+00</td>\n",
       "      <td>7.989458e+00</td>\n",
       "      <td>1.496564e+01</td>\n",
       "      <td>9.447756e-01</td>\n",
       "      <td>2.995606e-02</td>\n",
       "      <td>2.181235e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.002534e+01</td>\n",
       "      <td>1.282308e+01</td>\n",
       "      <td>1.653305e+01</td>\n",
       "      <td>1.346784e+01</td>\n",
       "      <td>6.256626e-01</td>\n",
       "      <td>1.472865e+00</td>\n",
       "      <td>7.345629e-01</td>\n",
       "      <td>1.113901e+00</td>\n",
       "      <td>1.079560e+00</td>\n",
       "      <td>1.070733e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.654679e+00</td>\n",
       "      <td>2.206470e+00</td>\n",
       "      <td>2.268565e+00</td>\n",
       "      <td>7.004411e+00</td>\n",
       "      <td>5.450415e+00</td>\n",
       "      <td>6.387013e+00</td>\n",
       "      <td>3.187139e+00</td>\n",
       "      <td>1.055817e-01</td>\n",
       "      <td>7.970007e-02</td>\n",
       "      <td>6.582942e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.278190e+01</td>\n",
       "      <td>1.167814e+01</td>\n",
       "      <td>1.210906e+01</td>\n",
       "      <td>1.139630e+01</td>\n",
       "      <td>1.051031e+01</td>\n",
       "      <td>9.689001e+00</td>\n",
       "      <td>9.826766e+00</td>\n",
       "      <td>9.507744e+00</td>\n",
       "      <td>7.965000e+00</td>\n",
       "      <td>7.808000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>2.600000e-03</td>\n",
       "      <td>-8.026215e+02</td>\n",
       "      <td>-3.656444e+02</td>\n",
       "      <td>-2.607022e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.137033e+01</td>\n",
       "      <td>1.975028e+01</td>\n",
       "      <td>2.029223e+01</td>\n",
       "      <td>1.943848e+01</td>\n",
       "      <td>1.857653e+01</td>\n",
       "      <td>1.803132e+01</td>\n",
       "      <td>1.783308e+01</td>\n",
       "      <td>1.686470e+01</td>\n",
       "      <td>1.628600e+01</td>\n",
       "      <td>1.608700e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.386853e+01</td>\n",
       "      <td>-3.923129e+00</td>\n",
       "      <td>7.989458e+00</td>\n",
       "      <td>1.496564e+01</td>\n",
       "      <td>9.447756e-01</td>\n",
       "      <td>2.995606e-02</td>\n",
       "      <td>2.181235e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.271930e+01</td>\n",
       "      <td>2.084202e+01</td>\n",
       "      <td>2.149993e+01</td>\n",
       "      <td>2.049579e+01</td>\n",
       "      <td>1.857653e+01</td>\n",
       "      <td>1.885876e+01</td>\n",
       "      <td>1.783308e+01</td>\n",
       "      <td>1.714314e+01</td>\n",
       "      <td>1.693600e+01</td>\n",
       "      <td>1.674200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.386853e+01</td>\n",
       "      <td>-3.923129e+00</td>\n",
       "      <td>7.989458e+00</td>\n",
       "      <td>1.496564e+01</td>\n",
       "      <td>9.447756e-01</td>\n",
       "      <td>2.995606e-02</td>\n",
       "      <td>2.181235e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.406738e+01</td>\n",
       "      <td>2.170480e+01</td>\n",
       "      <td>2.257708e+01</td>\n",
       "      <td>2.128109e+01</td>\n",
       "      <td>1.857653e+01</td>\n",
       "      <td>1.969211e+01</td>\n",
       "      <td>1.783308e+01</td>\n",
       "      <td>1.783367e+01</td>\n",
       "      <td>1.753100e+01</td>\n",
       "      <td>1.738300e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.386853e+01</td>\n",
       "      <td>-3.923129e+00</td>\n",
       "      <td>7.989458e+00</td>\n",
       "      <td>1.496564e+01</td>\n",
       "      <td>9.447756e-01</td>\n",
       "      <td>2.995606e-02</td>\n",
       "      <td>2.181235e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.868277e+01</td>\n",
       "      <td>3.056738e+01</td>\n",
       "      <td>2.623996e+01</td>\n",
       "      <td>3.215351e+01</td>\n",
       "      <td>2.003600e+01</td>\n",
       "      <td>1.889200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>8.026368e+02</td>\n",
       "      <td>5.512816e+02</td>\n",
       "      <td>6.766674e+02</td>\n",
       "      <td>4.176311e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mag_auto_g    mag_auto_i    mag_auto_r    mag_auto_z     ypetromag  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06   \n",
       "mean   3.622024e+01  2.265907e+01  2.480370e+01  2.248994e+01  1.857653e+01   \n",
       "std    3.002534e+01  1.282308e+01  1.653305e+01  1.346784e+01  6.256626e-01   \n",
       "min    1.278190e+01  1.167814e+01  1.210906e+01  1.139630e+01  1.051031e+01   \n",
       "25%    2.137033e+01  1.975028e+01  2.029223e+01  1.943848e+01  1.857653e+01   \n",
       "50%    2.271930e+01  2.084202e+01  2.149993e+01  2.049579e+01  1.857653e+01   \n",
       "75%    2.406738e+01  2.170480e+01  2.257708e+01  2.128109e+01  1.857653e+01   \n",
       "max    9.900000e+01  9.900000e+01  9.900000e+01  9.900000e+01  2.868277e+01   \n",
       "\n",
       "          jpetromag     hpetromag    kspetromag        w1mpro        w2mpro  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06   \n",
       "mean   1.867119e+01  1.783308e+01  1.714314e+01  1.680671e+01  1.663623e+01   \n",
       "std    1.472865e+00  7.345629e-01  1.113901e+00  1.079560e+00  1.070733e+00   \n",
       "min    9.689001e+00  9.826766e+00  9.507744e+00  7.965000e+00  7.808000e+00   \n",
       "25%    1.803132e+01  1.783308e+01  1.686470e+01  1.628600e+01  1.608700e+01   \n",
       "50%    1.885876e+01  1.783308e+01  1.714314e+01  1.693600e+01  1.674200e+01   \n",
       "75%    1.969211e+01  1.783308e+01  1.783367e+01  1.753100e+01  1.738300e+01   \n",
       "max    3.056738e+01  2.623996e+01  3.215351e+01  2.003600e+01  1.889200e+01   \n",
       "\n",
       "       ...  extended_class_r  extended_class_i  extended_class_z  \\\n",
       "count  ...      4.113452e+06      4.113452e+06      4.113452e+06   \n",
       "mean   ...      1.606736e+00      1.857049e+00      1.830402e+00   \n",
       "std    ...      2.654679e+00      2.206470e+00      2.268565e+00   \n",
       "min    ...     -9.000000e+00     -9.000000e+00     -9.000000e+00   \n",
       "25%    ...      0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "50%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "75%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "max    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "\n",
       "                 pm         pmdec          pmra  radial_velocity  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06     4.113452e+06   \n",
       "mean   1.386853e+01 -3.923129e+00  7.989458e+00     1.496564e+01   \n",
       "std    7.004411e+00  5.450415e+00  6.387013e+00     3.187139e+00   \n",
       "min    2.600000e-03 -8.026215e+02 -3.656444e+02    -2.607022e+02   \n",
       "25%    1.386853e+01 -3.923129e+00  7.989458e+00     1.496564e+01   \n",
       "50%    1.386853e+01 -3.923129e+00  7.989458e+00     1.496564e+01   \n",
       "75%    1.386853e+01 -3.923129e+00  7.989458e+00     1.496564e+01   \n",
       "max    8.026368e+02  5.512816e+02  6.766674e+02     4.176311e+02   \n",
       "\n",
       "       classprob_dsc_combmod_star  classprob_dsc_combmod_galaxy  \\\n",
       "count                4.113452e+06                  4.113452e+06   \n",
       "mean                 9.447756e-01                  2.995606e-02   \n",
       "std                  1.055817e-01                  7.970007e-02   \n",
       "min                  0.000000e+00                  0.000000e+00   \n",
       "25%                  9.447756e-01                  2.995606e-02   \n",
       "50%                  9.447756e-01                  2.995606e-02   \n",
       "75%                  9.447756e-01                  2.995606e-02   \n",
       "max                  1.000000e+00                  1.000000e+00   \n",
       "\n",
       "       classprob_dsc_combmod_quasar  \n",
       "count                  4.113452e+06  \n",
       "mean                   2.181235e-02  \n",
       "std                    6.582942e-02  \n",
       "min                    0.000000e+00  \n",
       "25%                    2.181235e-02  \n",
       "50%                    2.181235e-02  \n",
       "75%                    2.181235e-02  \n",
       "max                    1.000000e+00  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag_auto_g</th>\n",
       "      <th>mag_auto_i</th>\n",
       "      <th>mag_auto_r</th>\n",
       "      <th>mag_auto_z</th>\n",
       "      <th>ypetromag</th>\n",
       "      <th>jpetromag</th>\n",
       "      <th>hpetromag</th>\n",
       "      <th>kspetromag</th>\n",
       "      <th>w1mpro</th>\n",
       "      <th>w2mpro</th>\n",
       "      <th>...</th>\n",
       "      <th>extended_class_r</th>\n",
       "      <th>extended_class_i</th>\n",
       "      <th>extended_class_z</th>\n",
       "      <th>pm</th>\n",
       "      <th>pmdec</th>\n",
       "      <th>pmra</th>\n",
       "      <th>radial_velocity</th>\n",
       "      <th>classprob_dsc_combmod_star</th>\n",
       "      <th>classprob_dsc_combmod_galaxy</th>\n",
       "      <th>classprob_dsc_combmod_quasar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "      <td>4.113452e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.620479e+01</td>\n",
       "      <td>2.265848e+01</td>\n",
       "      <td>2.479992e+01</td>\n",
       "      <td>2.247560e+01</td>\n",
       "      <td>1.857690e+01</td>\n",
       "      <td>1.867087e+01</td>\n",
       "      <td>1.783262e+01</td>\n",
       "      <td>1.714267e+01</td>\n",
       "      <td>1.680647e+01</td>\n",
       "      <td>1.663633e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.607860e+00</td>\n",
       "      <td>1.857935e+00</td>\n",
       "      <td>1.833176e+00</td>\n",
       "      <td>1.386589e+01</td>\n",
       "      <td>-3.923989e+00</td>\n",
       "      <td>8.023788e+00</td>\n",
       "      <td>1.516541e+01</td>\n",
       "      <td>9.446607e-01</td>\n",
       "      <td>3.034684e-02</td>\n",
       "      <td>2.150791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.001175e+01</td>\n",
       "      <td>1.282129e+01</td>\n",
       "      <td>1.652386e+01</td>\n",
       "      <td>1.342852e+01</td>\n",
       "      <td>6.248419e-01</td>\n",
       "      <td>1.472027e+00</td>\n",
       "      <td>7.353093e-01</td>\n",
       "      <td>1.113538e+00</td>\n",
       "      <td>1.079176e+00</td>\n",
       "      <td>1.070361e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.653605e+00</td>\n",
       "      <td>2.206180e+00</td>\n",
       "      <td>2.263713e+00</td>\n",
       "      <td>6.979330e+00</td>\n",
       "      <td>5.342078e+00</td>\n",
       "      <td>6.437422e+00</td>\n",
       "      <td>3.281982e+00</td>\n",
       "      <td>1.056586e-01</td>\n",
       "      <td>8.018615e-02</td>\n",
       "      <td>6.536032e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.316891e+01</td>\n",
       "      <td>1.246892e+01</td>\n",
       "      <td>1.211705e+01</td>\n",
       "      <td>1.221446e+01</td>\n",
       "      <td>1.037403e+01</td>\n",
       "      <td>1.013221e+01</td>\n",
       "      <td>8.260656e+00</td>\n",
       "      <td>8.256360e+00</td>\n",
       "      <td>7.068000e+00</td>\n",
       "      <td>6.085000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>5.783000e-03</td>\n",
       "      <td>-7.481313e+02</td>\n",
       "      <td>-3.379641e+02</td>\n",
       "      <td>-3.898801e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.137233e+01</td>\n",
       "      <td>1.974964e+01</td>\n",
       "      <td>2.029244e+01</td>\n",
       "      <td>1.943801e+01</td>\n",
       "      <td>1.857690e+01</td>\n",
       "      <td>1.803143e+01</td>\n",
       "      <td>1.783262e+01</td>\n",
       "      <td>1.686352e+01</td>\n",
       "      <td>1.628600e+01</td>\n",
       "      <td>1.608700e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.386589e+01</td>\n",
       "      <td>-3.923989e+00</td>\n",
       "      <td>8.023788e+00</td>\n",
       "      <td>1.516541e+01</td>\n",
       "      <td>9.446607e-01</td>\n",
       "      <td>3.034684e-02</td>\n",
       "      <td>2.150791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.272001e+01</td>\n",
       "      <td>2.084224e+01</td>\n",
       "      <td>2.150103e+01</td>\n",
       "      <td>2.049578e+01</td>\n",
       "      <td>1.857690e+01</td>\n",
       "      <td>1.885797e+01</td>\n",
       "      <td>1.783262e+01</td>\n",
       "      <td>1.714267e+01</td>\n",
       "      <td>1.693600e+01</td>\n",
       "      <td>1.674200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.386589e+01</td>\n",
       "      <td>-3.923989e+00</td>\n",
       "      <td>8.023788e+00</td>\n",
       "      <td>1.516541e+01</td>\n",
       "      <td>9.446607e-01</td>\n",
       "      <td>3.034684e-02</td>\n",
       "      <td>2.150791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.406546e+01</td>\n",
       "      <td>2.170478e+01</td>\n",
       "      <td>2.257610e+01</td>\n",
       "      <td>2.128138e+01</td>\n",
       "      <td>1.857690e+01</td>\n",
       "      <td>1.969202e+01</td>\n",
       "      <td>1.783262e+01</td>\n",
       "      <td>1.783368e+01</td>\n",
       "      <td>1.753000e+01</td>\n",
       "      <td>1.738200e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.386589e+01</td>\n",
       "      <td>-3.923989e+00</td>\n",
       "      <td>8.023788e+00</td>\n",
       "      <td>1.516541e+01</td>\n",
       "      <td>9.446607e-01</td>\n",
       "      <td>3.034684e-02</td>\n",
       "      <td>2.150791e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.677974e+01</td>\n",
       "      <td>2.986823e+01</td>\n",
       "      <td>2.972356e+01</td>\n",
       "      <td>2.691011e+01</td>\n",
       "      <td>2.004100e+01</td>\n",
       "      <td>1.886600e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>7.995270e+02</td>\n",
       "      <td>5.512816e+02</td>\n",
       "      <td>6.727384e+02</td>\n",
       "      <td>7.610851e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mag_auto_g    mag_auto_i    mag_auto_r    mag_auto_z     ypetromag  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06   \n",
       "mean   3.620479e+01  2.265848e+01  2.479992e+01  2.247560e+01  1.857690e+01   \n",
       "std    3.001175e+01  1.282129e+01  1.652386e+01  1.342852e+01  6.248419e-01   \n",
       "min    1.316891e+01  1.246892e+01  1.211705e+01  1.221446e+01  1.037403e+01   \n",
       "25%    2.137233e+01  1.974964e+01  2.029244e+01  1.943801e+01  1.857690e+01   \n",
       "50%    2.272001e+01  2.084224e+01  2.150103e+01  2.049578e+01  1.857690e+01   \n",
       "75%    2.406546e+01  2.170478e+01  2.257610e+01  2.128138e+01  1.857690e+01   \n",
       "max    9.900000e+01  9.900000e+01  9.900000e+01  9.900000e+01  2.677974e+01   \n",
       "\n",
       "          jpetromag     hpetromag    kspetromag        w1mpro        w2mpro  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06  4.113452e+06   \n",
       "mean   1.867087e+01  1.783262e+01  1.714267e+01  1.680647e+01  1.663633e+01   \n",
       "std    1.472027e+00  7.353093e-01  1.113538e+00  1.079176e+00  1.070361e+00   \n",
       "min    1.013221e+01  8.260656e+00  8.256360e+00  7.068000e+00  6.085000e+00   \n",
       "25%    1.803143e+01  1.783262e+01  1.686352e+01  1.628600e+01  1.608700e+01   \n",
       "50%    1.885797e+01  1.783262e+01  1.714267e+01  1.693600e+01  1.674200e+01   \n",
       "75%    1.969202e+01  1.783262e+01  1.783368e+01  1.753000e+01  1.738200e+01   \n",
       "max    2.986823e+01  2.972356e+01  2.691011e+01  2.004100e+01  1.886600e+01   \n",
       "\n",
       "       ...  extended_class_r  extended_class_i  extended_class_z  \\\n",
       "count  ...      4.113452e+06      4.113452e+06      4.113452e+06   \n",
       "mean   ...      1.607860e+00      1.857935e+00      1.833176e+00   \n",
       "std    ...      2.653605e+00      2.206180e+00      2.263713e+00   \n",
       "min    ...     -9.000000e+00     -9.000000e+00     -9.000000e+00   \n",
       "25%    ...      0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "50%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "75%    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "max    ...      3.000000e+00      3.000000e+00      3.000000e+00   \n",
       "\n",
       "                 pm         pmdec          pmra  radial_velocity  \\\n",
       "count  4.113452e+06  4.113452e+06  4.113452e+06     4.113452e+06   \n",
       "mean   1.386589e+01 -3.923989e+00  8.023788e+00     1.516541e+01   \n",
       "std    6.979330e+00  5.342078e+00  6.437422e+00     3.281982e+00   \n",
       "min    5.783000e-03 -7.481313e+02 -3.379641e+02    -3.898801e+02   \n",
       "25%    1.386589e+01 -3.923989e+00  8.023788e+00     1.516541e+01   \n",
       "50%    1.386589e+01 -3.923989e+00  8.023788e+00     1.516541e+01   \n",
       "75%    1.386589e+01 -3.923989e+00  8.023788e+00     1.516541e+01   \n",
       "max    7.995270e+02  5.512816e+02  6.727384e+02     7.610851e+02   \n",
       "\n",
       "       classprob_dsc_combmod_star  classprob_dsc_combmod_galaxy  \\\n",
       "count                4.113452e+06                  4.113452e+06   \n",
       "mean                 9.446607e-01                  3.034684e-02   \n",
       "std                  1.056586e-01                  8.018615e-02   \n",
       "min                  0.000000e+00                  0.000000e+00   \n",
       "25%                  9.446607e-01                  3.034684e-02   \n",
       "50%                  9.446607e-01                  3.034684e-02   \n",
       "75%                  9.446607e-01                  3.034684e-02   \n",
       "max                  1.000000e+00                  1.000000e+00   \n",
       "\n",
       "       classprob_dsc_combmod_quasar  \n",
       "count                  4.113452e+06  \n",
       "mean                   2.150791e-02  \n",
       "std                    6.536032e-02  \n",
       "min                    0.000000e+00  \n",
       "25%                    2.150791e-02  \n",
       "50%                    2.150791e-02  \n",
       "75%                    2.150791e-02  \n",
       "max                    1.000000e+00  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the summary statistics of the training data\n",
    "display(X_train.describe())\n",
    "\n",
    "# Display the summary statistics of the testing data\n",
    "display(X_test.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a87859",
   "metadata": {},
   "source": [
    "## TensorFlow <a class=\"anchor\" id=\"five\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f44b7",
   "metadata": {},
   "source": [
    "TensorFlow was developed by Google and released in 2015. It is a highly scalable framework that is widely used for production-level machine learning applications.\n",
    "\n",
    "TensorFlow is based on a data flow graph, where nodes represent mathematical operations and edges represent data inputs and outputs. This approach allows TensorFlow to optimize computations and run them efficiently on both CPUs and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18a14d3",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f01844",
   "metadata": {},
   "source": [
    "Model architecture refers to the overall structure and design of a machine learning model. It includes the number and type of layers, the number of neurons or units in each layer, the activation functions used in each layer, the optimization algorithm used for training, and other design choices that are made when creating a model.\n",
    "\n",
    "An autoencoder is a type of neural network used for unsupervised learning. \n",
    "\n",
    "In TensorFlow, the autoencoder model can be defined using the Keras API. \n",
    "The model architecture consists of an encoder and a decoder network that are trained to reconstruct the input data. The model parameters can be optimized using backpropagation and gradient descent algorithms. \n",
    "The trained model can be used for dimensionality reduction, anomaly detection, and data generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c6a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'TF Tester 1 Iterate between tensorflow and keras'\n",
    "# Define the simple autoencoder function\n",
    "def Autoencoder_Simple(input_size):\n",
    "    # Calculate the hidden layer size (half of the input size)\n",
    "    hidden_size = int(input_size / 2.0)\n",
    "    # Calculate the bottleneck layer size (half of the hidden layer size)\n",
    "    bottleneck_size = int(hidden_size / 2.0)\n",
    "    # Define the input layer with the specified input size\n",
    "    input_tab = Input(shape=(input_size,))\n",
    "    # Define the first hidden layer with 'relu' activation function\n",
    "    hidden_1 = layers.Dense(hidden_size, activation='relu')(input_tab)\n",
    "    # Define the bottleneck layer with 'relu' activation function\n",
    "    bottleneck = layers.Dense(bottleneck_size, activation='relu')(hidden_1)\n",
    "    # Define the second hidden layer with 'relu' activation function\n",
    "    hidden_2 = layers.Dense(hidden_size, activation='relu')(bottleneck)\n",
    "    # Define the output layer with 'linear' activation function\n",
    "    output_tab = layers.Dense(input_size, activation='linear')(hidden_2)\n",
    "    # Create the encoder model, which includes the input layer and bottleneck layer\n",
    "    encoder = Model(input_tab, bottleneck)\n",
    "    # Create the full autoencoder model, which includes the input and output layers\n",
    "    model = Model(input_tab, output_tab)\n",
    "\n",
    "    # Return both the full autoencoder model and the encoder model\n",
    "    return model, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8668ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 18:57:56.778165: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-07 18:57:56.778862: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "'TF Tester 1'\n",
    "from tensorflow.keras import layers\n",
    "# Set the input size based on the number of features in the dataset\n",
    "input_size = X.shape[1]\n",
    "# Call the Autoencoder_Simple function, passing the input_size as an argument\n",
    "model, encoder = Autoencoder_Simple(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2e5614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'TF Tester 1 generic cartoon to interpret bottleneck try to make them look alike'\n",
    "\n",
    "# Plot the model architecture\n",
    "plot_model(model, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb11be",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97593a7",
   "metadata": {},
   "source": [
    "Model training is the process of training a machine learning model to make accurate predictions on new data.\n",
    "In unsupervised learning, model training refers to the process of learning the underlying structure of the data without the use of explicit labels. \n",
    "\n",
    "Model Training involves optimizing the parameters of an autoencoder model in TensorFlow to minimize the difference between the input and output data. \n",
    "This is done by minimizing the reconstruction error using an optimization algorithm such as stochastic gradient descent. \n",
    "The model is typically trained on a large dataset, with the goal of learning useful representations of the data that can be used for tasks such as data compression, denoising, and anomaly detection. \n",
    "The training process involves adjusting the weights and biases of the neural network through backpropagation, which involves computing gradients of the loss function with respect to the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 18:58:20.061768: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-07 18:58:20.309966: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model using the scaled training data and validation data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'TF Tester 1'\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Set the number of epochs\n",
    "n_epochs = 50\n",
    "\n",
    "# Compile the model with Adam optimizer and mean squared error loss function\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the model using the scaled training data and validation data\n",
    "history = model.fit(X_train_scaled, X_train_scaled,\n",
    "                    epochs=n_epochs, batch_size=16, verbose=2,\n",
    "                    validation_data=(X_test_scaled, X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'TF Tester 1'\n",
    "# Plot the training and testing loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5035dc",
   "metadata": {},
   "source": [
    "## PyTorch <a class=\"anchor\" id=\"six\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cff5f",
   "metadata": {},
   "source": [
    "PyTorch was developed by Facebook and released in 2017. It is a highly flexible framework that is easy to use and customize. \n",
    "\n",
    "PyTorch is based on a dynamic computational graph, which allows developers to build models in a more Pythonic way. This approach is especially helpful when building dynamic models with changing input shapes or sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a90a7e",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616a138",
   "metadata": {},
   "source": [
    "Model architecture refers to the overall structure and design of a machine learning model. It includes the number and type of layers, the number of neurons or units in each layer, the activation functions used in each layer, the optimization algorithm used for training, and other design choices that are made when creating a model.\n",
    "\n",
    "Autoencoder is a neural network architecture used for unsupervised learning of efficient codings. \n",
    "\n",
    "In PyTorch, defining an autoencoder model involves creating an encoder and a decoder network, which can be trained together to learn a compressed representation of the input data. \n",
    "The encoder network takes in the input and produces a lower-dimensional representation, while the decoder network takes this representation and produces the reconstructed output. \n",
    "The goal is to minimize the difference between the input and output, while also minimizing the dimensionality of the representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cfbc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "'PT Tester 1'\n",
    "# Define the Autoencoder model\n",
    "# Define a class named Autoencoder that inherits from the PyTorch nn.Module class\n",
    "class Autoencoder(nn.Module):\n",
    "    # Initialize the Autoencoder class, taking an input_size argument\n",
    "    def __init__(self, input_size):\n",
    "        # Call the constructor of the parent nn.Module class\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Calculate the size of the hidden layer as half of the input_size\n",
    "        hidden_size = int(input_size / 2)\n",
    "        # Calculate the size of the bottleneck layer as half of the hidden_size\n",
    "        bottleneck_size = int(hidden_size / 2)\n",
    "        # Define the encoder part of the autoencoder as a sequence of layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Add a linear layer with input_size as input and hidden_size as output\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            # Add a ReLU activation layer\n",
    "            nn.ReLU(),\n",
    "            # Add a linear layer with hidden_size as input and bottleneck_size as output\n",
    "            nn.Linear(hidden_size, bottleneck_size),\n",
    "            # Add a ReLU activation layer\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Define the decoder part of the autoencoder as a sequence of layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Add a linear layer with bottleneck_size as input and hidden_size as output\n",
    "            nn.Linear(bottleneck_size, hidden_size),\n",
    "            # Add a ReLU activation layer\n",
    "            nn.ReLU(),\n",
    "            # Add a linear layer with hidden_size as input and input_size as output\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "        )\n",
    "\n",
    "    # Define the forward pass of the autoencoder\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the encoder to get the bottleneck representation\n",
    "        x = self.encoder(x)\n",
    "        # Pass the bottleneck representation through the decoder to get the reconstructed output\n",
    "        x = self.decoder(x)\n",
    "        # Return the reconstructed output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e03b6e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_torch contains nan: tensor(False)\n",
      "X_train_torch contains inf: tensor(False)\n",
      "X_test_torch contains nan: tensor(False)\n",
      "X_test_torch contains inf: tensor(False)\n"
     ]
    }
   ],
   "source": [
    "'PT Tester 1 give more explanation to keras'\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch TensorDataset for the training and testing data\n",
    "train_dataset = TensorDataset(X_train_torch)  \n",
    "test_dataset = TensorDataset(X_test_torch)\n",
    "# Create a DataLoader for the training and testing data\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Check to ensure that the data does not contain any nan or infinity values\n",
    "print(\"X_train_torch contains nan:\", torch.isnan(X_train_torch).any())\n",
    "print(\"X_train_torch contains inf:\", torch.isinf(X_train_torch).any())\n",
    "print(\"X_test_torch contains nan:\", torch.isnan(X_test_torch).any())\n",
    "print(\"X_test_torch contains inf:\", torch.isinf(X_test_torch).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40bd1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "'PT Tester 1'\n",
    "# Instantiate the model\n",
    "# Get the number of features in the input data\n",
    "input_size = X.shape[1] \n",
    "# Create an instance of the Autoencoder class with the input size\n",
    "model = Autoencoder(input_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "import torch.optim as optim\n",
    "# Define the loss function as mean squared error (MSE) loss\n",
    "criterion = nn.MSELoss() \n",
    "# Create an Adam optimizer with the specified learning rate and the model's parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26078e25",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb76c7d",
   "metadata": {},
   "source": [
    "Model training is the process of training a machine learning model to make accurate predictions on new data.\n",
    "In unsupervised learning, model training refers to the process of learning the underlying structure of the data without the use of explicit labels. \n",
    "\n",
    "In PyTorch, Autoencoder Model Training involves defining the model architecture, setting the hyperparameters, and training the model. \n",
    "The process begins with defining the encoder and decoder neural network architecture using the nn.Module class. Next, the hyperparameters, such as learning rate, batch size, and number of epochs, are set. \n",
    "Then, the optimizer and loss function are defined. \n",
    "Finally, the model is trained on the training dataset, validated on the validation dataset, and tested on the testing dataset. \n",
    "After training, the model can be used for prediction and generating new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44117e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Train Loss: 0.204974, Test Loss: 0.180165\n",
      "Epoch: 2/50, Train Loss: 0.176585, Test Loss: 0.175485\n",
      "Epoch: 3/50, Train Loss: 0.171567, Test Loss: 0.168642\n",
      "Epoch: 4/50, Train Loss: 0.197860, Test Loss: 0.194732\n",
      "Epoch: 5/50, Train Loss: 0.191366, Test Loss: 0.185105\n",
      "Epoch: 6/50, Train Loss: 0.183782, Test Loss: 0.182551\n",
      "Epoch: 7/50, Train Loss: 0.184195, Test Loss: 0.182726\n",
      "Epoch: 8/50, Train Loss: 0.184192, Test Loss: 0.182597\n",
      "Epoch: 9/50, Train Loss: 0.184411, Test Loss: 0.185470\n",
      "Epoch: 10/50, Train Loss: 0.182817, Test Loss: 0.180296\n",
      "Epoch: 11/50, Train Loss: 0.183069, Test Loss: 0.182173\n",
      "Epoch: 12/50, Train Loss: 0.184476, Test Loss: 0.183482\n",
      "Epoch: 13/50, Train Loss: 0.184548, Test Loss: 0.183062\n",
      "Epoch: 14/50, Train Loss: 0.184422, Test Loss: 0.181984\n",
      "Epoch: 15/50, Train Loss: 0.184155, Test Loss: 0.186118\n",
      "Epoch: 16/50, Train Loss: 0.184086, Test Loss: 0.185751\n",
      "Epoch: 17/50, Train Loss: 0.184288, Test Loss: 0.183167\n",
      "Epoch: 18/50, Train Loss: 0.184408, Test Loss: 0.190368\n",
      "Epoch: 19/50, Train Loss: 0.184166, Test Loss: 0.182013\n",
      "Epoch: 20/50, Train Loss: 0.184299, Test Loss: 0.182965\n",
      "Epoch: 21/50, Train Loss: 0.184320, Test Loss: 0.188758\n",
      "Epoch: 22/50, Train Loss: 0.184405, Test Loss: 0.182461\n",
      "Epoch: 23/50, Train Loss: 0.184702, Test Loss: 0.183229\n",
      "Epoch: 24/50, Train Loss: 0.184036, Test Loss: 0.184159\n",
      "Epoch: 25/50, Train Loss: 0.184053, Test Loss: 0.183055\n",
      "Epoch: 26/50, Train Loss: 0.184493, Test Loss: 0.182483\n",
      "Epoch: 27/50, Train Loss: 0.184141, Test Loss: 0.186442\n",
      "Epoch: 28/50, Train Loss: 0.184628, Test Loss: 0.183238\n",
      "Epoch: 29/50, Train Loss: 0.184792, Test Loss: 0.182296\n",
      "Epoch: 30/50, Train Loss: 0.184565, Test Loss: 0.182591\n",
      "Epoch: 31/50, Train Loss: 0.184569, Test Loss: 0.184549\n",
      "Epoch: 32/50, Train Loss: 0.184111, Test Loss: 0.182878\n",
      "Epoch: 33/50, Train Loss: 0.184840, Test Loss: 0.183035\n",
      "Epoch: 34/50, Train Loss: 0.184522, Test Loss: 0.182288\n",
      "Epoch: 35/50, Train Loss: 0.184345, Test Loss: 0.182474\n",
      "Epoch: 36/50, Train Loss: 0.184023, Test Loss: 0.182138\n",
      "Epoch: 37/50, Train Loss: 0.184334, Test Loss: 0.182417\n",
      "Epoch: 38/50, Train Loss: 0.184844, Test Loss: 0.181717\n",
      "Epoch: 39/50, Train Loss: 0.184653, Test Loss: 0.182992\n",
      "Epoch: 40/50, Train Loss: 0.184247, Test Loss: 0.184875\n",
      "Epoch: 41/50, Train Loss: 0.184033, Test Loss: 0.182938\n",
      "Epoch: 42/50, Train Loss: 0.183228, Test Loss: 0.182625\n",
      "Epoch: 43/50, Train Loss: 0.183353, Test Loss: 0.184283\n",
      "Epoch: 44/50, Train Loss: 0.183265, Test Loss: 0.182878\n",
      "Epoch: 45/50, Train Loss: 0.183363, Test Loss: 0.182230\n",
      "Epoch: 46/50, Train Loss: 0.183643, Test Loss: 0.183758\n",
      "Epoch: 47/50, Train Loss: 0.183767, Test Loss: 0.182404\n",
      "Epoch: 48/50, Train Loss: 0.183774, Test Loss: 0.183001\n",
      "Epoch: 49/50, Train Loss: 0.183544, Test Loss: 0.181167\n",
      "Epoch: 50/50, Train Loss: 0.183839, Test Loss: 0.183598\n"
     ]
    }
   ],
   "source": [
    "'PT Tester 1'\n",
    "# Set the number of epochs for training\n",
    "n_epochs = 50\n",
    "\n",
    "# Loop through each epoch\n",
    "for epoch in range(n_epochs):\n",
    "    # Initialize the train loss for the current epoch to 0\n",
    "    train_loss = 0.0\n",
    "    # Loop through each batch in the train_loader\n",
    "    for batch in train_loader:\n",
    "        # Get the input data from the current batch\n",
    "        inputs = batch[0]      \n",
    "        # Reset the optimizer gradients to 0\n",
    "        optimizer.zero_grad()   \n",
    "        # Pass the inputs through the model to get the outputs\n",
    "        outputs = model(inputs)     \n",
    "        # Calculate the loss between the outputs and the original inputs\n",
    "        loss = criterion(outputs, inputs)      \n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()    \n",
    "        # Update the model parameters using the optimizer\n",
    "        optimizer.step()     \n",
    "        # Accumulate the train loss for the current batch\n",
    "        train_loss += loss.item()\n",
    "    # Initialize the test loss for the current epoch to 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    # Disable gradient calculations for evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through each batch in the test_loader\n",
    "        for batch in test_loader:\n",
    "            # Get the input data from the current batch\n",
    "            inputs = batch[0]\n",
    "            # Pass the inputs through the model to get the outputs\n",
    "            outputs = model(inputs)  \n",
    "            # Calculate the loss between the outputs and the original inputs\n",
    "            loss = criterion(outputs, inputs)    \n",
    "            # Accumulate the test loss for the current batch\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    # Print the train and test loss for the current epoch\n",
    "    print(f\"Epoch: {epoch+1}/{n_epochs}, Train Loss: {train_loss/len(train_loader):.6f}, Test Loss: {test_loss/len(test_loader):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48505132",
   "metadata": {},
   "outputs": [],
   "source": [
    "'PT Tester 1'\n",
    "# Plot the training and testing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725a6392",
   "metadata": {},
   "source": [
    "## Tensorflow vs PyTorch  <a class=\"anchor\" id=\"seven\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b1175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
